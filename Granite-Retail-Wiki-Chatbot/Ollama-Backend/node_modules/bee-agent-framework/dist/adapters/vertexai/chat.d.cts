import { ExecutionOptions, LLMCache, LLMMeta, EmbeddingOptions, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, BaseLLMEvents } from '../../llms/base.cjs';
import { GetRunContext } from '../../context.cjs';
import { E as Emitter } from '../../emitter-DdThRYHg.cjs';
import { VertexAI, BaseModelParams } from '@google-cloud/vertexai';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from '../../llms/chat.cjs';
import { BaseMessage } from '../../llms/primitives/message.cjs';
import '../../errors.cjs';
import '../../internals/types.cjs';
import '../../internals/helpers/guards.cjs';
import '../../internals/serializable.cjs';
import '../../cache/base.cjs';
import 'promise-based-task';
import '../../internals/helpers/promise.cjs';

declare class VertexAIChatLLMOutput extends ChatLLMOutput {
    readonly chunks: BaseMessage[];
    constructor(chunk: BaseMessage);
    get messages(): BaseMessage[];
    merge(other: VertexAIChatLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        chunks: BaseMessage[];
    };
    loadSnapshot(snapshot: typeof this.createSnapshot): void;
}
interface VertexAIChatLLMInput {
    modelId: string;
    project: string;
    location: string;
    client?: VertexAI;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<VertexAIChatLLMOutput>;
    parameters?: BaseModelParams;
}
type VertexAIChatLLMEvents = ChatLLMGenerateEvents<VertexAIChatLLMOutput>;
declare class VertexAIChatLLM extends ChatLLM<VertexAIChatLLMOutput> {
    protected readonly input: VertexAIChatLLMInput;
    readonly emitter: Emitter<VertexAIChatLLMEvents>;
    protected client: VertexAI;
    protected parameters?: BaseModelParams;
    constructor(input: VertexAIChatLLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: BaseMessage[], options: GenerateOptions, run: GetRunContext<this>): Promise<VertexAIChatLLMOutput>;
    protected _stream(input: BaseMessage[], options: Partial<StreamGenerateOptions>, run: GetRunContext<this>): AsyncStream<VertexAIChatLLMOutput, void>;
    createSnapshot(): {
        input: VertexAIChatLLMInput;
        client: VertexAI;
        parameters: BaseModelParams | undefined;
        modelId: string;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, VertexAIChatLLMOutput>>;
        cache: LLMCache<VertexAIChatLLMOutput>;
    };
}

export { VertexAIChatLLM, type VertexAIChatLLMEvents, type VertexAIChatLLMInput, VertexAIChatLLMOutput };
