import { BaseLLMOutput, ExecutionOptions, LLMCache, LLMMeta, EmbeddingOptions, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, BaseLLMEvents } from '../../llms/base.js';
import { LLMEvents, LLM, LLMInput } from '../../llms/llm.js';
import { GetRunContext } from '../../context.js';
import { E as Emitter } from '../../emitter-DRfJC1TP.js';
import { VertexAI, BaseModelParams } from '@google-cloud/vertexai';
import '../../errors.js';
import '../../internals/types.js';
import '../../internals/helpers/guards.js';
import '../../internals/serializable.js';
import '../../cache/base.js';
import 'promise-based-task';
import '../../internals/helpers/promise.js';

interface VertexAILLMChunk {
    text: string;
    metadata: Record<string, any>;
}
declare class VertexAILLMOutput extends BaseLLMOutput {
    readonly chunks: VertexAILLMChunk[];
    constructor(chunk: VertexAILLMChunk);
    merge(other: VertexAILLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        chunks: VertexAILLMChunk[];
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface VertexAILLMInput {
    modelId: string;
    project: string;
    location: string;
    client?: VertexAI;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<VertexAILLMOutput>;
    parameters?: BaseModelParams;
}
type VertexAILLMEvents = LLMEvents<VertexAILLMOutput>;
declare class VertexAILLM extends LLM<VertexAILLMOutput> {
    protected readonly input: VertexAILLMInput;
    readonly emitter: Emitter<VertexAILLMEvents>;
    protected client: VertexAI;
    protected parameters?: BaseModelParams;
    constructor(input: VertexAILLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: LLMInput[], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: LLMInput): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: LLMInput, options: GenerateOptions, run: GetRunContext<this>): Promise<VertexAILLMOutput>;
    protected _stream(input: LLMInput, options: Partial<StreamGenerateOptions>, run: GetRunContext<this>): AsyncStream<VertexAILLMOutput, void>;
    createSnapshot(): {
        input: VertexAILLMInput;
        client: VertexAI;
        parameters: BaseModelParams | undefined;
        modelId: string;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, VertexAILLMOutput>>;
        cache: LLMCache<VertexAILLMOutput>;
    };
    loadSnapshot({ input, ...snapshot }: ReturnType<typeof this.createSnapshot>): void;
}

export { VertexAILLM, type VertexAILLMEvents, type VertexAILLMInput, VertexAILLMOutput };
