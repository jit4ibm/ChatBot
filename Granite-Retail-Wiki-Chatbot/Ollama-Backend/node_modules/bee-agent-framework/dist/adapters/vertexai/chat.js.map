{"version":3,"sources":["../../../src/adapters/vertexai/chat.ts"],"names":["VertexAIChatLLMOutput","ChatLLMOutput","chunks","constructor","chunk","push","messages","merge","other","getTextContent","map","result","text","join","toString","createSnapshot","shallowCopy","loadSnapshot","snapshot","Object","assign","VertexAIChatLLM","ChatLLM","emitter","client","parameters","input","modelId","executionOptions","cache","Emitter","root","child","namespace","creator","VertexAI","project","location","register","registerVertexAI","meta","tokenLimit","Infinity","embed","options","NotImplementedError","tokenize","generativeModel","createModel","response","countTokens","contents","msg","parts","role","tokensCount","totalTokens","_generate","run","guided","json","signalRace","generateContent","signal","BaseMessage","of","Role","ASSISTANT","processContentResponse","_stream","chat","startChat","sendMessageStream","stream","aborted","throwIfAborted"],"mappings":";;;;;;;;;;;AAqCO,MAAMA,8BAA8BC,aAAAA,CAAAA;EArC3C;;;AAsCkBC,EAAAA,MAAAA,GAAwB,EAAA;AAExCC,EAAAA,WAAAA,CAAYC,KAAoB,EAAA;AAC9B,IAAK,KAAA,EAAA;AACL,IAAKF,IAAAA,CAAAA,MAAAA,CAAOG,KAAKD,KAAAA,CAAAA;AACnB;AAEA,EAAA,IAAIE,QAAW,GAAA;AACb,IAAA,OAAO,IAAKJ,CAAAA,MAAAA;AACd;AAEAK,EAAAA,KAAAA,CAAMC,KAAoC,EAAA;AACxC,IAAA,IAAA,CAAKN,MAAOG,CAAAA,IAAAA,CAAI,GAAIG,KAAAA,CAAMN,MAAM,CAAA;AAClC;EAEAO,cAAyB,GAAA;AACvB,IAAO,OAAA,IAAA,CAAKP,OAAOQ,GAAI,CAAA,CAACC,WAAWA,MAAOC,CAAAA,IAAI,CAAEC,CAAAA,IAAAA,CAAK,EAAA,CAAA;AACvD;EAEAC,QAAW,GAAA;AACT,IAAA,OAAO,KAAKL,cAAc,EAAA;AAC5B;EAEAM,cAAiB,GAAA;AACf,IAAO,OAAA;MAAEb,MAAQc,EAAAA,WAAAA,CAAY,KAAKd,MAAM;AAAE,KAAA;AAC5C;AAEAe,EAAAA,YAAAA,CAAaC,QAA4C,EAAA;AACvDC,IAAOC,MAAAA,CAAAA,MAAAA,CAAO,MAAMF,QAAAA,CAAAA;AACtB;AACF;AAcO,MAAMG,wBAAwBC,OAAAA,CAAAA;EAlFrC;;;;AAmFkBC,EAAAA,OAAAA;AAKNC,EAAAA,MAAAA;AACAC,EAAAA,UAAAA;AAEVtB,EAAAA,WAAAA,CAA+BuB,KAA6B,EAAA;AAC1D,IAAA,KAAA,CAAMA,KAAMC,CAAAA,OAAAA,EAASD,KAAME,CAAAA,gBAAAA,EAAkBF,MAAMG,KAAK,CAAA,EAAA,IAD3BH,CAAAA,KAAAA,GAAAA,KAAAA,EAAAA,IAAAA,CARfH,OAAUO,GAAAA,OAAAA,CAAQC,KAAKC,KAA6B,CAAA;MAClEC,SAAW,EAAA;AAAC,QAAA,UAAA;AAAY,QAAA;;MACxBC,OAAS,EAAA;KACX,CAAA;AAOE,IAAA,IAAA,CAAKT,aAAaC,KAAMD,CAAAA,UAAAA;AACxB,IAAKD,IAAAA,CAAAA,MAAAA,GAAS,IAAIW,QAAS,CAAA;AAAEC,MAAAA,OAAAA,EAASV,KAAMU,CAAAA,OAAAA;AAASC,MAAAA,QAAAA,EAAUX,KAAMW,CAAAA;KAAS,CAAA;AAChF;EAEA;AACE,IAAA,IAAA,CAAKC,QAAQ,EAAA;AACbC,IAAAA,gBAAAA,EAAAA;AACF;AAEA,EAAA,MAAMC,IAAyB,GAAA;AAC7B,IAAO,OAAA;MAAEC,UAAYC,EAAAA;AAAS,KAAA;AAChC;;EAGA,MAAMC,KAAAA,CAAMjB,OAAwBkB,OAAsD,EAAA;AACxF,IAAA,MAAM,IAAIC,mBAAAA,EAAAA;AACZ;AAEA,EAAA,MAAMC,SAASpB,KAAsD,EAAA;AACnE,IAAA,MAAMqB,eAAkBC,GAAAA,WAAAA,CAAY,IAAKxB,CAAAA,MAAAA,EAAQ,KAAKG,OAAO,CAAA;AAC7D,IAAMsB,MAAAA,QAAAA,GAAW,MAAMF,eAAAA,CAAgBG,WAAY,CAAA;MACjDC,QAAUzB,EAAAA,KAAAA,CAAMhB,GAAI,CAAA,CAAC0C,GAAS,MAAA;QAAEC,KAAO,EAAA;AAAC,UAAA;AAAEzC,YAAAA,IAAAA,EAAMwC,GAAIxC,CAAAA;AAAK;;AAAI0C,QAAAA,IAAAA,EAAMF,GAAIE,CAAAA;OAAK,CAAA;KAC9E,CAAA;AACA,IAAO,OAAA;AACLC,MAAAA,WAAAA,EAAaN,QAASO,CAAAA;AACxB,KAAA;AACF;EAEA,MAAgBC,SAAAA,CACd/B,KACAkB,EAAAA,OAAAA,EACAc,GACgC,EAAA;AAChC,IAAMX,MAAAA,eAAAA,GAAkBC,WACtB,CAAA,IAAA,CAAKxB,MACL,EAAA,IAAA,CAAKG,SACLiB,OAAQe,CAAAA,MAAAA,EAAQC,IAChB,EAAA,IAAA,CAAKnC,UAAU,CAAA;AAEjB,IAAA,MAAMwB,QAAW,GAAA,MAAMY,UACrB,CAAA,MACEd,gBAAgBe,eAAgB,CAAA;MAC9BX,QAAUzB,EAAAA,KAAAA,CAAMhB,GAAI,CAAA,CAAC0C,GAAS,MAAA;QAAEC,KAAO,EAAA;AAAC,UAAA;AAAEzC,YAAAA,IAAAA,EAAMwC,GAAIxC,CAAAA;AAAK;;AAAI0C,QAAAA,IAAAA,EAAMF,GAAIE,CAAAA;OAAK,CAAA;KAC9E,CAAA,EACFI,IAAIK,MAAM,CAAA;AAEZ,IAAMpD,MAAAA,MAAAA,GAASqD,YAAYC,EAAG,CAAA;AAC5BX,MAAAA,IAAAA,EAAMY,IAAKC,CAAAA,SAAAA;MACXvD,IAAMwD,EAAAA,sBAAAA,CAAuBnB,SAASA,QAAQ;KAChD,CAAA;AACA,IAAO,OAAA,IAAIjD,sBAAsBW,MAAAA,CAAAA;AACnC;EAEA,OAAiB0D,OAAAA,CACf3C,KACAkB,EAAAA,OAAAA,EACAc,GAC0C,EAAA;AAC1C,IAAMX,MAAAA,eAAAA,GAAkBC,WACtB,CAAA,IAAA,CAAKxB,MACL,EAAA,IAAA,CAAKG,SACLiB,OAASe,EAAAA,MAAAA,EAAQC,IACjB,EAAA,IAAA,CAAKnC,UAAU,CAAA;AAEjB,IAAM6C,MAAAA,IAAAA,GAAOvB,gBAAgBwB,SAAS,EAAA;AACtC,IAAMtB,MAAAA,QAAAA,GAAW,MAAMqB,IAAAA,CAAKE,iBAAkB9C,CAAAA,KAAAA,CAAMhB,IAAI,CAAC0C,GAAAA,KAAQA,GAAIxC,CAAAA,IAAI,CAAA,CAAA;AACzE,IAAiBR,WAAAA,MAAAA,KAAAA,IAAS6C,SAASwB,MAAQ,EAAA;AACzC,MAAI7B,IAAAA,OAAAA,EAASmB,QAAQW,OAAS,EAAA;AAC5B,QAAA;AACF;AACA,MAAM/D,MAAAA,MAAAA,GAASqD,YAAYC,EAAG,CAAA;AAC5BX,QAAAA,IAAAA,EAAMY,IAAKC,CAAAA,SAAAA;AACXvD,QAAAA,IAAAA,EAAMwD,uBAAuBhE,KAAAA;OAC/B,CAAA;AACA,MAAM,MAAA,IAAIJ,sBAAsBW,MAAAA,CAAAA;AAClC;AACA+C,IAAAA,GAAAA,CAAIK,OAAOY,cAAc,EAAA;AAC3B;EAEA5D,cAAiB,GAAA;AACf,IAAO,OAAA;AACL,MAAA,GAAG,MAAMA,cAAAA,EAAAA;MACTW,KAAOV,EAAAA,WAAAA,CAAY,KAAKU,KAAK,CAAA;AAC7BF,MAAAA,MAAAA,EAAQ,IAAKA,CAAAA,MAAAA;AACbC,MAAAA,UAAAA,EAAY,IAAKA,CAAAA;AACnB,KAAA;AACF;AACF","file":"chat.js","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport {\n  AsyncStream,\n  BaseLLMTokenizeOutput,\n  EmbeddingOptions,\n  EmbeddingOutput,\n  ExecutionOptions,\n  GenerateOptions,\n  LLMCache,\n  LLMMeta,\n  StreamGenerateOptions,\n} from \"@/llms/base.js\";\nimport { shallowCopy } from \"@/serializer/utils.js\";\nimport type { GetRunContext } from \"@/context.js\";\nimport { Emitter } from \"@/emitter/emitter.js\";\nimport { VertexAI, BaseModelParams as Params } from \"@google-cloud/vertexai\";\nimport { ChatLLM, ChatLLMGenerateEvents, ChatLLMOutput } from \"@/llms/chat.js\";\nimport { BaseMessage, Role } from \"@/llms/primitives/message.js\";\nimport { signalRace } from \"@/internals/helpers/promise.js\";\nimport { processContentResponse, registerVertexAI, createModel } from \"./utils.js\";\nimport { NotImplementedError } from \"@/errors.js\";\n\nexport class VertexAIChatLLMOutput extends ChatLLMOutput {\n  public readonly chunks: BaseMessage[] = [];\n\n  constructor(chunk: BaseMessage) {\n    super();\n    this.chunks.push(chunk);\n  }\n\n  get messages() {\n    return this.chunks;\n  }\n\n  merge(other: VertexAIChatLLMOutput): void {\n    this.chunks.push(...other.chunks);\n  }\n\n  getTextContent(): string {\n    return this.chunks.map((result) => result.text).join(\"\");\n  }\n\n  toString() {\n    return this.getTextContent();\n  }\n\n  createSnapshot() {\n    return { chunks: shallowCopy(this.chunks) };\n  }\n\n  loadSnapshot(snapshot: typeof this.createSnapshot): void {\n    Object.assign(this, snapshot);\n  }\n}\n\nexport interface VertexAIChatLLMInput {\n  modelId: string;\n  project: string;\n  location: string;\n  client?: VertexAI;\n  executionOptions?: ExecutionOptions;\n  cache?: LLMCache<VertexAIChatLLMOutput>;\n  parameters?: Params;\n}\n\nexport type VertexAIChatLLMEvents = ChatLLMGenerateEvents<VertexAIChatLLMOutput>;\n\nexport class VertexAIChatLLM extends ChatLLM<VertexAIChatLLMOutput> {\n  public readonly emitter = Emitter.root.child<VertexAIChatLLMEvents>({\n    namespace: [\"vertexai\", \"llm\"],\n    creator: this,\n  });\n\n  protected client: VertexAI;\n  protected parameters?: Params;\n\n  constructor(protected readonly input: VertexAIChatLLMInput) {\n    super(input.modelId, input.executionOptions, input.cache);\n    this.parameters = input.parameters;\n    this.client = new VertexAI({ project: input.project, location: input.location });\n  }\n\n  static {\n    this.register();\n    registerVertexAI();\n  }\n\n  async meta(): Promise<LLMMeta> {\n    return { tokenLimit: Infinity };\n  }\n\n  // eslint-disable-next-line unused-imports/no-unused-vars\n  async embed(input: BaseMessage[][], options?: EmbeddingOptions): Promise<EmbeddingOutput> {\n    throw new NotImplementedError();\n  }\n\n  async tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput> {\n    const generativeModel = createModel(this.client, this.modelId);\n    const response = await generativeModel.countTokens({\n      contents: input.map((msg) => ({ parts: [{ text: msg.text }], role: msg.role })),\n    });\n    return {\n      tokensCount: response.totalTokens,\n    };\n  }\n\n  protected async _generate(\n    input: BaseMessage[],\n    options: GenerateOptions,\n    run: GetRunContext<this>,\n  ): Promise<VertexAIChatLLMOutput> {\n    const generativeModel = createModel(\n      this.client,\n      this.modelId,\n      options.guided?.json,\n      this.parameters,\n    );\n    const response = await signalRace(\n      () =>\n        generativeModel.generateContent({\n          contents: input.map((msg) => ({ parts: [{ text: msg.text }], role: msg.role })),\n        }),\n      run.signal,\n    );\n    const result = BaseMessage.of({\n      role: Role.ASSISTANT,\n      text: processContentResponse(response.response),\n    });\n    return new VertexAIChatLLMOutput(result);\n  }\n\n  protected async *_stream(\n    input: BaseMessage[],\n    options: Partial<StreamGenerateOptions>,\n    run: GetRunContext<this>,\n  ): AsyncStream<VertexAIChatLLMOutput, void> {\n    const generativeModel = createModel(\n      this.client,\n      this.modelId,\n      options?.guided?.json,\n      this.parameters,\n    );\n    const chat = generativeModel.startChat();\n    const response = await chat.sendMessageStream(input.map((msg) => msg.text));\n    for await (const chunk of response.stream) {\n      if (options?.signal?.aborted) {\n        break;\n      }\n      const result = BaseMessage.of({\n        role: Role.ASSISTANT,\n        text: processContentResponse(chunk),\n      });\n      yield new VertexAIChatLLMOutput(result);\n    }\n    run.signal.throwIfAborted();\n  }\n\n  createSnapshot() {\n    return {\n      ...super.createSnapshot(),\n      input: shallowCopy(this.input),\n      client: this.client,\n      parameters: this.parameters,\n    };\n  }\n}\n"]}