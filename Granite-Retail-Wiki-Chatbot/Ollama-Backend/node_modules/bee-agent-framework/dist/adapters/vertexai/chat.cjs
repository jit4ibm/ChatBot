'use strict';

var utils_cjs = require('../../serializer/utils.cjs');
var emitter_cjs = require('../../emitter/emitter.cjs');
var vertexai = require('@google-cloud/vertexai');
var chat_cjs = require('../../llms/chat.cjs');
var message_cjs = require('../../llms/primitives/message.cjs');
var promise_cjs = require('../../internals/helpers/promise.cjs');
var utils_cjs$1 = require('./utils.cjs');
var errors_cjs = require('../../errors.cjs');

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class VertexAIChatLLMOutput extends chat_cjs.ChatLLMOutput {
  static {
    __name(this, "VertexAIChatLLMOutput");
  }
  chunks = [];
  constructor(chunk) {
    super();
    this.chunks.push(chunk);
  }
  get messages() {
    return this.chunks;
  }
  merge(other) {
    this.chunks.push(...other.chunks);
  }
  getTextContent() {
    return this.chunks.map((result) => result.text).join("");
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      chunks: utils_cjs.shallowCopy(this.chunks)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class VertexAIChatLLM extends chat_cjs.ChatLLM {
  static {
    __name(this, "VertexAIChatLLM");
  }
  input;
  emitter;
  client;
  parameters;
  constructor(input) {
    super(input.modelId, input.executionOptions, input.cache), this.input = input, this.emitter = emitter_cjs.Emitter.root.child({
      namespace: [
        "vertexai",
        "llm"
      ],
      creator: this
    });
    this.parameters = input.parameters;
    this.client = new vertexai.VertexAI({
      project: input.project,
      location: input.location
    });
  }
  static {
    this.register();
    utils_cjs$1.registerVertexAI();
  }
  async meta() {
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new errors_cjs.NotImplementedError();
  }
  async tokenize(input) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId);
    const response = await generativeModel.countTokens({
      contents: input.map((msg) => ({
        parts: [
          {
            text: msg.text
          }
        ],
        role: msg.role
      }))
    });
    return {
      tokensCount: response.totalTokens
    };
  }
  async _generate(input, options, run) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId, options.guided?.json, this.parameters);
    const response = await promise_cjs.signalRace(() => generativeModel.generateContent({
      contents: input.map((msg) => ({
        parts: [
          {
            text: msg.text
          }
        ],
        role: msg.role
      }))
    }), run.signal);
    const result = message_cjs.BaseMessage.of({
      role: message_cjs.Role.ASSISTANT,
      text: utils_cjs$1.processContentResponse(response.response)
    });
    return new VertexAIChatLLMOutput(result);
  }
  async *_stream(input, options, run) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId, options?.guided?.json, this.parameters);
    const chat = generativeModel.startChat();
    const response = await chat.sendMessageStream(input.map((msg) => msg.text));
    for await (const chunk of response.stream) {
      if (options?.signal?.aborted) {
        break;
      }
      const result = message_cjs.BaseMessage.of({
        role: message_cjs.Role.ASSISTANT,
        text: utils_cjs$1.processContentResponse(chunk)
      });
      yield new VertexAIChatLLMOutput(result);
    }
    run.signal.throwIfAborted();
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      input: utils_cjs.shallowCopy(this.input),
      client: this.client,
      parameters: this.parameters
    };
  }
}

exports.VertexAIChatLLM = VertexAIChatLLM;
exports.VertexAIChatLLMOutput = VertexAIChatLLMOutput;
//# sourceMappingURL=chat.cjs.map
//# sourceMappingURL=chat.cjs.map