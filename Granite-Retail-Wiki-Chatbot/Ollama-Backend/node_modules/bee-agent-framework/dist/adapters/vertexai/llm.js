import { LLM } from '../../llms/llm.js';
import { BaseLLMOutput } from '../../llms/base.js';
import { shallowCopy } from '../../serializer/utils.js';
import { Emitter } from '../../emitter/emitter.js';
import { VertexAI } from '@google-cloud/vertexai';
import { Role } from '../../llms/primitives/message.js';
import { signalRace } from '../../internals/helpers/promise.js';
import { registerVertexAI, createModel, processContentResponse, getTokenCount } from './utils.js';
import { NotImplementedError } from '../../errors.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class VertexAILLMOutput extends BaseLLMOutput {
  static {
    __name(this, "VertexAILLMOutput");
  }
  chunks = [];
  constructor(chunk) {
    super();
    this.chunks.push(chunk);
  }
  merge(other) {
    this.chunks.push(...other.chunks);
  }
  getTextContent() {
    return this.chunks.map((result) => result.text).join("");
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      chunks: shallowCopy(this.chunks)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class VertexAILLM extends LLM {
  static {
    __name(this, "VertexAILLM");
  }
  input;
  emitter;
  client;
  parameters;
  constructor(input) {
    super(input.modelId, input.executionOptions, input.cache), this.input = input, this.emitter = Emitter.root.child({
      namespace: [
        "vertexai",
        "llm"
      ],
      creator: this
    });
    this.parameters = input.parameters;
    this.client = input.client ?? new VertexAI({
      project: input.project,
      location: input.location
    });
  }
  static {
    this.register();
    registerVertexAI();
  }
  async meta() {
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new NotImplementedError();
  }
  async tokenize(input) {
    const generativeModel = createModel(this.client, this.modelId);
    const response = await generativeModel.countTokens({
      contents: [
        {
          parts: [
            {
              text: input
            }
          ],
          role: Role.USER
        }
      ]
    });
    return {
      tokensCount: response.totalTokens
    };
  }
  async _generate(input, options, run) {
    const generativeModel = createModel(this.client, this.modelId, options.guided?.json, this.parameters);
    const responses = await signalRace(() => generativeModel.generateContent(input), run.signal);
    const result = {
      text: processContentResponse(responses.response),
      metadata: {
        tokenCount: getTokenCount(responses.response)
      }
    };
    return new VertexAILLMOutput(result);
  }
  async *_stream(input, options, run) {
    const generativeModel = createModel(this.client, this.modelId, options?.guided?.json, this.parameters);
    const response = await generativeModel.generateContentStream(input);
    for await (const chunk of response.stream) {
      if (options?.signal?.aborted) {
        break;
      }
      const result = {
        text: processContentResponse(chunk),
        metadata: {
          tokenCount: getTokenCount(chunk)
        }
      };
      yield new VertexAILLMOutput(result);
    }
    run.signal.throwIfAborted();
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      input: shallowCopy(this.input),
      client: this.client,
      parameters: this.parameters
    };
  }
  loadSnapshot({ input, ...snapshot }) {
    super.loadSnapshot(snapshot);
    Object.assign(this, {
      input
    });
  }
}

export { VertexAILLM, VertexAILLMOutput };
//# sourceMappingURL=llm.js.map
//# sourceMappingURL=llm.js.map