import { shallowCopy } from '../../serializer/utils.js';
import { Emitter } from '../../emitter/emitter.js';
import { VertexAI } from '@google-cloud/vertexai';
import { ChatLLMOutput, ChatLLM } from '../../llms/chat.js';
import { BaseMessage, Role } from '../../llms/primitives/message.js';
import { signalRace } from '../../internals/helpers/promise.js';
import { registerVertexAI, createModel, processContentResponse } from './utils.js';
import { NotImplementedError } from '../../errors.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class VertexAIChatLLMOutput extends ChatLLMOutput {
  static {
    __name(this, "VertexAIChatLLMOutput");
  }
  chunks = [];
  constructor(chunk) {
    super();
    this.chunks.push(chunk);
  }
  get messages() {
    return this.chunks;
  }
  merge(other) {
    this.chunks.push(...other.chunks);
  }
  getTextContent() {
    return this.chunks.map((result) => result.text).join("");
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      chunks: shallowCopy(this.chunks)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class VertexAIChatLLM extends ChatLLM {
  static {
    __name(this, "VertexAIChatLLM");
  }
  input;
  emitter;
  client;
  parameters;
  constructor(input) {
    super(input.modelId, input.executionOptions, input.cache), this.input = input, this.emitter = Emitter.root.child({
      namespace: [
        "vertexai",
        "llm"
      ],
      creator: this
    });
    this.parameters = input.parameters;
    this.client = new VertexAI({
      project: input.project,
      location: input.location
    });
  }
  static {
    this.register();
    registerVertexAI();
  }
  async meta() {
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new NotImplementedError();
  }
  async tokenize(input) {
    const generativeModel = createModel(this.client, this.modelId);
    const response = await generativeModel.countTokens({
      contents: input.map((msg) => ({
        parts: [
          {
            text: msg.text
          }
        ],
        role: msg.role
      }))
    });
    return {
      tokensCount: response.totalTokens
    };
  }
  async _generate(input, options, run) {
    const generativeModel = createModel(this.client, this.modelId, options.guided?.json, this.parameters);
    const response = await signalRace(() => generativeModel.generateContent({
      contents: input.map((msg) => ({
        parts: [
          {
            text: msg.text
          }
        ],
        role: msg.role
      }))
    }), run.signal);
    const result = BaseMessage.of({
      role: Role.ASSISTANT,
      text: processContentResponse(response.response)
    });
    return new VertexAIChatLLMOutput(result);
  }
  async *_stream(input, options, run) {
    const generativeModel = createModel(this.client, this.modelId, options?.guided?.json, this.parameters);
    const chat = generativeModel.startChat();
    const response = await chat.sendMessageStream(input.map((msg) => msg.text));
    for await (const chunk of response.stream) {
      if (options?.signal?.aborted) {
        break;
      }
      const result = BaseMessage.of({
        role: Role.ASSISTANT,
        text: processContentResponse(chunk)
      });
      yield new VertexAIChatLLMOutput(result);
    }
    run.signal.throwIfAborted();
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      input: shallowCopy(this.input),
      client: this.client,
      parameters: this.parameters
    };
  }
}

export { VertexAIChatLLM, VertexAIChatLLMOutput };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map