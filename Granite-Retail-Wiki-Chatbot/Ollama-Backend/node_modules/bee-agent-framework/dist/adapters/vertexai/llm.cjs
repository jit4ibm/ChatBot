'use strict';

var llm_cjs = require('../../llms/llm.cjs');
var base_cjs = require('../../llms/base.cjs');
var utils_cjs = require('../../serializer/utils.cjs');
var emitter_cjs = require('../../emitter/emitter.cjs');
var vertexai = require('@google-cloud/vertexai');
var message_cjs = require('../../llms/primitives/message.cjs');
var promise_cjs = require('../../internals/helpers/promise.cjs');
var utils_cjs$1 = require('./utils.cjs');
var errors_cjs = require('../../errors.cjs');

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class VertexAILLMOutput extends base_cjs.BaseLLMOutput {
  static {
    __name(this, "VertexAILLMOutput");
  }
  chunks = [];
  constructor(chunk) {
    super();
    this.chunks.push(chunk);
  }
  merge(other) {
    this.chunks.push(...other.chunks);
  }
  getTextContent() {
    return this.chunks.map((result) => result.text).join("");
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      chunks: utils_cjs.shallowCopy(this.chunks)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class VertexAILLM extends llm_cjs.LLM {
  static {
    __name(this, "VertexAILLM");
  }
  input;
  emitter;
  client;
  parameters;
  constructor(input) {
    super(input.modelId, input.executionOptions, input.cache), this.input = input, this.emitter = emitter_cjs.Emitter.root.child({
      namespace: [
        "vertexai",
        "llm"
      ],
      creator: this
    });
    this.parameters = input.parameters;
    this.client = input.client ?? new vertexai.VertexAI({
      project: input.project,
      location: input.location
    });
  }
  static {
    this.register();
    utils_cjs$1.registerVertexAI();
  }
  async meta() {
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new errors_cjs.NotImplementedError();
  }
  async tokenize(input) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId);
    const response = await generativeModel.countTokens({
      contents: [
        {
          parts: [
            {
              text: input
            }
          ],
          role: message_cjs.Role.USER
        }
      ]
    });
    return {
      tokensCount: response.totalTokens
    };
  }
  async _generate(input, options, run) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId, options.guided?.json, this.parameters);
    const responses = await promise_cjs.signalRace(() => generativeModel.generateContent(input), run.signal);
    const result = {
      text: utils_cjs$1.processContentResponse(responses.response),
      metadata: {
        tokenCount: utils_cjs$1.getTokenCount(responses.response)
      }
    };
    return new VertexAILLMOutput(result);
  }
  async *_stream(input, options, run) {
    const generativeModel = utils_cjs$1.createModel(this.client, this.modelId, options?.guided?.json, this.parameters);
    const response = await generativeModel.generateContentStream(input);
    for await (const chunk of response.stream) {
      if (options?.signal?.aborted) {
        break;
      }
      const result = {
        text: utils_cjs$1.processContentResponse(chunk),
        metadata: {
          tokenCount: utils_cjs$1.getTokenCount(chunk)
        }
      };
      yield new VertexAILLMOutput(result);
    }
    run.signal.throwIfAborted();
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      input: utils_cjs.shallowCopy(this.input),
      client: this.client,
      parameters: this.parameters
    };
  }
  loadSnapshot({ input, ...snapshot }) {
    super.loadSnapshot(snapshot);
    Object.assign(this, {
      input
    });
  }
}

exports.VertexAILLM = VertexAILLM;
exports.VertexAILLMOutput = VertexAILLMOutput;
//# sourceMappingURL=llm.cjs.map
//# sourceMappingURL=llm.cjs.map