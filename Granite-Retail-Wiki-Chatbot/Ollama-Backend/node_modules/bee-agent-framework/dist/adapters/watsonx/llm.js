import { LLM } from '../../llms/llm.js';
import { BaseLLMOutput, LLMOutputError, LLMFatalError, LLMError } from '../../llms/base.js';
import { HttpError } from '@ibm-generative-ai/node-sdk';
import * as R from 'remeda';
import { FrameworkError, ValueError } from '../../errors.js';
import { Cache, CacheFn } from '../../cache/decoratorCache.js';
import { shallowCopy } from '../../serializer/utils.js';
import { safeSum } from '../../internals/helpers/number.js';
import { omitUndefined } from '../../internals/helpers/object.js';
import { createURLParams, RestfulClient, RestfulClientError } from '../../internals/fetcher.js';
import { Emitter } from '../../emitter/emitter.js';
import { getEnv } from '../../internals/env.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class WatsonXLLMOutput extends BaseLLMOutput {
  static {
    __name(this, "WatsonXLLMOutput");
  }
  meta;
  results;
  constructor(content) {
    super();
    this.meta = content.meta;
    this.results = content.results;
  }
  static {
    this.register();
  }
  getTextContent() {
    return this.finalResult.generated_text;
  }
  get finalResult() {
    if (this.results.length === 0) {
      throw new LLMOutputError("No chunks to get final result from!");
    }
    const processors = {
      generated_text: /* @__PURE__ */ __name((value = "", oldValue = "") => oldValue + value, "generated_text"),
      input_token_count: safeSum,
      generated_token_count: safeSum,
      stop_reason: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "stop_reason")
    };
    const finalResult = {};
    for (const next of this.results) {
      for (const [key, value] of R.entries(next)) {
        const oldValue = finalResult[key];
        finalResult[key] = (processors[key] ?? takeFirst)(value, oldValue);
      }
    }
    return finalResult;
  }
  merge(other) {
    Cache.getInstance(this, "finalResult").clear();
    this.results.push(...other.results);
    Object.assign(this.meta, omitUndefined(other.meta));
  }
  createSnapshot() {
    return {
      results: shallowCopy(this.results),
      meta: shallowCopy(this.meta)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
  toString() {
    return this.getTextContent();
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", typeof Readonly === "undefined" ? Object : Readonly),
  _ts_metadata("design:paramtypes", [])
], WatsonXLLMOutput.prototype, "finalResult", null);
function createApiClient({ deploymentId, apiKey, baseUrl, authBaseUrl = "https://iam.cloud.ibm.com", region, accessToken, version = "2023-05-02", projectId, spaceId }) {
  region = region || getEnv("WATSONX_REGION") || "us-south";
  const paths = (() => {
    const pathPrefix = deploymentId ? `/ml/v1/deployments/${deploymentId}` : "/ml/v1";
    const queryParams = createURLParams({
      version
    });
    return {
      generate: `${pathPrefix}/text/generation?${queryParams}`,
      generate_stream: `${pathPrefix}/text/generation_stream?${queryParams}`,
      tokenization: `/ml/v1/text/tokenization?${queryParams}`,
      models: `/ml/v1/foundation_model_specs?${queryParams}`,
      deployment: deploymentId ? `/ml/v4/deployments/${deploymentId}?${createURLParams({
        version,
        project_id: projectId,
        space_id: projectId ? void 0 : spaceId
      })}` : "not_defined_endpoint",
      embeddings: "/ml/v1/text/embeddings"
    };
  })();
  if (accessToken && apiKey) {
    throw new ValueError(`Use either "accessToken" or "apiKey".`);
  } else if (!accessToken && !apiKey) {
    accessToken = getEnv("WATSONX_ACCESS_TOKEN");
    apiKey = accessToken ? void 0 : getEnv("WATSONX_API_KEY");
  }
  if (!accessToken && !apiKey) {
    throw new ValueError([
      `Neither "accessToken" nor "apiKey" has been provided.`,
      `Either set them directly or put them in ENV ("WATSONX_ACCESS_TOKEN" / "WATSONX_API_KEY")`
    ].join("\n"));
  }
  const getHeaders = CacheFn.create(async () => {
    const getAccessToken = /* @__PURE__ */ __name(async () => {
      if (accessToken) {
        return {
          ttl: Infinity,
          token: accessToken
        };
      }
      const response2 = await fetch(new URL("/identity/token", authBaseUrl), {
        method: "POST",
        headers: {
          "Content-Type": "application/x-www-form-urlencoded"
        },
        body: createURLParams({
          grant_type: "urn:ibm:params:oauth:grant-type:apikey",
          apikey: apiKey
        })
      });
      if (!response2.ok) {
        throw new RestfulClientError("Failed to retrieve an API token.", [], {
          context: response2
        });
      }
      const data = await response2.json();
      if (!data?.access_token) {
        throw new RestfulClientError("Access Token was not found in the response.");
      }
      return {
        ttl: (data.expires_in - 60) * 1e3,
        token: data.access_token
      };
    }, "getAccessToken");
    const response = await getAccessToken();
    getHeaders.updateTTL(response.ttl);
    return new Headers({
      "Authorization": `Bearer ${response.token}`,
      "Accept": "application/json",
      "Content-Type": "application/json"
    });
  });
  return new RestfulClient({
    baseUrl: baseUrl || `https://${region}.ml.cloud.ibm.com`,
    paths,
    headers: getHeaders
  });
}
__name(createApiClient, "createApiClient");
class WatsonXLLM extends LLM {
  static {
    __name(this, "WatsonXLLM");
  }
  emitter = Emitter.root.child({
    namespace: [
      "watsonx",
      "llm"
    ],
    creator: this
  });
  client;
  projectId;
  deploymentId;
  spaceId;
  transform;
  moderations;
  parameters;
  constructor(input) {
    super(input.modelId, input.executionOptions, input.cache);
    this.projectId = input.projectId;
    this.spaceId = input.spaceId;
    this.deploymentId = input.deploymentId;
    this.moderations = input.moderations;
    this.transform = input.transform ?? ((input2) => input2);
    this.client = createApiClient(input);
    this.parameters = input.parameters ?? {};
  }
  static {
    this.register();
  }
  async meta() {
    let modelId = this.modelId;
    if (this.deploymentId) {
      const { entity } = await this.client.fetch("deployment");
      modelId = entity.base_model_id ?? modelId;
    }
    if (!modelId) {
      throw new LLMFatalError(`Cannot retrieve metadata for model '${modelId ?? "undefined"}'`);
    }
    const { resources: [model] } = await this.client.fetch("models", {
      searchParams: createURLParams({
        filters: `modelid_${modelId}`,
        limit: "1"
      })
    });
    return {
      tokenLimit: model?.model_limits?.max_sequence_length ?? Infinity
    };
  }
  async embed(input, options) {
    const response = await this.client.fetch("embeddings", {
      method: "POST",
      searchParams: new URLSearchParams({
        version: "2023-10-25"
      }),
      body: JSON.stringify({
        inputs: input,
        model_id: this.modelId,
        project_id: this.projectId,
        parameters: {
          truncate_input_tokens: 512
        }
      }),
      signal: options?.signal
    });
    if (response.results?.length !== input.length) {
      throw new Error("Missing embedding");
    }
    const embeddings = response.results.map((result) => result.embedding);
    return {
      embeddings
    };
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      spaceId: this.spaceId,
      deploymentId: this.deploymentId,
      projectId: this.projectId,
      parameters: shallowCopy(this.parameters),
      moderations: shallowCopy(this.moderations),
      executionOptions: shallowCopy(this.executionOptions),
      transform: this.transform,
      client: this.client
    };
  }
  loadSnapshot(snapshot) {
    super.loadSnapshot(snapshot);
  }
  _transformError(error) {
    if (error instanceof FrameworkError) {
      throw error;
    }
    if (error instanceof HttpError) {
      throw new LLMError("LLM has occurred an error!", [
        error
      ], {
        isRetryable: [
          408,
          425,
          429,
          500,
          503
        ].includes(error.status_code)
      });
    }
    return new LLMError("LLM has occurred an error!", [
      error
    ]);
  }
  async tokenize(input) {
    try {
      const { result } = await this.client.fetch("tokenization", {
        method: "POST",
        body: JSON.stringify({
          input,
          model_id: this.modelId,
          project_id: this.projectId,
          space_id: this.projectId ? void 0 : this.spaceId,
          parameters: {
            return_tokens: true
          }
        })
      });
      return {
        tokensCount: result.token_count,
        tokens: result.tokens
      };
    } catch (e) {
      throw this._transformError(e);
    }
  }
  async _generate(input, options, run) {
    try {
      const response = await this.client.fetch("generate", {
        method: "POST",
        body: JSON.stringify(this.transform({
          input,
          ...!this.deploymentId && {
            model_id: this.modelId,
            project_id: this.projectId,
            space_id: this.projectId ? void 0 : this.spaceId
          },
          parameters: options?.parameters ?? this.parameters,
          moderations: options?.moderations ?? this.moderations
        })),
        signal: run.signal
      });
      return this._rawResponseToOutput(response);
    } catch (e) {
      throw this._transformError(e);
    }
  }
  async *_stream(input, options, run) {
    try {
      const response = this.client.stream("generate_stream", {
        method: "POST",
        body: JSON.stringify(this.transform({
          input,
          ...!this.deploymentId && {
            model_id: this.modelId,
            project_id: this.projectId,
            space_id: this.projectId ? void 0 : this.spaceId
          },
          parameters: options?.parameters ?? this.parameters,
          moderations: options?.moderations ?? this.moderations
        })),
        signal: run.signal
      });
      for await (const msg of response) {
        const content = JSON.parse(msg.data);
        yield this._rawResponseToOutput(content);
      }
    } catch (e) {
      throw this._transformError(e);
    }
  }
  _rawResponseToOutput(raw) {
    return new WatsonXLLMOutput({
      results: raw.results ?? [],
      meta: R.pickBy({
        model_id: raw.model_id,
        created_at: raw.created_at
      }, R.isDefined),
      system: raw.system ?? []
    });
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", Function),
  _ts_metadata("design:paramtypes", []),
  _ts_metadata("design:returntype", Promise)
], WatsonXLLM.prototype, "meta", null);

export { WatsonXLLM, WatsonXLLMOutput };
//# sourceMappingURL=llm.js.map
//# sourceMappingURL=llm.js.map