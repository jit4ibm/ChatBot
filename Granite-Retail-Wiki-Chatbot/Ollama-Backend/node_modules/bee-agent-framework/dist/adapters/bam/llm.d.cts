import { BaseLLMOutput, GenerateOptions, ExecutionOptions, LLMCache, LLMMeta, EmbeddingOptions, EmbeddingOutput, BaseLLMEvents, BaseLLMTokenizeOutput, AsyncStream } from '../../llms/base.cjs';
import { LLMEvents, LLM, LLMInput } from '../../llms/llm.cjs';
import { TextGenerationCreateOutput, TextGenerationCreateStreamOutput, TextGenerationCreateInput, TextGenerationCreateStreamInput, Client } from '@ibm-generative-ai/node-sdk';
import { ExcludeNonStringIndex } from '../../internals/types.cjs';
import { E as Emitter } from '../../emitter-DdThRYHg.cjs';
import { GetRunContext } from '../../context.cjs';
import '../../errors.cjs';
import '../../internals/helpers/guards.cjs';
import '../../internals/serializable.cjs';
import '../../cache/base.cjs';
import 'promise-based-task';
import '../../internals/helpers/promise.cjs';

type BAMLLMOutputMeta = Omit<ExcludeNonStringIndex<TextGenerationCreateOutput>, "results">;
type BAMLLMOutputResult = ExcludeNonStringIndex<TextGenerationCreateOutput["results"][number]>;
type BAMLLMOutputModeration = ExcludeNonStringIndex<TextGenerationCreateStreamOutput["moderations"]>;
interface BAMLLMOutputConstructor {
    meta: BAMLLMOutputMeta;
    results: BAMLLMOutputResult[];
    moderations?: BAMLLMOutputModeration | BAMLLMOutputModeration[];
}
declare class BAMLLMOutput extends BaseLLMOutput {
    readonly meta: BAMLLMOutputMeta;
    readonly results: BAMLLMOutputResult[];
    readonly moderations: BAMLLMOutputModeration[];
    constructor(content: BAMLLMOutputConstructor);
    getTextContent(): string;
    get finalModeration(): Readonly<BAMLLMOutputModeration>;
    get finalResult(): Readonly<BAMLLMOutputResult>;
    merge(other: BAMLLMOutput): void;
    createSnapshot(): {
        results: ExcludeNonStringIndex<{
            [key: string]: unknown;
            input_text?: string | null;
            generated_text: string;
            generated_token_count: number;
            input_token_count?: number | null;
            stop_reason: "not_finished" | "max_tokens" | "eos_token" | "cancelled" | "time_limit" | "stop_sequence" | "token_limit" | "error";
            stop_sequence?: string | null;
            generated_tokens?: (({
                text?: string | null;
                logprob?: (number | null) | (string | null);
                rank?: number | null;
                top_tokens?: (({
                    text?: string | null;
                    logprob?: (number | null) | (string | null);
                })[]) | null;
            })[]) | null;
            input_tokens?: (({
                text?: string | null;
                logprob?: (number | null) | (string | null);
                rank?: number | null;
                top_tokens?: (({
                    text?: string | null;
                    logprob?: (number | null) | (string | null);
                })[]) | null;
            })[]) | null;
            seed?: number | null;
            moderations?: {
                hap?: {
                    success: boolean;
                    score: number;
                    flagged: boolean;
                    position: {
                        start: number;
                        end: number;
                    };
                    tokens?: {
                        token?: string;
                        index?: number;
                        score?: number;
                    }[];
                }[];
                social_bias?: {
                    success: boolean;
                    score: number;
                    flagged: boolean;
                    position: {
                        start: number;
                        end: number;
                    };
                    tokens?: {
                        token?: string;
                        index?: number;
                        score?: number;
                    }[];
                }[];
            };
        }>[];
        moderations: BAMLLMOutputModeration[];
        meta: BAMLLMOutputMeta;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
    toString(): string;
    protected static _combineModerations(...entries: BAMLLMOutputModeration[]): ExcludeNonStringIndex<{
        hap?: {
            success: boolean;
            score: number;
            flagged: boolean;
            position: {
                start: number;
                end: number;
            };
            tokens?: {
                token?: string;
                index?: number;
                score?: number;
            }[];
        }[];
        social_bias?: {
            success: boolean;
            score: number;
            flagged: boolean;
            position: {
                start: number;
                end: number;
            };
            tokens?: {
                token?: string;
                index?: number;
                score?: number;
            }[];
        }[];
    }>;
}
type BAMLLMParameters = NonNullable<TextGenerationCreateInput["parameters"] & TextGenerationCreateStreamInput["parameters"]>;
interface BAMLLMGenerateOptions extends GenerateOptions {
    moderations?: TextGenerationCreateInput["moderations"];
}
interface BAMLLMInput {
    client?: Client;
    modelId: string;
    parameters?: BAMLLMParameters;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<BAMLLMOutput>;
}
type BAMLLMEvents = LLMEvents<BAMLLMOutput>;
declare class BAMLLM extends LLM<BAMLLMOutput, BAMLLMGenerateOptions> {
    readonly emitter: Emitter<BAMLLMEvents>;
    readonly client: Client;
    readonly parameters: Partial<BAMLLMParameters>;
    constructor({ client, parameters, modelId, cache, executionOptions }: BAMLLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: LLMInput[], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    createSnapshot(): {
        client: null;
        modelId: string;
        parameters: Partial<{
            top_k?: number | null;
            top_p?: number | null;
            guided?: ({
                format?: undefined;
                regex?: undefined;
                choice?: undefined;
                json_schema?: undefined;
            } & {
                grammar?: string;
            }) | ({
                grammar?: undefined;
            } & (({
                format?: undefined;
                regex?: undefined;
                json_schema?: undefined;
            } & {
                choice?: string[];
            }) | ({
                choice?: undefined;
            } & (({
                format?: undefined;
                json_schema?: undefined;
            } & {
                regex?: string;
            }) | ({
                regex?: undefined;
            } & (({
                format?: undefined;
            } & {
                json_schema?: Record<string, never>;
            }) | ({
                json_schema?: undefined;
            } & {
                format?: "TEXT" | "JSON";
            })))))));
            typical_p?: number | null;
            beam_width?: number | null;
            time_limit?: number | null;
            random_seed?: number | null;
            temperature?: number | null;
            length_penalty?: ({
                start_index?: number | null;
                decay_factor?: number | null;
            }) | null;
            max_new_tokens?: number | null;
            min_new_tokens?: number | null;
            return_options?: ({
                input_text?: boolean | null;
                token_ranks?: boolean | null;
                input_tokens?: boolean | null;
                top_n_tokens?: number | null;
                token_logprobs?: boolean | null;
                generated_tokens?: boolean | null;
                input_parameters?: boolean | null;
            }) | null;
            stop_sequences?: string[] | null;
            decoding_method?: "greedy" | "sample" | null;
            repetition_penalty?: number | null;
            include_stop_sequence?: boolean;
            truncate_input_tokens?: number | null;
        } & {
            top_k?: number | null;
            top_p?: number | null;
            guided?: ({
                format?: undefined;
                regex?: undefined;
                choice?: undefined;
                json_schema?: undefined;
            } & {
                grammar?: string;
            }) | ({
                grammar?: undefined;
            } & (({
                format?: undefined;
                regex?: undefined;
                json_schema?: undefined;
            } & {
                choice?: string[];
            }) | ({
                choice?: undefined;
            } & (({
                format?: undefined;
                json_schema?: undefined;
            } & {
                regex?: string;
            }) | ({
                regex?: undefined;
            } & (({
                format?: undefined;
            } & {
                json_schema?: Record<string, never>;
            }) | ({
                json_schema?: undefined;
            } & {
                format?: "TEXT" | "JSON";
            })))))));
            typical_p?: number | null;
            beam_width?: number | null;
            time_limit?: number | null;
            random_seed?: number | null;
            temperature?: number | null;
            length_penalty?: ({
                start_index?: number | null;
                decay_factor?: number | null;
            }) | null;
            max_new_tokens?: number | null;
            min_new_tokens?: number | null;
            return_options?: ({
                input_text?: boolean | null;
                token_ranks?: boolean | null;
                input_tokens?: boolean | null;
                top_n_tokens?: number | null;
                token_logprobs?: boolean | null;
                generated_tokens?: boolean | null;
                input_parameters?: boolean | null;
            }) | null;
            stop_sequences?: string[] | null;
            decoding_method?: "greedy" | "sample" | null;
            repetition_penalty?: number | null;
            include_stop_sequence?: boolean;
            truncate_input_tokens?: number | null;
        }>;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, BAMLLMOutput>>;
        cache: LLMCache<BAMLLMOutput>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
    protected _transformError(error: Error): Error;
    tokenize(input: LLMInput): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: LLMInput, options: BAMLLMGenerateOptions, run: GetRunContext<typeof this>): Promise<BAMLLMOutput>;
    protected _stream(input: string, options: BAMLLMGenerateOptions, run: GetRunContext<typeof this>): AsyncStream<BAMLLMOutput, void>;
    protected _rawResponseToOutput(raw: TextGenerationCreateOutput | TextGenerationCreateStreamOutput): BAMLLMOutput;
    protected _prepareParameters(overrides?: GenerateOptions): typeof this.parameters;
}

export { BAMLLM, type BAMLLMEvents, type BAMLLMGenerateOptions, type BAMLLMInput, BAMLLMOutput, type BAMLLMOutputConstructor, type BAMLLMOutputMeta, type BAMLLMOutputModeration, type BAMLLMOutputResult, type BAMLLMParameters };
