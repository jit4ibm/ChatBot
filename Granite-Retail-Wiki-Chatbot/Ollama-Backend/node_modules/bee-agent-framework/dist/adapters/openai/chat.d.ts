import { EmbeddingOptions, LLMMeta, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, ExecutionOptions, BaseLLMEvents, LLMCache } from '../../llms/base.js';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { E as Emitter } from '../../emitter-DRfJC1TP.js';
import { OpenAI, AzureOpenAI } from 'openai';
import { GetRunContext } from '../../context.js';
import { EmbeddingCreateParams, ChatCompletionCreateParams, ChatCompletionChunk, ChatModel } from 'openai/resources/index';
import '../../errors.js';
import '../../internals/types.js';
import '../../internals/helpers/guards.js';
import '../../internals/serializable.js';
import '../../cache/base.js';
import 'promise-based-task';
import '../../internals/helpers/promise.js';

type Parameters = Omit<ChatCompletionCreateParams, "stream" | "messages" | "model">;
type Response = Omit<ChatCompletionChunk, "object">;
declare class OpenAIChatLLMOutput extends ChatLLMOutput {
    readonly responses: Response[];
    constructor(response: Response);
    get messages(): BaseMessage[];
    getTextContent(): string;
    merge(other: OpenAIChatLLMOutput): void;
    toString(): string;
    createSnapshot(): {
        responses: Response[];
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface Input {
    modelId?: ChatModel;
    client?: OpenAI | AzureOpenAI;
    parameters?: Partial<Parameters>;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<OpenAIChatLLMOutput>;
    azure?: boolean;
}
type OpenAIChatLLMEvents = ChatLLMGenerateEvents<OpenAIChatLLMOutput>;
interface OpenAIEmbeddingOptions extends EmbeddingOptions, Omit<EmbeddingCreateParams, "input" | "model"> {
}
declare class OpenAIChatLLM extends ChatLLM<OpenAIChatLLMOutput> {
    readonly emitter: Emitter<OpenAIChatLLMEvents>;
    readonly client: OpenAI | AzureOpenAI;
    readonly parameters: Partial<Parameters>;
    constructor({ client, modelId, parameters, executionOptions, cache, azure }?: Input);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: OpenAIEmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected _prepareRequest(input: BaseMessage[], options?: GenerateOptions): ChatCompletionCreateParams;
    protected _generate(input: BaseMessage[], options: Partial<GenerateOptions>, run: GetRunContext<typeof this>): Promise<OpenAIChatLLMOutput>;
    protected _stream(input: BaseMessage[], options: StreamGenerateOptions | undefined, run: GetRunContext<typeof this>): AsyncStream<OpenAIChatLLMOutput>;
    createSnapshot(): {
        parameters: Partial<Parameters>;
        client: OpenAI | AzureOpenAI;
        modelId: string;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, OpenAIChatLLMOutput>>;
        cache: LLMCache<OpenAIChatLLMOutput>;
    };
}

export { OpenAIChatLLM, type OpenAIChatLLMEvents, OpenAIChatLLMOutput, type OpenAIEmbeddingOptions };
