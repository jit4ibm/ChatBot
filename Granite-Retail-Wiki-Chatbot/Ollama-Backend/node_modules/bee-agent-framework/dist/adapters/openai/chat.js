import { shallowCopy } from '../../serializer/utils.js';
import { ChatLLMOutput, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { Emitter } from '../../emitter/emitter.js';
import { AzureOpenAI, OpenAI } from 'openai';
import { promptTokensEstimate } from 'openai-chat-tokens';
import { Serializer } from '../../serializer/serializer.js';
import { getProp, getPropStrict } from '../../internals/helpers/object.js';
import { isString } from 'remeda';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class OpenAIChatLLMOutput extends ChatLLMOutput {
  static {
    __name(this, "OpenAIChatLLMOutput");
  }
  responses;
  constructor(response) {
    super();
    this.responses = [
      response
    ];
  }
  static {
    this.register();
  }
  get messages() {
    return this.responses.flatMap((response) => response.choices).flatMap((choice) => BaseMessage.of({
      role: choice.delta.role,
      text: choice.delta.content
    }));
  }
  getTextContent() {
    return this.messages.map((msg) => msg.text).join("\n");
  }
  merge(other) {
    this.responses.push(...other.responses);
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      responses: shallowCopy(this.responses)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class OpenAIChatLLM extends ChatLLM {
  static {
    __name(this, "OpenAIChatLLM");
  }
  emitter = Emitter.root.child({
    namespace: [
      "openai",
      "chat_llm"
    ],
    creator: this
  });
  client;
  parameters;
  constructor({ client, modelId, parameters, executionOptions = {}, cache, azure } = {}) {
    super(modelId || "gpt-4o-mini", executionOptions, cache);
    if (client) {
      this.client = client;
    } else if (azure) {
      this.client = new AzureOpenAI();
    } else {
      this.client = new OpenAI();
    }
    this.parameters = parameters ?? {
      temperature: 0
    };
  }
  static {
    this.register();
    Serializer.register(AzureOpenAI, {
      toPlain: /* @__PURE__ */ __name((value) => ({
        azureADTokenProvider: getPropStrict(value, "_azureADTokenProvider"),
        apiVersion: getPropStrict(value, "apiVersion"),
        deployment: getPropStrict(value, "_deployment")
      }), "toPlain"),
      fromPlain: /* @__PURE__ */ __name((value) => new AzureOpenAI(value.azureADTokenProvider), "fromPlain")
    });
    Serializer.register(OpenAI, {
      toPlain: /* @__PURE__ */ __name((value) => ({
        options: getPropStrict(value, "_options")
      }), "toPlain"),
      fromPlain: /* @__PURE__ */ __name((value) => new OpenAI(value.options), "fromPlain")
    });
  }
  async meta() {
    if (this.modelId.includes("gpt-4o") || this.modelId.includes("gpt-4-turbo") || this.modelId.includes("gpt-4-0125-preview") || this.modelId.includes("gpt-4-1106-preview")) {
      return {
        tokenLimit: 128 * 1024
      };
    } else if (this.modelId.includes("gpt-4")) {
      return {
        tokenLimit: 8 * 1024
      };
    } else if (this.modelId.includes("gpt-3.5-turbo")) {
      return {
        tokenLimit: 16 * 1024
      };
    } else if (this.modelId.includes("gpt-3.5")) {
      return {
        tokenLimit: 8 * 1024
      };
    }
    return {
      tokenLimit: Infinity
    };
  }
  async embed(input, options = {}) {
    const response = await this.client.embeddings.create({
      ...options,
      model: this.modelId,
      input: input.flatMap((messages) => messages).flatMap((msg) => msg.text)
    }, {
      signal: options?.signal
    });
    const embeddings = response.data.map((data) => data.embedding);
    return {
      embeddings
    };
  }
  async tokenize(input) {
    const tokensCount = promptTokensEstimate({
      messages: input.map((msg) => ({
        role: msg.role,
        content: msg.text
      }))
    });
    return {
      tokensCount
    };
  }
  _prepareRequest(input, options) {
    return {
      ...this.parameters,
      model: this.modelId,
      stream: false,
      messages: input.map((message) => ({
        role: message.role,
        content: message.text
      })),
      response_format: (() => {
        if (options?.guided?.json) {
          const schema = isString(options.guided.json) ? JSON.parse(options.guided.json) : options.guided.json;
          if (getProp(schema, [
            "type"
          ]) === "object" && !getProp(schema, [
            "properties"
          ])) {
            schema.properties = {};
          }
          return {
            type: "json_schema",
            json_schema: {
              name: "schema",
              schema
            }
          };
        }
      })()
    };
  }
  async _generate(input, options, run) {
    const response = await this.client.chat.completions.create({
      ...this._prepareRequest(input, options),
      stream: false
    }, {
      signal: run.signal
    });
    return new OpenAIChatLLMOutput({
      id: response.id,
      model: response.model,
      created: response.created,
      usage: response.usage,
      service_tier: response.service_tier,
      system_fingerprint: response.system_fingerprint,
      choices: response.choices.map((choice) => ({
        delta: choice.message,
        index: 1,
        logprobs: choice.logprobs,
        finish_reason: choice.finish_reason
      }))
    });
  }
  async *_stream(input, options, run) {
    for await (const chunk of await this.client.chat.completions.create({
      ...this._prepareRequest(input, options),
      stream: true
    }, {
      signal: run.signal
    })) {
      yield new OpenAIChatLLMOutput(chunk);
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      parameters: shallowCopy(this.parameters),
      client: this.client
    };
  }
}

export { OpenAIChatLLM, OpenAIChatLLMOutput };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map