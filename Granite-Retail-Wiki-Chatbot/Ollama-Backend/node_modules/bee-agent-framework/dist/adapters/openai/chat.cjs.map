{"version":3,"sources":["../../../src/adapters/openai/chat.ts"],"names":["OpenAIChatLLMOutput","ChatLLMOutput","responses","constructor","response","register","messages","flatMap","choices","choice","BaseMessage","of","role","delta","text","content","getTextContent","map","msg","join","merge","other","push","toString","createSnapshot","shallowCopy","loadSnapshot","snapshot","Object","assign","OpenAIChatLLM","ChatLLM","emitter","Emitter","root","child","namespace","creator","client","parameters","modelId","executionOptions","cache","azure","AzureOpenAI","OpenAI","temperature","Serializer","toPlain","value","azureADTokenProvider","getPropStrict","apiVersion","deployment","fromPlain","options","meta","includes","tokenLimit","Infinity","embed","input","embeddings","create","model","signal","data","embedding","tokenize","tokensCount","promptTokensEstimate","_prepareRequest","stream","message","response_format","guided","json","schema","isString","JSON","parse","getProp","properties","type","json_schema","name","_generate","run","chat","completions","id","created","usage","service_tier","system_fingerprint","index","logprobs","finish_reason","_stream","chunk"],"mappings":";;;;;;;;;;;;;;AAmDO,MAAMA,4BAA4BC,sBAAAA,CAAAA;EAnDzC;;;AAoDkBC,EAAAA,SAAAA;AAEhBC,EAAAA,WAAAA,CAAYC,QAAoB,EAAA;AAC9B,IAAK,KAAA,EAAA;AACL,IAAA,IAAA,CAAKF,SAAY,GAAA;AAACE,MAAAA;;AACpB;EAEA;AACE,IAAA,IAAA,CAAKC,QAAQ,EAAA;AACf;AAEA,EAAA,IAAIC,QAAW,GAAA;AACb,IAAA,OAAO,IAAKJ,CAAAA,SAAAA,CACTK,OAAQ,CAAA,CAACH,QAAaA,KAAAA,QAAAA,CAASI,OAAO,CAAA,CACtCD,OAAQ,CAAA,CAACE,MACRC,KAAAA,uBAAAA,CAAYC,EAAG,CAAA;AACbC,MAAAA,IAAAA,EAAMH,OAAOI,KAAMD,CAAAA,IAAAA;AACnBE,MAAAA,IAAAA,EAAML,OAAOI,KAAME,CAAAA;AACrB,KAAA,CAAA,CAAA;AAEN;EAEAC,cAAyB,GAAA;AACvB,IAAO,OAAA,IAAA,CAAKV,SAASW,GAAI,CAAA,CAACC,QAAQA,GAAIJ,CAAAA,IAAI,CAAEK,CAAAA,IAAAA,CAAK,IAAA,CAAA;AACnD;AAEAC,EAAAA,KAAAA,CAAMC,KAAkC,EAAA;AACtC,IAAA,IAAA,CAAKnB,SAAUoB,CAAAA,IAAAA,CAAI,GAAID,KAAAA,CAAMnB,SAAS,CAAA;AACxC;EAEAqB,QAAmB,GAAA;AACjB,IAAA,OAAO,KAAKP,cAAc,EAAA;AAC5B;EAEAQ,cAAiB,GAAA;AACf,IAAO,OAAA;MACLtB,SAAWuB,EAAAA,qBAAAA,CAAY,KAAKvB,SAAS;AACvC,KAAA;AACF;AAEAwB,EAAAA,YAAAA,CAAaC,QAAwD,EAAA;AACnEC,IAAOC,MAAAA,CAAAA,MAAAA,CAAO,MAAMF,QAAAA,CAAAA;AACtB;AACF;AAgBO,MAAMG,sBAAsBC,gBAAAA,CAAAA;EA/GnC;;;EAgHkBC,OAAUC,GAAAA,mBAAAA,CAAQC,KAAKC,KAA2B,CAAA;IAChEC,SAAW,EAAA;AAAC,MAAA,QAAA;AAAU,MAAA;;IACtBC,OAAS,EAAA;GACX,CAAA;AAEgBC,EAAAA,MAAAA;AACAC,EAAAA,UAAAA;EAEhBpC,WAAY,CAAA,EAAEmC,MAAQE,EAAAA,OAAAA,EAASD,UAAYE,EAAAA,gBAAAA,GAAmB,EAAC,EAAGC,KAAOC,EAAAA,KAAAA,EAAiB,GAAA,EAAI,EAAA;AAC5F,IAAMH,KAAAA,CAAAA,OAAAA,IAAW,aAAeC,EAAAA,gBAAAA,EAAkBC,KAAAA,CAAAA;AAClD,IAAA,IAAIJ,MAAQ,EAAA;AACV,MAAA,IAAA,CAAKA,MAASA,GAAAA,MAAAA;AAChB,KAAA,MAAA,IAAWK,KAAO,EAAA;AAChB,MAAKL,IAAAA,CAAAA,MAAAA,GAAS,IAAIM,kBAAAA,EAAAA;KACb,MAAA;AACL,MAAKN,IAAAA,CAAAA,MAAAA,GAAS,IAAIO,aAAAA,EAAAA;AACpB;AACA,IAAA,IAAA,CAAKN,aAAaA,UAAc,IAAA;MAAEO,WAAa,EAAA;AAAE,KAAA;AACnD;EAEA;AACE,IAAA,IAAA,CAAKzC,QAAQ,EAAA;AACb0C,IAAAA,yBAAAA,CAAW1C,SAASuC,kBAAa,EAAA;AAC/BI,MAAAA,OAAAA,0BAAUC,KAAW,MAAA;QACnBC,oBAAsBC,EAAAA,wBAAAA,CAAcF,OAAO,uBAAA,CAAA;QAC3CG,UAAYD,EAAAA,wBAAAA,CAAcF,OAAO,YAAA,CAAA;QACjCI,UAAYF,EAAAA,wBAAAA,CAAcF,OAAO,aAAA;OAH1B,CAAA,EAAA,SAAA,CAAA;AAKTK,MAAAA,SAAAA,0BAAYL,KAAU,KAAA,IAAIL,kBAAYK,CAAAA,KAAAA,CAAMC,oBAAoB,CAArD,EAAA,WAAA;KACb,CAAA;AACAH,IAAAA,yBAAAA,CAAW1C,SAASwC,aAAQ,EAAA;AAC1BG,MAAAA,OAAAA,0BAAUC,KAAW,MAAA;QACnBM,OAASJ,EAAAA,wBAAAA,CAAcF,OAAO,UAAA;OADvB,CAAA,EAAA,SAAA,CAAA;AAGTK,MAAAA,SAAAA,0BAAYL,KAAU,KAAA,IAAIJ,aAAOI,CAAAA,KAAAA,CAAMM,OAAO,CAAnC,EAAA,WAAA;KACb,CAAA;AACF;AAEA,EAAA,MAAMC,IAAyB,GAAA;AAC7B,IAAA,IACE,KAAKhB,OAAQiB,CAAAA,QAAAA,CAAS,QAAA,CACtB,IAAA,IAAA,CAAKjB,QAAQiB,QAAS,CAAA,aAAA,KACtB,IAAKjB,CAAAA,OAAAA,CAAQiB,SAAS,oBAAA,CAAA,IACtB,KAAKjB,OAAQiB,CAAAA,QAAAA,CAAS,oBAAA,CACtB,EAAA;AACA,MAAO,OAAA;AAAEC,QAAAA,UAAAA,EAAY,GAAM,GAAA;AAAK,OAAA;AAClC,KAAA,MAAA,IAAW,IAAKlB,CAAAA,OAAAA,CAAQiB,QAAS,CAAA,OAAA,CAAU,EAAA;AACzC,MAAO,OAAA;AAAEC,QAAAA,UAAAA,EAAY,CAAI,GAAA;AAAK,OAAA;AAChC,KAAA,MAAA,IAAW,IAAKlB,CAAAA,OAAAA,CAAQiB,QAAS,CAAA,eAAA,CAAkB,EAAA;AACjD,MAAO,OAAA;AAAEC,QAAAA,UAAAA,EAAY,EAAK,GAAA;AAAK,OAAA;AACjC,KAAA,MAAA,IAAW,IAAKlB,CAAAA,OAAAA,CAAQiB,QAAS,CAAA,SAAA,CAAY,EAAA;AAC3C,MAAO,OAAA;AAAEC,QAAAA,UAAAA,EAAY,CAAI,GAAA;AAAK,OAAA;AAChC;AAEA,IAAO,OAAA;MACLA,UAAYC,EAAAA;AACd,KAAA;AACF;AAEA,EAAA,MAAMC,KACJC,CAAAA,KAAAA,EACAN,OAAkC,GAAA,EACR,EAAA;AAC1B,IAAA,MAAMnD,QAAW,GAAA,MAAM,IAAKkC,CAAAA,MAAAA,CAAOwB,WAAWC,MAC5C,CAAA;MACE,GAAGR,OAAAA;AACHS,MAAAA,KAAAA,EAAO,IAAKxB,CAAAA,OAAAA;MACZqB,KAAOA,EAAAA,KAAAA,CAAMtD,OAAQ,CAAA,CAACD,QAAaA,KAAAA,QAAAA,EAAUC,OAAQ,CAAA,CAACW,GAAQA,KAAAA,GAAAA,CAAIJ,IAAI;KAExE,EAAA;AAAEmD,MAAAA,MAAAA,EAAQV,OAASU,EAAAA;KAAO,CAAA;AAE5B,IAAA,MAAMH,aAAa1D,QAAS8D,CAAAA,IAAAA,CAAKjD,IAAI,CAACiD,IAAAA,KAASA,KAAKC,SAAS,CAAA;AAC7D,IAAO,OAAA;AAAEL,MAAAA;AAAW,KAAA;AACtB;AAEA,EAAA,MAAMM,SAASP,KAAsD,EAAA;AACnE,IAAA,MAAMQ,cAAcC,qCAAqB,CAAA;MACvChE,QAAUuD,EAAAA,KAAAA,CAAM5C,GACd,CAAA,CAACC,GACE,MAAA;AACCN,QAAAA,IAAAA,EAAMM,GAAIN,CAAAA,IAAAA;AACVG,QAAAA,OAAAA,EAASG,GAAIJ,CAAAA;OACf,CAAA;KAEN,CAAA;AAEA,IAAO,OAAA;AACLuD,MAAAA;AACF,KAAA;AACF;AAEUE,EAAAA,eAAAA,CACRV,OACAN,OAC4B,EAAA;AAM5B,IAAO,OAAA;AACL,MAAA,GAAG,IAAKhB,CAAAA,UAAAA;AACRyB,MAAAA,KAAAA,EAAO,IAAKxB,CAAAA,OAAAA;MACZgC,MAAQ,EAAA,KAAA;MACRlE,QAAUuD,EAAAA,KAAAA,CAAM5C,GACd,CAAA,CAACwD,OAA4B,MAAA;AAC3B7D,QAAAA,IAAAA,EAAM6D,OAAQ7D,CAAAA,IAAAA;AACdG,QAAAA,OAAAA,EAAS0D,OAAQ3D,CAAAA;OACnB,CAAA,CAAA;AAGF4D,MAAAA,eAAAA,EAAAA,CAAkB,MAAA;AAChB,QAAInB,IAAAA,OAAAA,EAASoB,QAAQC,IAAM,EAAA;AACzB,UAAA,MAAMC,MAASC,GAAAA,eAAAA,CAASvB,OAAQoB,CAAAA,MAAAA,CAAOC,IAAI,CAAA,GACvCG,IAAKC,CAAAA,KAAAA,CAAMzB,OAAQoB,CAAAA,MAAAA,CAAOC,IAAI,CAAA,GAC9BrB,QAAQoB,MAAOC,CAAAA,IAAAA;AAGnB,UAAA,IAAIK,mBAAQJ,MAAQ,EAAA;AAAC,YAAA;WAAO,CAAM,KAAA,QAAA,IAAY,CAACI,kBAAAA,CAAQJ,MAAQ,EAAA;AAAC,YAAA;WAAa,CAAG,EAAA;AAC9EA,YAAAA,MAAAA,CAAOK,aAAa,EAAC;AACvB;AAEA,UAAO,OAAA;YACLC,IAAM,EAAA,aAAA;YACNC,WAAa,EAAA;cACXC,IAAM,EAAA,QAAA;AACNR,cAAAA;AACF;AACF,WAAA;AACF;OACF;AACF,KAAA;AACF;EAEA,MAAgBS,SAAAA,CACdzB,KACAN,EAAAA,OAAAA,EACAgC,GAC8B,EAAA;AAC9B,IAAA,MAAMnF,WAAW,MAAM,IAAA,CAAKkC,MAAOkD,CAAAA,IAAAA,CAAKC,YAAY1B,MAClD,CAAA;MACE,GAAG,IAAA,CAAKQ,eAAgBV,CAAAA,KAAAA,EAAON,OAAAA,CAAAA;MAC/BiB,MAAQ,EAAA;KAEV,EAAA;AACEP,MAAAA,MAAAA,EAAQsB,GAAItB,CAAAA;KACd,CAAA;AAGF,IAAA,OAAO,IAAIjE,mBAAoB,CAAA;AAC7B0F,MAAAA,EAAAA,EAAItF,QAASsF,CAAAA,EAAAA;AACb1B,MAAAA,KAAAA,EAAO5D,QAAS4D,CAAAA,KAAAA;AAChB2B,MAAAA,OAAAA,EAASvF,QAASuF,CAAAA,OAAAA;AAClBC,MAAAA,KAAAA,EAAOxF,QAASwF,CAAAA,KAAAA;AAChBC,MAAAA,YAAAA,EAAczF,QAASyF,CAAAA,YAAAA;AACvBC,MAAAA,kBAAAA,EAAoB1F,QAAS0F,CAAAA,kBAAAA;AAC7BtF,MAAAA,OAAAA,EAASJ,QAASI,CAAAA,OAAAA,CAAQS,GACxB,CAAA,CAACR,MACE,MAAA;AACCI,QAAAA,KAAAA,EAAOJ,MAAOgE,CAAAA,OAAAA;QACdsB,KAAO,EAAA,CAAA;AACPC,QAAAA,QAAAA,EAAUvF,MAAOuF,CAAAA,QAAAA;AACjBC,QAAAA,aAAAA,EAAexF,MAAOwF,CAAAA;OACxB,CAAA;KAEN,CAAA;AACF;EAEA,OAAiBC,OAAAA,CACfrC,KACAN,EAAAA,OAAAA,EACAgC,GACkC,EAAA;AAClC,IAAA,WAAA,MAAiBY,SAAS,MAAM,IAAA,CAAK7D,MAAOkD,CAAAA,IAAAA,CAAKC,YAAY1B,MAC3D,CAAA;MACE,GAAG,IAAA,CAAKQ,eAAgBV,CAAAA,KAAAA,EAAON,OAAAA,CAAAA;MAC/BiB,MAAQ,EAAA;KAEV,EAAA;AACEP,MAAAA,MAAAA,EAAQsB,GAAItB,CAAAA;AACd,KAAA,CACC,EAAA;AACD,MAAM,MAAA,IAAIjE,oBAAoBmG,KAAAA,CAAAA;AAChC;AACF;EAEA3E,cAAiB,GAAA;AACf,IAAO,OAAA;AACL,MAAA,GAAG,MAAMA,cAAAA,EAAAA;MACTe,UAAYd,EAAAA,qBAAAA,CAAY,KAAKc,UAAU,CAAA;AACvCD,MAAAA,MAAAA,EAAQ,IAAKA,CAAAA;AACf,KAAA;AACF;AACF","file":"chat.cjs","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport {\n  AsyncStream,\n  BaseLLMTokenizeOutput,\n  EmbeddingOptions,\n  EmbeddingOutput,\n  ExecutionOptions,\n  GenerateOptions,\n  LLMCache,\n  LLMMeta,\n  StreamGenerateOptions,\n} from \"@/llms/base.js\";\nimport { shallowCopy } from \"@/serializer/utils.js\";\nimport { ChatLLM, ChatLLMGenerateEvents, ChatLLMOutput } from \"@/llms/chat.js\";\nimport { BaseMessage, RoleType } from \"@/llms/primitives/message.js\";\nimport { Emitter } from \"@/emitter/emitter.js\";\nimport { ClientOptions, OpenAI, AzureOpenAI } from \"openai\";\nimport { GetRunContext } from \"@/context.js\";\nimport { promptTokensEstimate } from \"openai-chat-tokens\";\nimport { Serializer } from \"@/serializer/serializer.js\";\nimport { getProp, getPropStrict } from \"@/internals/helpers/object.js\";\nimport { isString } from \"remeda\";\nimport type {\n  ChatCompletionChunk,\n  ChatCompletionCreateParams,\n  ChatCompletionMessageParam,\n  ChatCompletionSystemMessageParam,\n  ChatCompletionUserMessageParam,\n  ChatCompletionAssistantMessageParam,\n  ChatModel,\n  EmbeddingCreateParams,\n} from \"openai/resources/index\";\n\ntype Parameters = Omit<ChatCompletionCreateParams, \"stream\" | \"messages\" | \"model\">;\ntype Response = Omit<ChatCompletionChunk, \"object\">;\n\nexport class OpenAIChatLLMOutput extends ChatLLMOutput {\n  public readonly responses: Response[];\n\n  constructor(response: Response) {\n    super();\n    this.responses = [response];\n  }\n\n  static {\n    this.register();\n  }\n\n  get messages() {\n    return this.responses\n      .flatMap((response) => response.choices)\n      .flatMap((choice) =>\n        BaseMessage.of({\n          role: choice.delta.role as RoleType,\n          text: choice.delta.content!,\n        }),\n      );\n  }\n\n  getTextContent(): string {\n    return this.messages.map((msg) => msg.text).join(\"\\n\");\n  }\n\n  merge(other: OpenAIChatLLMOutput): void {\n    this.responses.push(...other.responses);\n  }\n\n  toString(): string {\n    return this.getTextContent();\n  }\n\n  createSnapshot() {\n    return {\n      responses: shallowCopy(this.responses),\n    };\n  }\n\n  loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void {\n    Object.assign(this, snapshot);\n  }\n}\n\ninterface Input {\n  modelId?: ChatModel;\n  client?: OpenAI | AzureOpenAI;\n  parameters?: Partial<Parameters>;\n  executionOptions?: ExecutionOptions;\n  cache?: LLMCache<OpenAIChatLLMOutput>;\n  azure?: boolean;\n}\n\nexport type OpenAIChatLLMEvents = ChatLLMGenerateEvents<OpenAIChatLLMOutput>;\nexport interface OpenAIEmbeddingOptions\n  extends EmbeddingOptions,\n    Omit<EmbeddingCreateParams, \"input\" | \"model\"> {}\n\nexport class OpenAIChatLLM extends ChatLLM<OpenAIChatLLMOutput> {\n  public readonly emitter = Emitter.root.child<OpenAIChatLLMEvents>({\n    namespace: [\"openai\", \"chat_llm\"],\n    creator: this,\n  });\n\n  public readonly client: OpenAI | AzureOpenAI;\n  public readonly parameters: Partial<Parameters>;\n\n  constructor({ client, modelId, parameters, executionOptions = {}, cache, azure }: Input = {}) {\n    super(modelId || \"gpt-4o-mini\", executionOptions, cache);\n    if (client) {\n      this.client = client;\n    } else if (azure) {\n      this.client = new AzureOpenAI();\n    } else {\n      this.client = new OpenAI();\n    }\n    this.parameters = parameters ?? { temperature: 0 };\n  }\n\n  static {\n    this.register();\n    Serializer.register(AzureOpenAI, {\n      toPlain: (value) => ({\n        azureADTokenProvider: getPropStrict(value, \"_azureADTokenProvider\"),\n        apiVersion: getPropStrict(value, \"apiVersion\"),\n        deployment: getPropStrict(value, \"_deployment\"),\n      }),\n      fromPlain: (value) => new AzureOpenAI(value.azureADTokenProvider),\n    });\n    Serializer.register(OpenAI, {\n      toPlain: (value) => ({\n        options: getPropStrict(value, \"_options\") as ClientOptions,\n      }),\n      fromPlain: (value) => new OpenAI(value.options),\n    });\n  }\n\n  async meta(): Promise<LLMMeta> {\n    if (\n      this.modelId.includes(\"gpt-4o\") ||\n      this.modelId.includes(\"gpt-4-turbo\") ||\n      this.modelId.includes(\"gpt-4-0125-preview\") ||\n      this.modelId.includes(\"gpt-4-1106-preview\")\n    ) {\n      return { tokenLimit: 128 * 1024 };\n    } else if (this.modelId.includes(\"gpt-4\")) {\n      return { tokenLimit: 8 * 1024 };\n    } else if (this.modelId.includes(\"gpt-3.5-turbo\")) {\n      return { tokenLimit: 16 * 1024 };\n    } else if (this.modelId.includes(\"gpt-3.5\")) {\n      return { tokenLimit: 8 * 1024 };\n    }\n\n    return {\n      tokenLimit: Infinity,\n    };\n  }\n\n  async embed(\n    input: BaseMessage[][],\n    options: OpenAIEmbeddingOptions = {},\n  ): Promise<EmbeddingOutput> {\n    const response = await this.client.embeddings.create(\n      {\n        ...options,\n        model: this.modelId,\n        input: input.flatMap((messages) => messages).flatMap((msg) => msg.text),\n      },\n      { signal: options?.signal },\n    );\n    const embeddings = response.data.map((data) => data.embedding);\n    return { embeddings };\n  }\n\n  async tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput> {\n    const tokensCount = promptTokensEstimate({\n      messages: input.map(\n        (msg) =>\n          ({\n            role: msg.role,\n            content: msg.text,\n          }) as ChatCompletionMessageParam,\n      ),\n    });\n\n    return {\n      tokensCount,\n    };\n  }\n\n  protected _prepareRequest(\n    input: BaseMessage[],\n    options?: GenerateOptions,\n  ): ChatCompletionCreateParams {\n    type OpenAIMessage =\n      | ChatCompletionSystemMessageParam\n      | ChatCompletionUserMessageParam\n      | ChatCompletionAssistantMessageParam;\n\n    return {\n      ...this.parameters,\n      model: this.modelId,\n      stream: false,\n      messages: input.map(\n        (message): OpenAIMessage => ({\n          role: message.role as OpenAIMessage[\"role\"],\n          content: message.text,\n        }),\n      ),\n\n      response_format: (() => {\n        if (options?.guided?.json) {\n          const schema = isString(options.guided.json)\n            ? JSON.parse(options.guided.json)\n            : options.guided.json;\n\n          // OpenAI requires that 'properties' must be defined when type equals 'object'\n          if (getProp(schema, [\"type\"]) === \"object\" && !getProp(schema, [\"properties\"])) {\n            schema.properties = {};\n          }\n\n          return {\n            type: \"json_schema\",\n            json_schema: {\n              name: \"schema\",\n              schema,\n            },\n          };\n        }\n      })(),\n    };\n  }\n\n  protected async _generate(\n    input: BaseMessage[],\n    options: Partial<GenerateOptions>,\n    run: GetRunContext<typeof this>,\n  ): Promise<OpenAIChatLLMOutput> {\n    const response = await this.client.chat.completions.create(\n      {\n        ...this._prepareRequest(input, options),\n        stream: false,\n      },\n      {\n        signal: run.signal,\n      },\n    );\n\n    return new OpenAIChatLLMOutput({\n      id: response.id,\n      model: response.model,\n      created: response.created,\n      usage: response.usage,\n      service_tier: response.service_tier,\n      system_fingerprint: response.system_fingerprint,\n      choices: response.choices.map(\n        (choice) =>\n          ({\n            delta: choice.message,\n            index: 1,\n            logprobs: choice.logprobs,\n            finish_reason: choice.finish_reason,\n          }) as ChatCompletionChunk.Choice,\n      ),\n    });\n  }\n\n  protected async *_stream(\n    input: BaseMessage[],\n    options: StreamGenerateOptions | undefined,\n    run: GetRunContext<typeof this>,\n  ): AsyncStream<OpenAIChatLLMOutput> {\n    for await (const chunk of await this.client.chat.completions.create(\n      {\n        ...this._prepareRequest(input, options),\n        stream: true,\n      },\n      {\n        signal: run.signal,\n      },\n    )) {\n      yield new OpenAIChatLLMOutput(chunk);\n    }\n  }\n\n  createSnapshot() {\n    return {\n      ...super.createSnapshot(),\n      parameters: shallowCopy(this.parameters),\n      client: this.client,\n    };\n  }\n}\n"]}