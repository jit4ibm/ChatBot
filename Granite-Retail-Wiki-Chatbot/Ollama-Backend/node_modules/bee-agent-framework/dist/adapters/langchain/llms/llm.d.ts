import { BaseLLMOutput, LLMMeta, ExecutionOptions, LLMCache, EmbeddingOptions, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, BaseLLMEvents } from '../../../llms/base.js';
import { LLMEvents, LLM, LLMInput } from '../../../llms/llm.js';
import { BaseLLM } from '@langchain/core/language_models/llms';
import { E as Emitter } from '../../../emitter-DRfJC1TP.js';
import { GetRunContext } from '../../../context.js';
import '../../../errors.js';
import '../../../internals/types.js';
import '../../../internals/helpers/guards.js';
import '../../../internals/serializable.js';
import '../../../cache/base.js';
import 'promise-based-task';
import '../../../internals/helpers/promise.js';

declare class LangChainLLMOutput extends BaseLLMOutput {
    text: string;
    readonly meta: Record<string, any>;
    constructor(text: string, meta: Record<string, any>);
    merge(other: LangChainLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        text: string;
        meta: Record<string, any>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
type LangChainLLMEvents = LLMEvents<LangChainLLMOutput>;
declare class LangChainLLM extends LLM<LangChainLLMOutput> {
    readonly lcLLM: BaseLLM;
    private modelMeta?;
    readonly emitter: Emitter<LangChainLLMEvents>;
    protected readonly parameters: any;
    constructor(lcLLM: BaseLLM, modelMeta?: LLMMeta | undefined, executionOptions?: ExecutionOptions, cache?: LLMCache<LangChainLLMOutput>);
    meta(): Promise<LLMMeta>;
    embed(input: LLMInput[], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: LLMInput): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: LLMInput, _options: Partial<GenerateOptions>, run: GetRunContext<this>): Promise<LangChainLLMOutput>;
    protected _stream(input: string, _options: StreamGenerateOptions | undefined, run: GetRunContext<this>): AsyncStream<LangChainLLMOutput>;
    createSnapshot(): {
        modelId: string;
        modelMeta: LLMMeta | undefined;
        parameters: any;
        executionOptions: ExecutionOptions;
        lcLLM: string;
        emitter: Emitter<BaseLLMEvents<unknown, LangChainLLMOutput>>;
        cache: LLMCache<LangChainLLMOutput>;
    };
    loadSnapshot({ lcLLM, ...state }: ReturnType<typeof this.createSnapshot>): Promise<void>;
}

export { LangChainLLM, type LangChainLLMEvents, LangChainLLMOutput };
