'use strict';

var llm_cjs = require('../../../llms/llm.cjs');
var base_cjs = require('../../../llms/base.cjs');
var load = require('@langchain/core/load');
var object_cjs = require('../../../internals/helpers/object.cjs');
var utils_cjs = require('../../../serializer/utils.cjs');
var emitter_cjs = require('../../../emitter/emitter.cjs');
var errors_cjs = require('../../../errors.cjs');

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class LangChainLLMOutput extends base_cjs.BaseLLMOutput {
  static {
    __name(this, "LangChainLLMOutput");
  }
  text;
  meta;
  constructor(text, meta) {
    super(), this.text = text, this.meta = meta;
  }
  static {
    this.register();
  }
  merge(other) {
    this.text += other.text;
    object_cjs.assign(this.meta, other.meta);
  }
  getTextContent() {
    return this.text;
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      text: this.text,
      meta: utils_cjs.shallowCopy(this.meta)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class LangChainLLM extends llm_cjs.LLM {
  static {
    __name(this, "LangChainLLM");
  }
  lcLLM;
  modelMeta;
  emitter;
  parameters;
  constructor(lcLLM, modelMeta, executionOptions, cache) {
    super(lcLLM._modelType(), executionOptions, cache), this.lcLLM = lcLLM, this.modelMeta = modelMeta, this.emitter = emitter_cjs.Emitter.root.child({
      namespace: [
        "langchain",
        "llm"
      ],
      creator: this
    });
    this.parameters = lcLLM.invocationParams();
  }
  static {
    this.register();
  }
  async meta() {
    if (this.modelMeta) {
      return this.modelMeta;
    }
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new errors_cjs.NotImplementedError();
  }
  async tokenize(input) {
    return {
      tokensCount: await this.lcLLM.getNumTokens(input)
    };
  }
  async _generate(input, _options, run) {
    const { generations } = await this.lcLLM.generate([
      input
    ], {
      signal: run.signal
    });
    return new LangChainLLMOutput(generations[0][0].text, generations[0][0].generationInfo || {});
  }
  async *_stream(input, _options, run) {
    const response = this.lcLLM._streamResponseChunks(input, {
      signal: run.signal
    });
    for await (const chunk of response) {
      yield new LangChainLLMOutput(chunk.text, chunk.generationInfo || {});
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      modelMeta: this.modelMeta,
      parameters: utils_cjs.shallowCopy(this.parameters),
      executionOptions: utils_cjs.shallowCopy(this.executionOptions),
      lcLLM: JSON.stringify(this.lcLLM.toJSON())
    };
  }
  async loadSnapshot({ lcLLM, ...state }) {
    super.loadSnapshot(state);
    Object.assign(this, state, {
      lcLLM: await (async () => {
        if (lcLLM.includes("@ibm-generative-ai/node-sdk")) {
          const { GenAIModel } = await import('@ibm-generative-ai/node-sdk/langchain');
          return GenAIModel.fromJSON(lcLLM);
        }
        return await load.load(lcLLM);
      })()
    });
  }
}

exports.LangChainLLM = LangChainLLM;
exports.LangChainLLMOutput = LangChainLLMOutput;
//# sourceMappingURL=llm.cjs.map
//# sourceMappingURL=llm.cjs.map