import { LLMOutputError } from '../../llms/base.js';
import { shallowCopy } from '../../serializer/utils.js';
import { ChatLLMOutput, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { Emitter } from '../../emitter/emitter.js';
import { Ollama } from 'ollama';
import { signalRace } from '../../internals/helpers/promise.js';
import { Cache } from '../../cache/decoratorCache.js';
import { customMerge, getPropStrict } from '../../internals/helpers/object.js';
import { safeSum } from '../../internals/helpers/number.js';
import { registerClient, extractModelMeta, retrieveVersion, retrieveFormat } from './shared.js';
import { getEnv } from '../../internals/env.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class OllamaChatLLMOutput extends ChatLLMOutput {
  static {
    __name(this, "OllamaChatLLMOutput");
  }
  results;
  constructor(response) {
    super();
    this.results = [
      response
    ];
  }
  static {
    this.register();
  }
  get messages() {
    return this.results.flatMap((response) => BaseMessage.of({
      role: response.message.role,
      text: response.message.content
    }));
  }
  getTextContent() {
    return this.finalResult.message.content;
  }
  get finalResult() {
    if (this.results.length === 0) {
      throw new LLMOutputError("No chunks to get final result from!");
    }
    return customMerge(this.results, {
      message: /* @__PURE__ */ __name((value, oldValue) => ({
        role: value.role ?? oldValue.role,
        content: `${oldValue?.content ?? ""}${value?.content ?? ""}`,
        images: [
          ...oldValue?.images ?? [],
          ...value?.images ?? []
        ],
        tool_calls: [
          ...oldValue?.tool_calls ?? [],
          ...value?.tool_calls ?? []
        ]
      }), "message"),
      total_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "total_duration"),
      load_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "load_duration"),
      model: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "model"),
      done: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done"),
      done_reason: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done_reason"),
      created_at: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "created_at"),
      eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "eval_duration"),
      prompt_eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "prompt_eval_duration"),
      prompt_eval_count: safeSum,
      eval_count: safeSum
    });
  }
  merge(other) {
    Cache.getInstance(this, "finalResult").clear();
    this.results.push(...other.results);
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      results: shallowCopy(this.results)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", typeof Readonly === "undefined" ? Object : Readonly),
  _ts_metadata("design:paramtypes", [])
], OllamaChatLLMOutput.prototype, "finalResult", null);
class OllamaChatLLM extends ChatLLM {
  static {
    __name(this, "OllamaChatLLM");
  }
  emitter = Emitter.root.child({
    namespace: [
      "ollama",
      "chat_llm"
    ],
    creator: this
  });
  client;
  parameters;
  constructor({ client, modelId, parameters, executionOptions = {}, cache } = {
    modelId: "llama3.1"
  }) {
    super(modelId, executionOptions, cache);
    this.client = client ?? new Ollama({
      fetch,
      host: getEnv("OLLAMA_HOST")
    });
    this.parameters = parameters ?? {
      temperature: 0,
      repeat_penalty: 1,
      num_predict: 2048
    };
  }
  static {
    this.register();
    registerClient();
  }
  async meta() {
    const model = await this.client.show({
      model: this.modelId
    });
    return extractModelMeta(model);
  }
  async embed(input, options = {}) {
    const response = await this.client.embed({
      model: this.modelId,
      input: input.flatMap((messages) => messages).flatMap((msg) => msg.text),
      options: options?.options,
      truncate: options?.truncate
    });
    return {
      embeddings: response.embeddings
    };
  }
  async tokenize(input) {
    const contentLength = input.reduce((acc, msg) => acc + msg.text.length, 0);
    return {
      tokensCount: Math.ceil(contentLength / 4)
    };
  }
  async version() {
    const config = getPropStrict(this.client, "config");
    return retrieveVersion(config.host, config.fetch);
  }
  async _generate(input, options, run) {
    const response = await signalRace(async () => this.client.chat({
      ...await this.prepareParameters(input, options),
      stream: false
    }), run.signal, () => this.client.abort());
    return new OllamaChatLLMOutput(response);
  }
  async *_stream(input, options, run) {
    for await (const chunk of await this.client.chat({
      ...await this.prepareParameters(input, options),
      stream: true
    })) {
      if (run.signal.aborted) {
        break;
      }
      yield new OllamaChatLLMOutput(chunk);
    }
    run.signal.throwIfAborted();
  }
  async prepareParameters(input, overrides) {
    return {
      model: this.modelId,
      messages: input.map((msg) => ({
        role: msg.role,
        content: msg.text
      })),
      options: this.parameters,
      format: retrieveFormat(await this.version(), overrides?.guided)
    };
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      parameters: shallowCopy(this.parameters),
      executionOptions: shallowCopy(this.executionOptions),
      client: this.client
    };
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", Function),
  _ts_metadata("design:paramtypes", []),
  _ts_metadata("design:returntype", Promise)
], OllamaChatLLM.prototype, "version", null);

export { OllamaChatLLM, OllamaChatLLMOutput };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map