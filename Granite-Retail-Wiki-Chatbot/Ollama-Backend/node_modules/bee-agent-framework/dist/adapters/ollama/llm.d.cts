import { EmbeddingOptions, BaseLLMOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, LLMMeta, EmbeddingOutput, BaseLLMTokenizeOutput, ExecutionOptions, BaseLLMEvents, LLMCache } from '../../llms/base.cjs';
import { LLMEvents, LLM, LLMInput } from '../../llms/llm.cjs';
import { E as Emitter } from '../../emitter-DdThRYHg.cjs';
import { Options, GenerateResponse, Ollama, GenerateRequest } from 'ollama';
import { GetRunContext } from '../../context.cjs';
import '../../errors.cjs';
import '../../internals/types.cjs';
import '../../internals/helpers/guards.cjs';
import '../../internals/serializable.cjs';
import '../../cache/base.cjs';
import 'promise-based-task';
import '../../internals/helpers/promise.cjs';

interface Input {
    modelId: string;
    client?: Ollama;
    parameters?: Partial<Options>;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<OllamaLLMOutput>;
}
interface OllamaEmbeddingOptions extends EmbeddingOptions {
    options?: Partial<Options>;
    truncate?: boolean;
}
declare class OllamaLLMOutput extends BaseLLMOutput {
    readonly results: GenerateResponse[];
    constructor(result: GenerateResponse);
    getTextContent(): string;
    get finalResult(): Readonly<GenerateResponse>;
    merge(other: OllamaLLMOutput): void;
    createSnapshot(): {
        results: GenerateResponse[];
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
    toString(): string;
}
type OllamaLLMEvents = LLMEvents<OllamaLLMOutput>;
declare class OllamaLLM extends LLM<OllamaLLMOutput> {
    readonly emitter: Emitter<OllamaLLMEvents>;
    readonly client: Ollama;
    readonly parameters: Partial<Options>;
    constructor({ client, modelId, parameters, executionOptions, cache }: Input);
    protected _generate(input: LLMInput, options: GenerateOptions, run: GetRunContext<typeof this>): Promise<OllamaLLMOutput>;
    protected _stream(input: LLMInput, options: Partial<StreamGenerateOptions>, run: GetRunContext<typeof this>): AsyncStream<OllamaLLMOutput, void>;
    version(): Promise<string>;
    meta(): Promise<LLMMeta>;
    embed(input: LLMInput[], options?: OllamaEmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: LLMInput): Promise<BaseLLMTokenizeOutput>;
    protected prepareParameters(input: LLMInput, overrides?: GenerateOptions): Promise<GenerateRequest>;
    createSnapshot(): {
        modelId: string;
        executionOptions: ExecutionOptions;
        parameters: Partial<Options>;
        client: Ollama;
        emitter: Emitter<BaseLLMEvents<unknown, OllamaLLMOutput>>;
        cache: LLMCache<OllamaLLMOutput>;
    };
}

export { type OllamaEmbeddingOptions, OllamaLLM, type OllamaLLMEvents, OllamaLLMOutput };
