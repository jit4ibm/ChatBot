'use strict';

var base_cjs = require('../../llms/base.cjs');
var utils_cjs = require('../../serializer/utils.cjs');
var chat_cjs = require('../../llms/chat.cjs');
var message_cjs = require('../../llms/primitives/message.cjs');
var emitter_cjs = require('../../emitter/emitter.cjs');
var ollama = require('ollama');
var promise_cjs = require('../../internals/helpers/promise.cjs');
var decoratorCache_cjs = require('../../cache/decoratorCache.cjs');
var object_cjs = require('../../internals/helpers/object.cjs');
var number_cjs = require('../../internals/helpers/number.cjs');
var shared_cjs = require('./shared.cjs');
var env_cjs = require('../../internals/env.cjs');

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class OllamaChatLLMOutput extends chat_cjs.ChatLLMOutput {
  static {
    __name(this, "OllamaChatLLMOutput");
  }
  results;
  constructor(response) {
    super();
    this.results = [
      response
    ];
  }
  static {
    this.register();
  }
  get messages() {
    return this.results.flatMap((response) => message_cjs.BaseMessage.of({
      role: response.message.role,
      text: response.message.content
    }));
  }
  getTextContent() {
    return this.finalResult.message.content;
  }
  get finalResult() {
    if (this.results.length === 0) {
      throw new base_cjs.LLMOutputError("No chunks to get final result from!");
    }
    return object_cjs.customMerge(this.results, {
      message: /* @__PURE__ */ __name((value, oldValue) => ({
        role: value.role ?? oldValue.role,
        content: `${oldValue?.content ?? ""}${value?.content ?? ""}`,
        images: [
          ...oldValue?.images ?? [],
          ...value?.images ?? []
        ],
        tool_calls: [
          ...oldValue?.tool_calls ?? [],
          ...value?.tool_calls ?? []
        ]
      }), "message"),
      total_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "total_duration"),
      load_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "load_duration"),
      model: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "model"),
      done: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done"),
      done_reason: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done_reason"),
      created_at: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "created_at"),
      eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "eval_duration"),
      prompt_eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "prompt_eval_duration"),
      prompt_eval_count: number_cjs.safeSum,
      eval_count: number_cjs.safeSum
    });
  }
  merge(other) {
    decoratorCache_cjs.Cache.getInstance(this, "finalResult").clear();
    this.results.push(...other.results);
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      results: utils_cjs.shallowCopy(this.results)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
_ts_decorate([
  decoratorCache_cjs.Cache(),
  _ts_metadata("design:type", typeof Readonly === "undefined" ? Object : Readonly),
  _ts_metadata("design:paramtypes", [])
], OllamaChatLLMOutput.prototype, "finalResult", null);
class OllamaChatLLM extends chat_cjs.ChatLLM {
  static {
    __name(this, "OllamaChatLLM");
  }
  emitter = emitter_cjs.Emitter.root.child({
    namespace: [
      "ollama",
      "chat_llm"
    ],
    creator: this
  });
  client;
  parameters;
  constructor({ client, modelId, parameters, executionOptions = {}, cache } = {
    modelId: "llama3.1"
  }) {
    super(modelId, executionOptions, cache);
    this.client = client ?? new ollama.Ollama({
      fetch,
      host: env_cjs.getEnv("OLLAMA_HOST")
    });
    this.parameters = parameters ?? {
      temperature: 0,
      repeat_penalty: 1,
      num_predict: 2048
    };
  }
  static {
    this.register();
    shared_cjs.registerClient();
  }
  async meta() {
    const model = await this.client.show({
      model: this.modelId
    });
    return shared_cjs.extractModelMeta(model);
  }
  async embed(input, options = {}) {
    const response = await this.client.embed({
      model: this.modelId,
      input: input.flatMap((messages) => messages).flatMap((msg) => msg.text),
      options: options?.options,
      truncate: options?.truncate
    });
    return {
      embeddings: response.embeddings
    };
  }
  async tokenize(input) {
    const contentLength = input.reduce((acc, msg) => acc + msg.text.length, 0);
    return {
      tokensCount: Math.ceil(contentLength / 4)
    };
  }
  async version() {
    const config = object_cjs.getPropStrict(this.client, "config");
    return shared_cjs.retrieveVersion(config.host, config.fetch);
  }
  async _generate(input, options, run) {
    const response = await promise_cjs.signalRace(async () => this.client.chat({
      ...await this.prepareParameters(input, options),
      stream: false
    }), run.signal, () => this.client.abort());
    return new OllamaChatLLMOutput(response);
  }
  async *_stream(input, options, run) {
    for await (const chunk of await this.client.chat({
      ...await this.prepareParameters(input, options),
      stream: true
    })) {
      if (run.signal.aborted) {
        break;
      }
      yield new OllamaChatLLMOutput(chunk);
    }
    run.signal.throwIfAborted();
  }
  async prepareParameters(input, overrides) {
    return {
      model: this.modelId,
      messages: input.map((msg) => ({
        role: msg.role,
        content: msg.text
      })),
      options: this.parameters,
      format: shared_cjs.retrieveFormat(await this.version(), overrides?.guided)
    };
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      parameters: utils_cjs.shallowCopy(this.parameters),
      executionOptions: utils_cjs.shallowCopy(this.executionOptions),
      client: this.client
    };
  }
}
_ts_decorate([
  decoratorCache_cjs.Cache(),
  _ts_metadata("design:type", Function),
  _ts_metadata("design:paramtypes", []),
  _ts_metadata("design:returntype", Promise)
], OllamaChatLLM.prototype, "version", null);

exports.OllamaChatLLM = OllamaChatLLM;
exports.OllamaChatLLMOutput = OllamaChatLLMOutput;
//# sourceMappingURL=chat.cjs.map
//# sourceMappingURL=chat.cjs.map