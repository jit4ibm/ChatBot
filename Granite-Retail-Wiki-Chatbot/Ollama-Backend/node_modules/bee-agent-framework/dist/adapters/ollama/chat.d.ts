import { LLMMeta, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, ExecutionOptions, BaseLLMEvents, LLMCache } from '../../llms/base.js';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { E as Emitter } from '../../emitter-DRfJC1TP.js';
import { ChatResponse, Ollama, Options, ChatRequest } from 'ollama';
import { GetRunContext } from '../../context.js';
import { OllamaEmbeddingOptions } from './llm.js';
import '../../errors.js';
import '../../internals/types.js';
import '../../internals/helpers/guards.js';
import '../../internals/serializable.js';
import '../../cache/base.js';
import 'promise-based-task';
import '../../internals/helpers/promise.js';
import '../../llms/llm.js';

declare class OllamaChatLLMOutput extends ChatLLMOutput {
    readonly results: ChatResponse[];
    constructor(response: ChatResponse);
    get messages(): BaseMessage[];
    getTextContent(): string;
    get finalResult(): Readonly<ChatResponse>;
    merge(other: OllamaChatLLMOutput): void;
    toString(): string;
    createSnapshot(): {
        results: ChatResponse[];
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface Input {
    modelId: string;
    client?: Ollama;
    parameters?: Partial<Options>;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<OllamaChatLLMOutput>;
}
type OllamaChatLLMEvents = ChatLLMGenerateEvents<OllamaChatLLMOutput>;
declare class OllamaChatLLM extends ChatLLM<OllamaChatLLMOutput> {
    readonly emitter: Emitter<OllamaChatLLMEvents>;
    readonly client: Ollama;
    readonly parameters: Partial<Options>;
    constructor({ client, modelId, parameters, executionOptions, cache }?: Input);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: OllamaEmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    version(): Promise<string>;
    protected _generate(input: BaseMessage[], options: GenerateOptions, run: GetRunContext<typeof this>): Promise<OllamaChatLLMOutput>;
    protected _stream(input: BaseMessage[], options: Partial<StreamGenerateOptions>, run: GetRunContext<typeof this>): AsyncStream<OllamaChatLLMOutput>;
    protected prepareParameters(input: BaseMessage[], overrides?: GenerateOptions): Promise<ChatRequest>;
    createSnapshot(): {
        modelId: string;
        parameters: Partial<Options>;
        executionOptions: ExecutionOptions;
        client: Ollama;
        emitter: Emitter<BaseLLMEvents<unknown, OllamaChatLLMOutput>>;
        cache: LLMCache<OllamaChatLLMOutput>;
    };
}

export { OllamaChatLLM, type OllamaChatLLMEvents, OllamaChatLLMOutput };
