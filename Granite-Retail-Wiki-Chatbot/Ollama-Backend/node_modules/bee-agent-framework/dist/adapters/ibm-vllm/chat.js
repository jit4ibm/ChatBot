import { isFunction, isObjectType } from 'remeda';
import { IBMvLLM } from './llm.js';
import { Cache } from '../../cache/decoratorCache.js';
import { BaseMessage, Role } from '../../llms/primitives/message.js';
import { Emitter } from '../../emitter/emitter.js';
import { ChatLLMOutput, ChatLLM } from '../../llms/chat.js';
import { LLMError } from '../../llms/base.js';
import { transformAsyncIterable } from '../../internals/helpers/stream.js';
import { shallowCopy } from '../../serializer/utils.js';
import { IBMVllmChatLLMPreset } from './chatPreset.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class GrpcChatLLMOutput extends ChatLLMOutput {
  static {
    __name(this, "GrpcChatLLMOutput");
  }
  raw;
  constructor(rawOutput) {
    super();
    this.raw = rawOutput;
  }
  get messages() {
    const text = this.raw.getTextContent();
    return [
      BaseMessage.of({
        role: Role.ASSISTANT,
        text,
        meta: this.raw.meta
      })
    ];
  }
  merge(other) {
    Cache.getInstance(this, "messages").clear();
    this.raw.merge(other.raw);
  }
  getTextContent() {
    const [message] = this.messages;
    return message.text;
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      raw: shallowCopy(this.raw)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", Array),
  _ts_metadata("design:paramtypes", [])
], GrpcChatLLMOutput.prototype, "messages", null);
class IBMVllmChatLLM extends ChatLLM {
  static {
    __name(this, "IBMVllmChatLLM");
  }
  emitter = new Emitter({
    namespace: [
      "ibm_vllm",
      "chat_llm"
    ],
    creator: this
  });
  llm;
  config;
  constructor({ llm, config, cache }) {
    super(llm.modelId, llm.executionOptions, cache);
    this.llm = llm;
    this.config = config;
  }
  static {
    this.register();
  }
  async meta() {
    return this.llm.meta();
  }
  async embed(input, options) {
    const inputs = input.map((messages) => this.messagesToPrompt(messages));
    return this.llm.embed(inputs, options);
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      executionOptions: this.executionOptions,
      llm: this.llm,
      config: shallowCopy(this.config)
    };
  }
  async tokenize(messages) {
    const prompt = this.messagesToPrompt(messages);
    return this.llm.tokenize(prompt);
  }
  async _generate(messages, options, run) {
    const prompt = this.messagesToPrompt(messages);
    const rawResponse = await this.llm._generate(prompt, options, run);
    return new GrpcChatLLMOutput(rawResponse);
  }
  async *_stream(messages, options, run) {
    const prompt = this.messagesToPrompt(messages);
    const response = this.llm._stream(prompt, options, run);
    return yield* transformAsyncIterable(response, (output) => new GrpcChatLLMOutput(output));
  }
  messagesToPrompt(messages) {
    return this.config.messagesToPrompt(messages);
  }
  static fromPreset(modelId, overrides) {
    const presetFactory = IBMVllmChatLLMPreset[modelId];
    if (!presetFactory) {
      throw new LLMError(`Model "${modelId}" does not exist in preset.`);
    }
    const preset = presetFactory();
    let parameters = preset.base.parameters ?? {};
    if (overrides) {
      if (isFunction(overrides.parameters)) {
        parameters = overrides.parameters(parameters);
      } else if (isObjectType(overrides.parameters)) {
        parameters = overrides.parameters;
      }
    }
    return new IBMVllmChatLLM({
      config: preset.chat,
      llm: new IBMvLLM({
        ...preset.base,
        ...overrides,
        parameters,
        modelId
      })
    });
  }
}

export { GrpcChatLLMOutput, IBMVllmChatLLM };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map