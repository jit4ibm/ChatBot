{"version":3,"sources":["../../../src/adapters/ibm-vllm/chatPreset.ts"],"names":["IBMVllmModel","LLAMA_3_3_70B_INSTRUCT","LLAMA_3_1_405B_INSTRUCT_FP8","LLAMA_3_1_70B_INSTRUCT","LLAMA_3_1_8B_INSTRUCT","GRANITE_3_1_8B_INSTRUCT","IBMVllmChatLLMPreset","template","parameters","messagesToPrompt","LLMChatTemplates","get","base","modelId","method","stopping","stop_sequences","stop_sequence","include_stop_sequence","max_new_tokens","decoding","repetition_penalty","chat"],"mappings":";;;;AAyBO,MAAMA,YAAe,GAAA;EAC1BC,sBAAwB,EAAA,mCAAA;EACxBC,2BAA6B,EAAA,wCAAA;EAC7BC,sBAAwB,EAAA,mCAAA;EACxBC,qBAAuB,EAAA,kCAAA;EACvBC,uBAAyB,EAAA;AAC3B;AAGO,MAAMC,oBAAuB,GAAA;EAClC,CAACN,YAAAA,CAAaC,sBAAsB,GAAG,MAAA;AACrC,IAAA,MAAM,EAAEM,QAAUC,EAAAA,UAAAA,EAAYC,kBAAqBC,GAAAA,qCAAAA,CAAiBC,IAAI,UAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;AACJC,QAAAA,OAAAA,EAASb,YAAaC,CAAAA,sBAAAA;QACtBO,UAAY,EAAA;UACVM,MAAQ,EAAA,QAAA;UACRC,QAAU,EAAA;YACRC,cAAgB,EAAA;iBAAIR,UAAWS,CAAAA;;YAC/BC,qBAAuB,EAAA,KAAA;YACvBC,cAAgB,EAAA;AAClB,WAAA;UACAC,QAAU,EAAA;YACRC,kBAAoB,EAAA;AACtB;AACF;AACF,OAAA;MACAC,IAAM,EAAA;AACJb,QAAAA,gBAAAA,EAAkBA,iBAAiBF,QAAAA;AACrC;AACF,KAAA;AACF,GAAA;EACA,CAACP,YAAAA,CAAaE,2BAA2B,GAAG,MAAA;AAC1C,IAAA,MAAM,EAAEK,QAAUC,EAAAA,UAAAA,EAAYC,kBAAqBC,GAAAA,qCAAAA,CAAiBC,IAAI,UAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;AACJC,QAAAA,OAAAA,EAASb,YAAaG,CAAAA,sBAAAA;QACtBK,UAAY,EAAA;UACVM,MAAQ,EAAA,QAAA;UACRC,QAAU,EAAA;YACRC,cAAgB,EAAA;iBAAIR,UAAWS,CAAAA;;YAC/BC,qBAAuB,EAAA,KAAA;YACvBC,cAAgB,EAAA;AAClB,WAAA;UACAC,QAAU,EAAA;YACRC,kBAAoB,EAAA;AACtB;AACF;AACF,OAAA;MACAC,IAAM,EAAA;AACJb,QAAAA,gBAAAA,EAAkBA,iBAAiBF,QAAAA;AACrC;AACF,KAAA;AACF,GAAA;EACA,CAACP,YAAAA,CAAaG,sBAAsB,GAAG,MAAA;AACrC,IAAA,MAAM,EAAEI,QAAUC,EAAAA,UAAAA,EAAYC,kBAAqBC,GAAAA,qCAAAA,CAAiBC,IAAI,UAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;AACJC,QAAAA,OAAAA,EAASb,YAAaG,CAAAA,sBAAAA;QACtBK,UAAY,EAAA;UACVM,MAAQ,EAAA,QAAA;UACRC,QAAU,EAAA;YACRC,cAAgB,EAAA;iBAAIR,UAAWS,CAAAA;;YAC/BC,qBAAuB,EAAA,KAAA;YACvBC,cAAgB,EAAA;AAClB,WAAA;UACAC,QAAU,EAAA;YACRC,kBAAoB,EAAA;AACtB;AACF;AACF,OAAA;MACAC,IAAM,EAAA;AACJb,QAAAA,gBAAAA,EAAkBA,iBAAiBF,QAAAA;AACrC;AACF,KAAA;AACF,GAAA;EACA,CAACP,YAAAA,CAAaI,qBAAqB,GAAG,MAAA;AACpC,IAAA,MAAM,EAAEG,QAAUC,EAAAA,UAAAA,EAAYC,kBAAqBC,GAAAA,qCAAAA,CAAiBC,IAAI,QAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;AACJC,QAAAA,OAAAA,EAASb,YAAaI,CAAAA,qBAAAA;QACtBI,UAAY,EAAA;UACVM,MAAQ,EAAA,QAAA;UACRC,QAAU,EAAA;YACRC,cAAgB,EAAA;iBAAIR,UAAWS,CAAAA;;YAC/BC,qBAAuB,EAAA,KAAA;YACvBC,cAAgB,EAAA;AAClB;AACF;AACF,OAAA;MACAG,IAAM,EAAA;AACJb,QAAAA,gBAAAA,EAAkBA,iBAAiBF,QAAAA;AACrC;AACF,KAAA;AACF,GAAA;EACA,CAACP,YAAAA,CAAaK,uBAAuB,GAAG,MAAA;AACtC,IAAA,MAAM,EAAEE,QAAUC,EAAAA,UAAAA,EAAYC,kBAAqBC,GAAAA,qCAAAA,CAAiBC,IAAI,qBAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;AACJC,QAAAA,OAAAA,EAASb,YAAaK,CAAAA,uBAAAA;QACtBG,UAAY,EAAA;UACVM,MAAQ,EAAA,QAAA;UACRC,QAAU,EAAA;YACRC,cAAgB,EAAA;iBAAIR,UAAWS,CAAAA;;YAC/BC,qBAAuB,EAAA,KAAA;YACvBC,cAAgB,EAAA;AAClB;AACF;AACF,OAAA;MACAG,IAAM,EAAA;AACJb,QAAAA,gBAAAA,EAAkBA,iBAAiBF,QAAAA;AACrC;AACF,KAAA;AACF;AACF","file":"chatPreset.cjs","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { LLMChatTemplates } from \"@/adapters/shared/llmChatTemplates.js\";\nimport { IBMVllmInputConfig } from \"./chat.js\";\nimport { IBMvLLMInput } from \"./llm.js\";\n\ninterface IBMVllmChatLLMPreset {\n  chat: IBMVllmInputConfig;\n  base: IBMvLLMInput;\n}\n\nexport const IBMVllmModel = {\n  LLAMA_3_3_70B_INSTRUCT: \"meta-llama/llama-3-3-70b-instruct\",\n  LLAMA_3_1_405B_INSTRUCT_FP8: \"meta-llama/llama-3-1-405b-instruct-fp8\",\n  LLAMA_3_1_70B_INSTRUCT: \"meta-llama/llama-3-1-70b-instruct\",\n  LLAMA_3_1_8B_INSTRUCT: \"meta-llama/llama-3-1-8b-instruct\",\n  GRANITE_3_1_8B_INSTRUCT: \"ibm-granite/granite-3-1-8b-instruct\",\n} as const;\nexport type IBMVllmModel = (typeof IBMVllmModel)[keyof typeof IBMVllmModel];\n\nexport const IBMVllmChatLLMPreset = {\n  [IBMVllmModel.LLAMA_3_3_70B_INSTRUCT]: (): IBMVllmChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"llama3.3\");\n    return {\n      base: {\n        modelId: IBMVllmModel.LLAMA_3_3_70B_INSTRUCT,\n        parameters: {\n          method: \"GREEDY\",\n          stopping: {\n            stop_sequences: [...parameters.stop_sequence],\n            include_stop_sequence: false,\n            max_new_tokens: 2048,\n          },\n          decoding: {\n            repetition_penalty: 1,\n          },\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  [IBMVllmModel.LLAMA_3_1_405B_INSTRUCT_FP8]: (): IBMVllmChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"llama3.1\");\n    return {\n      base: {\n        modelId: IBMVllmModel.LLAMA_3_1_70B_INSTRUCT,\n        parameters: {\n          method: \"GREEDY\",\n          stopping: {\n            stop_sequences: [...parameters.stop_sequence],\n            include_stop_sequence: false,\n            max_new_tokens: 2048,\n          },\n          decoding: {\n            repetition_penalty: 1,\n          },\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  [IBMVllmModel.LLAMA_3_1_70B_INSTRUCT]: (): IBMVllmChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"llama3.1\");\n    return {\n      base: {\n        modelId: IBMVllmModel.LLAMA_3_1_70B_INSTRUCT,\n        parameters: {\n          method: \"GREEDY\",\n          stopping: {\n            stop_sequences: [...parameters.stop_sequence],\n            include_stop_sequence: false,\n            max_new_tokens: 2048,\n          },\n          decoding: {\n            repetition_penalty: 1,\n          },\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  [IBMVllmModel.LLAMA_3_1_8B_INSTRUCT]: (): IBMVllmChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"llama3\");\n    return {\n      base: {\n        modelId: IBMVllmModel.LLAMA_3_1_8B_INSTRUCT,\n        parameters: {\n          method: \"GREEDY\",\n          stopping: {\n            stop_sequences: [...parameters.stop_sequence],\n            include_stop_sequence: false,\n            max_new_tokens: 2048,\n          },\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  [IBMVllmModel.GRANITE_3_1_8B_INSTRUCT]: (): IBMVllmChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"granite3.1-Instruct\");\n    return {\n      base: {\n        modelId: IBMVllmModel.GRANITE_3_1_8B_INSTRUCT,\n        parameters: {\n          method: \"GREEDY\",\n          stopping: {\n            stop_sequences: [...parameters.stop_sequence],\n            include_stop_sequence: false,\n            max_new_tokens: 2048,\n          },\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n} as const satisfies Record<IBMVllmModel, () => IBMVllmChatLLMPreset>;\n\nexport type IBMVllmChatLLMPresetModel = keyof typeof IBMVllmChatLLMPreset;\n"]}