import { LLMChatTemplates } from '../shared/llmChatTemplates.js';

const IBMVllmModel = {
  LLAMA_3_3_70B_INSTRUCT: "meta-llama/llama-3-3-70b-instruct",
  LLAMA_3_1_405B_INSTRUCT_FP8: "meta-llama/llama-3-1-405b-instruct-fp8",
  LLAMA_3_1_70B_INSTRUCT: "meta-llama/llama-3-1-70b-instruct",
  LLAMA_3_1_8B_INSTRUCT: "meta-llama/llama-3-1-8b-instruct",
  GRANITE_3_1_8B_INSTRUCT: "ibm-granite/granite-3-1-8b-instruct"
};
const IBMVllmChatLLMPreset = {
  [IBMVllmModel.LLAMA_3_3_70B_INSTRUCT]: () => {
    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get("llama3.3");
    return {
      base: {
        modelId: IBMVllmModel.LLAMA_3_3_70B_INSTRUCT,
        parameters: {
          method: "GREEDY",
          stopping: {
            stop_sequences: [
              ...parameters.stop_sequence
            ],
            include_stop_sequence: false,
            max_new_tokens: 2048
          },
          decoding: {
            repetition_penalty: 1
          }
        }
      },
      chat: {
        messagesToPrompt: messagesToPrompt(template)
      }
    };
  },
  [IBMVllmModel.LLAMA_3_1_405B_INSTRUCT_FP8]: () => {
    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get("llama3.1");
    return {
      base: {
        modelId: IBMVllmModel.LLAMA_3_1_70B_INSTRUCT,
        parameters: {
          method: "GREEDY",
          stopping: {
            stop_sequences: [
              ...parameters.stop_sequence
            ],
            include_stop_sequence: false,
            max_new_tokens: 2048
          },
          decoding: {
            repetition_penalty: 1
          }
        }
      },
      chat: {
        messagesToPrompt: messagesToPrompt(template)
      }
    };
  },
  [IBMVllmModel.LLAMA_3_1_70B_INSTRUCT]: () => {
    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get("llama3.1");
    return {
      base: {
        modelId: IBMVllmModel.LLAMA_3_1_70B_INSTRUCT,
        parameters: {
          method: "GREEDY",
          stopping: {
            stop_sequences: [
              ...parameters.stop_sequence
            ],
            include_stop_sequence: false,
            max_new_tokens: 2048
          },
          decoding: {
            repetition_penalty: 1
          }
        }
      },
      chat: {
        messagesToPrompt: messagesToPrompt(template)
      }
    };
  },
  [IBMVllmModel.LLAMA_3_1_8B_INSTRUCT]: () => {
    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get("llama3");
    return {
      base: {
        modelId: IBMVllmModel.LLAMA_3_1_8B_INSTRUCT,
        parameters: {
          method: "GREEDY",
          stopping: {
            stop_sequences: [
              ...parameters.stop_sequence
            ],
            include_stop_sequence: false,
            max_new_tokens: 2048
          }
        }
      },
      chat: {
        messagesToPrompt: messagesToPrompt(template)
      }
    };
  },
  [IBMVllmModel.GRANITE_3_1_8B_INSTRUCT]: () => {
    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get("granite3.1-Instruct");
    return {
      base: {
        modelId: IBMVllmModel.GRANITE_3_1_8B_INSTRUCT,
        parameters: {
          method: "GREEDY",
          stopping: {
            stop_sequences: [
              ...parameters.stop_sequence
            ],
            include_stop_sequence: false,
            max_new_tokens: 2048
          }
        }
      },
      chat: {
        messagesToPrompt: messagesToPrompt(template)
      }
    };
  }
};

export { IBMVllmChatLLMPreset, IBMVllmModel };
//# sourceMappingURL=chatPreset.js.map
//# sourceMappingURL=chatPreset.js.map