import { BaseLLMOutput, LLMError } from '../../llms/base.js';
import { chunk, isString, isEmpty } from 'remeda';
import { LLM } from '../../llms/llm.js';
import { Emitter } from '../../emitter/emitter.js';
import { shallowCopy } from '../../serializer/utils.js';
import { FrameworkError, NotImplementedError } from '../../errors.js';
import { assign, omitUndefined } from '../../internals/helpers/object.js';
import { Client } from './client.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function isGrpcServiceError(err) {
  return err instanceof Error && err.constructor.name === "Error" && "code" in err && typeof err.code === "number";
}
__name(isGrpcServiceError, "isGrpcServiceError");
class IBMvLLMOutput extends BaseLLMOutput {
  static {
    __name(this, "IBMvLLMOutput");
  }
  text;
  meta;
  constructor(text, meta) {
    super(), this.text = text, this.meta = meta;
  }
  static {
    this.register();
  }
  merge(other) {
    this.text += other.text;
    assign(this.meta, other.meta);
  }
  getTextContent() {
    return this.text;
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      text: this.text,
      meta: shallowCopy(this.meta)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class IBMvLLM extends LLM {
  static {
    __name(this, "IBMvLLM");
  }
  emitter = new Emitter({
    namespace: [
      "ibm_vllm",
      "llm"
    ],
    creator: this
  });
  client;
  parameters;
  constructor({ client, modelId, parameters = {}, executionOptions, cache }) {
    super(modelId, executionOptions, cache);
    this.client = client ?? new Client();
    this.parameters = parameters ?? {};
  }
  static {
    this.register();
  }
  async meta() {
    const response = await this.client.modelInfo({
      model_id: this.modelId
    });
    return {
      tokenLimit: response.max_sequence_length
    };
  }
  async embed(input, { chunkSize, signal, ...options } = {}) {
    const results = await Promise.all(chunk(input, chunkSize ?? 100).map(async (texts) => {
      const response = await this.client.embed({
        model_id: this.modelId,
        truncate_input_tokens: options?.truncate_input_tokens ?? 512,
        texts
      }, {
        signal
      });
      const embeddings = response.results?.vectors.map((vector) => {
        const embedding = vector[vector.data]?.values;
        if (!embedding) {
          throw new LLMError("Missing embedding");
        }
        return embedding;
      });
      if (embeddings?.length !== texts.length) {
        throw new LLMError("Missing embedding");
      }
      return embeddings;
    }));
    return {
      embeddings: results.flat()
    };
  }
  async tokenize(input) {
    try {
      const response = await this.client.tokenize({
        model_id: this.modelId,
        requests: [
          {
            text: input
          }
        ]
      });
      const output = response.responses.at(0);
      if (!output) {
        throw new LLMError("Missing output", [], {
          context: {
            response
          }
        });
      }
      return {
        tokens: output.tokens,
        tokensCount: output.token_count
      };
    } catch (err) {
      throw this._transformError(err);
    }
  }
  async _generate(input, options, run) {
    try {
      const response = await this.client.generate({
        model_id: this.modelId,
        requests: [
          {
            text: input
          }
        ],
        params: this._prepareParameters(options)
      }, {
        signal: run.signal
      });
      const output = response.responses.at(0);
      if (!output) {
        throw new LLMError("Missing output", [], {
          context: {
            response
          }
        });
      }
      const { text, ...rest } = output;
      return new IBMvLLMOutput(text, rest);
    } catch (err) {
      throw this._transformError(err);
    }
  }
  async *_stream(input, options, run) {
    try {
      const stream = await this.client.generateStream({
        model_id: this.modelId,
        request: {
          text: input
        },
        params: this._prepareParameters(options)
      }, {
        signal: run.signal
      });
      for await (const chunk2 of stream) {
        const typedChunk = chunk2;
        const { text, ...rest } = typedChunk;
        if (text.length > 0) {
          yield new IBMvLLMOutput(text, rest);
        }
      }
    } catch (err) {
      throw this._transformError(err);
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      client: this.client,
      modelId: this.modelId,
      parameters: shallowCopy(this.parameters),
      executionOptions: shallowCopy(this.executionOptions)
    };
  }
  loadSnapshot(snapshot) {
    super.loadSnapshot(snapshot);
    Object.assign(this, snapshot);
  }
  _transformError(error) {
    if (error instanceof FrameworkError) {
      throw error;
    }
    if (isGrpcServiceError(error)) {
      throw new LLMError("LLM has occurred an error!", [
        error
      ], {
        isRetryable: [
          8,
          4,
          14
        ].includes(error.code)
      });
    }
    return new LLMError("LLM has occurred an error!", [
      error
    ]);
  }
  _prepareParameters(overrides) {
    const guided = omitUndefined(overrides?.guided ? {} : this.parameters.decoding ?? {});
    const guidedOverride = omitUndefined(overrides?.guided ?? {});
    if (guidedOverride?.choice) {
      guided.choice = {
        ...guided.choice,
        choices: guidedOverride.choice
      };
    } else if (guidedOverride?.grammar) {
      guided.grammar = guidedOverride.grammar;
    } else if (guidedOverride?.json) {
      guided.json_schema = isString(guidedOverride.json) ? guidedOverride.json : JSON.stringify(guidedOverride.json);
    } else if (guidedOverride?.regex) {
      guided.regex = guidedOverride.regex;
    } else if (!isEmpty(guidedOverride ?? {})) {
      throw new NotImplementedError(`Following types ${Object.keys(overrides.guided).join(",")}" for the constraint decoding are not supported!`);
    }
    return {
      ...this.parameters,
      decoding: guided
    };
  }
}

export { IBMvLLM, IBMvLLMOutput };
//# sourceMappingURL=llm.js.map
//# sourceMappingURL=llm.js.map