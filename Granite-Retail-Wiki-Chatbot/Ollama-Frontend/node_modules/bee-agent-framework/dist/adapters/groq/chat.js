import { shallowCopy } from '../../serializer/utils.js';
import { ChatLLMOutput, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { Emitter } from '../../emitter/emitter.js';
import { Groq } from 'groq-sdk';
import { Serializer } from '../../serializer/serializer.js';
import { getPropStrict } from '../../internals/helpers/object.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class ChatGroqOutput extends ChatLLMOutput {
  static {
    __name(this, "ChatGroqOutput");
  }
  responses;
  constructor(response) {
    super();
    this.responses = [
      response
    ];
  }
  static {
    this.register();
  }
  get messages() {
    return this.responses.flatMap((response) => response.choices).flatMap((choice) => BaseMessage.of({
      role: choice.delta.role,
      text: choice.delta.content
    }));
  }
  getTextContent() {
    return this.messages.map((msg) => msg.text).join("\n");
  }
  merge(other) {
    this.responses.push(...other.responses);
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      responses: shallowCopy(this.responses)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class GroqChatLLM extends ChatLLM {
  static {
    __name(this, "GroqChatLLM");
  }
  emitter = Emitter.root.child({
    namespace: [
      "groq",
      "chat_llm"
    ],
    creator: this
  });
  client;
  parameters;
  constructor({ client, modelId = "llama-3.1-70b-versatile", parameters = {
    temperature: 0
  }, executionOptions = {}, cache } = {}) {
    super(modelId, executionOptions, cache);
    this.client = client ?? new Groq();
    this.parameters = parameters ?? {};
  }
  static {
    this.register();
    Serializer.register(Groq, {
      toPlain: /* @__PURE__ */ __name((value) => ({
        options: getPropStrict(value, "_options")
      }), "toPlain"),
      fromPlain: /* @__PURE__ */ __name((value) => new Groq(value.options), "fromPlain")
    });
  }
  async meta() {
    if (this.modelId.includes("gemma") || this.modelId.includes("llama3") || this.modelId.includes("llama-guard")) {
      return {
        tokenLimit: 8 * 1024
      };
    } else if (this.modelId.includes("llava-v1.5")) {
      return {
        tokenLimit: 4 * 1024
      };
    } else if (this.modelId.includes("llama-3.1-70b") || this.modelId.includes("llama-3.1-8b")) {
      return {
        tokenLimit: 128 * 1024
      };
    } else if (this.modelId.includes("mixtral-8x7b")) {
      return {
        tokenLimit: 32 * 1024
      };
    }
    return {
      tokenLimit: Infinity
    };
  }
  async embed(input, options) {
    const { data } = await this.client.embeddings.create({
      model: this.modelId,
      input: input.flatMap((msgs) => msgs.map((msg) => msg.text)),
      encoding_format: "float"
    }, {
      signal: options?.signal,
      stream: false
    });
    return {
      embeddings: data.map(({ embedding }) => embedding)
    };
  }
  async tokenize(input) {
    const contentLength = input.reduce((acc, msg) => acc + msg.text.length, 0);
    return {
      tokensCount: Math.ceil(contentLength / 4)
    };
  }
  _prepareRequest(input, options) {
    return {
      ...this.parameters,
      model: this.modelId,
      stream: false,
      messages: input.map((message) => ({
        role: message.role,
        content: message.text
      })),
      ...options?.guided?.json && {
        response_format: {
          type: "json_object"
        }
      }
    };
  }
  async _generate(input, options, run) {
    const response = await this.client.chat.completions.create({
      ...this._prepareRequest(input, options),
      stream: false
    }, {
      signal: run.signal
    });
    return new ChatGroqOutput({
      id: response.id,
      model: response.model,
      created: response.created,
      system_fingerprint: response.system_fingerprint,
      choices: response.choices.map((choice) => ({
        delta: choice.message,
        index: choice.index,
        logprobs: choice.logprobs,
        finish_reason: choice.finish_reason
      }))
    });
  }
  async *_stream(input, options, run) {
    for await (const chunk of await this.client.chat.completions.create({
      ...this._prepareRequest(input, options),
      stream: true
    }, {
      signal: run.signal
    })) {
      yield new ChatGroqOutput(chunk);
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      parameters: shallowCopy(this.parameters),
      client: this.client
    };
  }
}

export { ChatGroqOutput, GroqChatLLM };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map