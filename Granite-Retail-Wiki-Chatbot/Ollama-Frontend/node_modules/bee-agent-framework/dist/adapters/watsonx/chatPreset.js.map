{"version":3,"sources":["../../../src/adapters/watsonx/chatPreset.ts"],"names":["WatsonXChatLLMPreset","template","messagesToPrompt","parameters","LLMChatTemplates","get","base","decoding_method","include_stop_sequence","max_new_tokens","repetition_penalty","stop_sequences","stop_sequence","chat"],"mappings":";;;;AAyBO,MAAMA,oBAAuB,GAAA;AAClC,EAAA,mCAAA,kBAAqC,MAAA,CAAA,MAAA;AACnC,IAAA,MAAM,EAAEC,QAAUC,EAAAA,gBAAAA,EAAkBC,YAAeC,GAAAA,gBAAAA,CAAiBC,IAAI,UAAA,CAAA;AAExE,IAAO,OAAA;MACLC,IAAM,EAAA;QACJH,UAAY,EAAA;UACVI,eAAiB,EAAA,QAAA;UACjBC,qBAAuB,EAAA,KAAA;UACvBC,cAAgB,EAAA,IAAA;UAChBC,kBAAoB,EAAA,CAAA;UACpBC,cAAgB,EAAA;eAAIR,UAAWS,CAAAA;;AACjC;AACF,OAAA;MACAC,IAAM,EAAA;AACJX,QAAAA,gBAAAA,EAAkBA,iBAAiBD,QAAAA;AACrC;AACF,KAAA;GAhBmC,EAAA,mCAAA,CAAA;AAkBrC,EAAA,2BAAA,kBAA6B,MAAA,CAAA,MAAA;AAC3B,IAAA,MAAM,EAAEA,QAAUE,EAAAA,UAAAA,EAAYD,kBAAqBE,GAAAA,gBAAAA,CAAiBC,IAAI,qBAAA,CAAA;AACxE,IAAO,OAAA;MACLC,IAAM,EAAA;QACJH,UAAY,EAAA;UACVI,eAAiB,EAAA,QAAA;UACjBE,cAAgB,EAAA,IAAA;UAChBD,qBAAuB,EAAA,KAAA;UACvBG,cAAgB,EAAA;eAAIR,UAAWS,CAAAA;;AACjC;AACF,OAAA;MACAC,IAAM,EAAA;AACJX,QAAAA,gBAAAA,EAAkBA,iBAAiBD,QAAAA;AACrC;AACF,KAAA;GAd2B,EAAA,2BAAA,CAAA;EAgB7B,2BAAA,GAAA;AACE,IAAOD,OAAAA,oBAAAA,CAAqB,2BAAA,CAA4B,EAAA;AAC1D,GAAA;AACA,EAAA,mCAAA,kBAAqC,MAAA,CAAA,MAAA;AACnC,IAAA,MAAM,EAAEC,QAAUC,EAAAA,gBAAAA,EAAkBC,YAAeC,GAAAA,gBAAAA,CAAiBC,IAAI,UAAA,CAAA;AAExE,IAAO,OAAA;MACLC,IAAM,EAAA;QACJH,UAAY,EAAA;UACVI,eAAiB,EAAA,QAAA;UACjBC,qBAAuB,EAAA,KAAA;UACvBC,cAAgB,EAAA,IAAA;UAChBC,kBAAoB,EAAA,CAAA;UACpBC,cAAgB,EAAA;eAAIR,UAAWS,CAAAA;;AACjC;AACF,OAAA;MACAC,IAAM,EAAA;AACJX,QAAAA,gBAAAA,EAAkBA,iBAAiBD,QAAAA;AACrC;AACF,KAAA;GAhBmC,EAAA,mCAAA,CAAA;EAkBrC,oCAAA,GAAA;AACE,IAAOD,OAAAA,oBAAAA,CAAqB,mCAAA,CAAoC,EAAA;AAClE,GAAA;EACA,kCAAA,GAAA;AACE,IAAOA,OAAAA,oBAAAA,CAAqB,mCAAA,CAAoC,EAAA;AAClE,GAAA;AACA,EAAA,iCAAA,kBAAmC,MAAA,CAAA,MAAA;AACjC,IAAA,MAAM,EAAEC,QAAUC,EAAAA,gBAAAA,EAAkBC,YAAeC,GAAAA,gBAAAA,CAAiBC,IAAI,QAAA,CAAA;AAExE,IAAO,OAAA;MACLC,IAAM,EAAA;QACJH,UAAY,EAAA;UACVI,eAAiB,EAAA,QAAA;UACjBE,cAAgB,EAAA,IAAA;UAChBD,qBAAuB,EAAA,KAAA;UACvBG,cAAgB,EAAA;eAAIR,UAAWS,CAAAA;;AACjC;AACF,OAAA;MACAC,IAAM,EAAA;AACJX,QAAAA,gBAAAA,EAAkBA,iBAAiBD,QAAAA;AACrC;AACF,KAAA;GAfiC,EAAA,iCAAA,CAAA;EAiBnC,gCAAA,GAAA;AACE,IAAOD,OAAAA,oBAAAA,CAAqB,iCAAA,CAAkC,EAAA;AAChE;AACF","file":"chatPreset.js","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { LLMChatTemplates } from \"@/adapters/shared/llmChatTemplates.js\";\nimport { WatsonXLLMInput } from \"@/adapters/watsonx/llm.js\";\nimport { WatsonXChatLLMInputConfig } from \"@/adapters/watsonx/chat.js\";\n\ninterface WatsonXChatLLMPreset {\n  chat: WatsonXChatLLMInputConfig;\n  base: Omit<WatsonXLLMInput, \"client\" | \"modelId\">;\n}\n\nexport const WatsonXChatLLMPreset = {\n  \"meta-llama/llama-3-3-70b-instruct\": (): WatsonXChatLLMPreset => {\n    const { template, messagesToPrompt, parameters } = LLMChatTemplates.get(\"llama3.3\");\n\n    return {\n      base: {\n        parameters: {\n          decoding_method: \"greedy\",\n          include_stop_sequence: false,\n          max_new_tokens: 2048,\n          repetition_penalty: 1,\n          stop_sequences: [...parameters.stop_sequence],\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  \"ibm/granite-3-8b-instruct\": (): WatsonXChatLLMPreset => {\n    const { template, parameters, messagesToPrompt } = LLMChatTemplates.get(\"granite3.1-Instruct\");\n    return {\n      base: {\n        parameters: {\n          decoding_method: \"greedy\",\n          max_new_tokens: 2048,\n          include_stop_sequence: false,\n          stop_sequences: [...parameters.stop_sequence],\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  \"ibm/granite-3-2b-instruct\"() {\n    return WatsonXChatLLMPreset[\"ibm/granite-3-8b-instruct\"]();\n  },\n  \"meta-llama/llama-3-1-70b-instruct\": (): WatsonXChatLLMPreset => {\n    const { template, messagesToPrompt, parameters } = LLMChatTemplates.get(\"llama3.1\");\n\n    return {\n      base: {\n        parameters: {\n          decoding_method: \"greedy\",\n          include_stop_sequence: false,\n          max_new_tokens: 2048,\n          repetition_penalty: 1,\n          stop_sequences: [...parameters.stop_sequence],\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  \"meta-llama/llama-3-1-405b-instruct\"() {\n    return WatsonXChatLLMPreset[\"meta-llama/llama-3-1-70b-instruct\"]();\n  },\n  \"meta-llama/llama-3-1-8b-instruct\"() {\n    return WatsonXChatLLMPreset[\"meta-llama/llama-3-1-70b-instruct\"]();\n  },\n  \"meta-llama/llama-3-70b-instruct\": (): WatsonXChatLLMPreset => {\n    const { template, messagesToPrompt, parameters } = LLMChatTemplates.get(\"llama3\");\n\n    return {\n      base: {\n        parameters: {\n          decoding_method: \"greedy\",\n          max_new_tokens: 1500,\n          include_stop_sequence: false,\n          stop_sequences: [...parameters.stop_sequence],\n        },\n      },\n      chat: {\n        messagesToPrompt: messagesToPrompt(template),\n      },\n    };\n  },\n  \"meta-llama/llama-3-8b-instruct\"() {\n    return WatsonXChatLLMPreset[\"meta-llama/llama-3-70b-instruct\"]();\n  },\n} as const;\n\nexport type WatsonXChatLLMPresetModel = keyof typeof WatsonXChatLLMPreset;\n"]}