import { LLMMeta, EmbeddingOptions, EmbeddingOutput, BaseLLMTokenizeOutput, GenerateOptions, StreamGenerateOptions, AsyncStream, ExecutionOptions, BaseLLMEvents, LLMCache } from '../../llms/base.js';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from '../../llms/chat.js';
import { BaseMessage } from '../../llms/primitives/message.js';
import { E as Emitter } from '../../emitter-DRfJC1TP.js';
import { AwsCredentialIdentity, Provider } from '@aws-sdk/types';
import { BedrockRuntimeClient, InferenceConfiguration, Message, SystemContentBlock, ContentBlockDeltaEvent, ConverseCommandOutput } from '@aws-sdk/client-bedrock-runtime';
import { GetRunContext } from '../../context.js';
import '../../errors.js';
import '../../internals/types.js';
import '../../internals/helpers/guards.js';
import '../../internals/serializable.js';
import '../../cache/base.js';
import 'promise-based-task';
import '../../internals/helpers/promise.js';

type Response = ContentBlockDeltaEvent | ConverseCommandOutput;
declare class ChatBedrockOutput extends ChatLLMOutput {
    readonly responses: Response[];
    constructor(response: Response);
    get messages(): BaseMessage[];
    getTextContent(): string;
    merge(other: ChatBedrockOutput): void;
    toString(): string;
    createSnapshot(): {
        responses: Response[];
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface Input {
    modelId?: string;
    region?: string;
    client?: BedrockRuntimeClient;
    credentials?: AwsCredentialIdentity | Provider<AwsCredentialIdentity>;
    parameters?: InferenceConfiguration;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<ChatBedrockOutput>;
}
type BedrockChatLLMEvents = ChatLLMGenerateEvents<ChatBedrockOutput>;
declare class BedrockChatLLM extends ChatLLM<ChatBedrockOutput> {
    readonly emitter: Emitter<BedrockChatLLMEvents>;
    readonly client: BedrockRuntimeClient;
    readonly parameters: Partial<InferenceConfiguration>;
    constructor({ client, modelId, region, credentials, parameters, executionOptions, cache, }?: Input);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: BaseMessage[], _options: Partial<GenerateOptions>, run: GetRunContext<typeof this>): Promise<ChatBedrockOutput>;
    protected _stream(input: BaseMessage[], _options: StreamGenerateOptions | undefined, run: GetRunContext<typeof this>): AsyncStream<ChatBedrockOutput>;
    createSnapshot(): {
        client: BedrockRuntimeClient;
        modelId: string;
        parameters: Partial<InferenceConfiguration>;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, ChatBedrockOutput>>;
        cache: LLMCache<ChatBedrockOutput>;
    };
    protected convertToConverseMessages(messages: BaseMessage[]): {
        conversation: Message[];
        systemMessage: SystemContentBlock[];
    };
}

export { BedrockChatLLM, type BedrockChatLLMEvents, ChatBedrockOutput };
