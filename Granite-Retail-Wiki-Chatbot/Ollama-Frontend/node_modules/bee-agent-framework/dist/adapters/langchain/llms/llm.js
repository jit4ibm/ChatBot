import { LLM } from '../../../llms/llm.js';
import { BaseLLMOutput } from '../../../llms/base.js';
import { load } from '@langchain/core/load';
import { assign } from '../../../internals/helpers/object.js';
import { shallowCopy } from '../../../serializer/utils.js';
import { Emitter } from '../../../emitter/emitter.js';
import { NotImplementedError } from '../../../errors.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
class LangChainLLMOutput extends BaseLLMOutput {
  static {
    __name(this, "LangChainLLMOutput");
  }
  text;
  meta;
  constructor(text, meta) {
    super(), this.text = text, this.meta = meta;
  }
  static {
    this.register();
  }
  merge(other) {
    this.text += other.text;
    assign(this.meta, other.meta);
  }
  getTextContent() {
    return this.text;
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      text: this.text,
      meta: shallowCopy(this.meta)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class LangChainLLM extends LLM {
  static {
    __name(this, "LangChainLLM");
  }
  lcLLM;
  modelMeta;
  emitter;
  parameters;
  constructor(lcLLM, modelMeta, executionOptions, cache) {
    super(lcLLM._modelType(), executionOptions, cache), this.lcLLM = lcLLM, this.modelMeta = modelMeta, this.emitter = Emitter.root.child({
      namespace: [
        "langchain",
        "llm"
      ],
      creator: this
    });
    this.parameters = lcLLM.invocationParams();
  }
  static {
    this.register();
  }
  async meta() {
    if (this.modelMeta) {
      return this.modelMeta;
    }
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new NotImplementedError();
  }
  async tokenize(input) {
    return {
      tokensCount: await this.lcLLM.getNumTokens(input)
    };
  }
  async _generate(input, _options, run) {
    const { generations } = await this.lcLLM.generate([
      input
    ], {
      signal: run.signal
    });
    return new LangChainLLMOutput(generations[0][0].text, generations[0][0].generationInfo || {});
  }
  async *_stream(input, _options, run) {
    const response = this.lcLLM._streamResponseChunks(input, {
      signal: run.signal
    });
    for await (const chunk of response) {
      yield new LangChainLLMOutput(chunk.text, chunk.generationInfo || {});
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      modelMeta: this.modelMeta,
      parameters: shallowCopy(this.parameters),
      executionOptions: shallowCopy(this.executionOptions),
      lcLLM: JSON.stringify(this.lcLLM.toJSON())
    };
  }
  async loadSnapshot({ lcLLM, ...state }) {
    super.loadSnapshot(state);
    Object.assign(this, state, {
      lcLLM: await (async () => {
        if (lcLLM.includes("@ibm-generative-ai/node-sdk")) {
          const { GenAIModel } = await import('@ibm-generative-ai/node-sdk/langchain');
          return GenAIModel.fromJSON(lcLLM);
        }
        return await load(lcLLM);
      })()
    });
  }
}

export { LangChainLLM, LangChainLLMOutput };
//# sourceMappingURL=llm.js.map
//# sourceMappingURL=llm.js.map