import { LLMMeta, ExecutionOptions, LLMCache, EmbeddingOptions, EmbeddingOutput, BaseLLMTokenizeOutput, AsyncStream, BaseLLMEvents, GenerateOptions } from '../../../llms/base.cjs';
import { BaseChatModelCallOptions, BaseChatModel } from '@langchain/core/language_models/chat_models';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from '../../../llms/chat.cjs';
import { BaseMessage } from '../../../llms/primitives/message.cjs';
import { BaseMessageChunk, BaseMessage as BaseMessage$1, ChatMessage } from '@langchain/core/messages';
import { E as Emitter } from '../../../emitter-DdThRYHg.cjs';
import { GetRunContext } from '../../../context.cjs';
import '../../../errors.cjs';
import '../../../internals/types.cjs';
import '../../../internals/helpers/guards.cjs';
import '../../../internals/serializable.cjs';
import '../../../cache/base.cjs';
import 'promise-based-task';
import '../../../internals/helpers/promise.cjs';

declare class LangChainChatLLMOutput extends ChatLLMOutput {
    messages: BaseMessage[];
    meta: Record<string, any>;
    constructor(messages: BaseMessage[], meta?: Record<string, any>);
    merge(other: LangChainChatLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        messages: BaseMessage[];
        meta: Record<string, any>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
type LangChainChatLLMParameters = Record<string, any>;
type MergedCallOptions<T> = {
    lc: T;
} & GenerateOptions;
type LangChainChatLLMEvents = ChatLLMGenerateEvents<LangChainChatLLMOutput>;
declare class LangChainChatLLM<CallOptions extends BaseChatModelCallOptions, OutputMessageType extends BaseMessageChunk> extends ChatLLM<LangChainChatLLMOutput, MergedCallOptions<CallOptions>> {
    readonly lcLLM: BaseChatModel<CallOptions, OutputMessageType>;
    protected modelMeta?: LLMMeta | undefined;
    readonly emitter: Emitter<LangChainChatLLMEvents>;
    readonly parameters: any;
    constructor(lcLLM: BaseChatModel<CallOptions, OutputMessageType>, modelMeta?: LLMMeta | undefined, executionOptions?: ExecutionOptions, cache?: LLMCache<LangChainChatLLMOutput>);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected get mappers(): {
        toLCMessage(message: BaseMessage): BaseMessage$1;
        fromLCMessage(message: BaseMessage$1 | ChatMessage): BaseMessage;
    };
    protected _generate(input: BaseMessage[], options: MergedCallOptions<CallOptions>, run: GetRunContext<typeof this>): Promise<LangChainChatLLMOutput>;
    protected _stream(input: BaseMessage[], options: MergedCallOptions<CallOptions>, run: GetRunContext<typeof this>): AsyncStream<LangChainChatLLMOutput>;
    createSnapshot(): {
        modelId: string;
        modelMeta: LLMMeta | undefined;
        parameters: any;
        executionOptions: ExecutionOptions;
        lcLLM: string;
        emitter: Emitter<BaseLLMEvents<unknown, LangChainChatLLMOutput>>;
        cache: LLMCache<LangChainChatLLMOutput>;
    };
    loadSnapshot({ lcLLM, ...state }: ReturnType<typeof this.createSnapshot>): Promise<void>;
}

export { LangChainChatLLM, type LangChainChatLLMEvents, LangChainChatLLMOutput, type LangChainChatLLMParameters };
