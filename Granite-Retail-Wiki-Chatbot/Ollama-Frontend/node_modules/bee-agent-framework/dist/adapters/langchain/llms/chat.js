import { shallowCopy } from '../../../serializer/utils.js';
import { load } from '@langchain/core/load';
import { ChatLLMOutput, ChatLLM } from '../../../llms/chat.js';
import { Role, BaseMessage } from '../../../llms/primitives/message.js';
import { ChatMessage } from '@langchain/core/messages';
import { Cache } from '../../../cache/decoratorCache.js';
import { omitUndefined, getProp } from '../../../internals/helpers/object.js';
import { Emitter } from '../../../emitter/emitter.js';
import { NotImplementedError } from '../../../errors.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class LangChainChatLLMOutput extends ChatLLMOutput {
  static {
    __name(this, "LangChainChatLLMOutput");
  }
  messages;
  meta;
  constructor(messages, meta = {}) {
    super(), this.messages = messages, this.meta = meta;
  }
  static {
    this.register();
  }
  merge(other) {
    this.messages.push(...other.messages);
    Object.assign(this.meta, omitUndefined(other.meta));
  }
  getTextContent() {
    return this.messages.map((msg) => msg.text).join("");
  }
  toString() {
    return this.getTextContent();
  }
  createSnapshot() {
    return {
      messages: shallowCopy(this.messages),
      meta: shallowCopy(this.meta)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
}
class LangChainChatLLM extends ChatLLM {
  static {
    __name(this, "LangChainChatLLM");
  }
  lcLLM;
  modelMeta;
  emitter;
  parameters;
  constructor(lcLLM, modelMeta, executionOptions, cache) {
    super(lcLLM._modelType(), executionOptions, cache), this.lcLLM = lcLLM, this.modelMeta = modelMeta, this.emitter = Emitter.root.child({
      namespace: [
        "langchain",
        "chat_llm"
      ],
      creator: this
    });
    this.parameters = lcLLM.invocationParams();
  }
  static {
    this.register();
  }
  async meta() {
    if (this.modelMeta) {
      return this.modelMeta;
    }
    return {
      tokenLimit: Infinity
    };
  }
  // eslint-disable-next-line unused-imports/no-unused-vars
  async embed(input, options) {
    throw new NotImplementedError();
  }
  async tokenize(input) {
    return {
      tokensCount: await this.lcLLM.getNumTokens(input)
    };
  }
  get mappers() {
    const roleMapper = /* @__PURE__ */ new Map([
      [
        "system",
        Role.SYSTEM
      ],
      [
        "assistant",
        Role.ASSISTANT
      ],
      [
        "ai",
        Role.ASSISTANT
      ],
      [
        "generic",
        Role.ASSISTANT
      ],
      [
        "function",
        Role.ASSISTANT
      ],
      [
        "tool",
        Role.ASSISTANT
      ],
      [
        "human",
        Role.USER
      ],
      [
        "tool",
        Role.ASSISTANT
      ]
    ]);
    return {
      toLCMessage(message) {
        return new ChatMessage({
          role: message.role,
          content: message.text,
          response_metadata: message.meta
        });
      },
      fromLCMessage(message) {
        const role = getProp(message, [
          "role"
        ], message._getType());
        const text = typeof message.content === "string" ? message.content : message.content.filter((msg) => msg.type === "text").map((msg) => msg.text).join("\n");
        return BaseMessage.of({
          role: roleMapper.has(role) ? roleMapper.get(role) : Role.ASSISTANT,
          text
        });
      }
    };
  }
  async _generate(input, options, run) {
    const lcMessages = input.map((msg) => this.mappers.toLCMessage(msg));
    const response = await this.lcLLM.invoke(lcMessages, {
      ...options?.lc,
      signal: run.signal
    });
    return new LangChainChatLLMOutput([
      this.mappers.fromLCMessage(response)
    ], response.response_metadata);
  }
  async *_stream(input, options, run) {
    const lcMessages = input.map((msg) => this.mappers.toLCMessage(msg));
    const response = this.lcLLM._streamResponseChunks(lcMessages, {
      ...options?.lc,
      signal: run.signal
    });
    for await (const chunk of response) {
      yield new LangChainChatLLMOutput([
        this.mappers.fromLCMessage(chunk.message)
      ], chunk.message.response_metadata);
    }
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      modelMeta: this.modelMeta,
      parameters: shallowCopy(this.parameters),
      executionOptions: shallowCopy(this.executionOptions),
      lcLLM: JSON.stringify(this.lcLLM.toJSON())
    };
  }
  async loadSnapshot({ lcLLM, ...state }) {
    super.loadSnapshot(state);
    Object.assign(this, state, {
      lcLLM: await (async () => {
        if (lcLLM.includes("@ibm-generative-ai/node-sdk")) {
          const { GenAIChatModel } = await import('@ibm-generative-ai/node-sdk/langchain');
          return GenAIChatModel.fromJSON(lcLLM);
        }
        return await load(lcLLM);
      })()
    });
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", void 0),
  _ts_metadata("design:paramtypes", [])
], LangChainChatLLM.prototype, "mappers", null);

export { LangChainChatLLM, LangChainChatLLMOutput };
//# sourceMappingURL=chat.js.map
//# sourceMappingURL=chat.js.map