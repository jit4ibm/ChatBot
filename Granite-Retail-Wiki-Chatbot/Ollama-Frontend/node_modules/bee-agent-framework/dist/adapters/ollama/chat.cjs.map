{"version":3,"sources":["../../../src/adapters/ollama/chat.ts"],"names":["OllamaChatLLMOutput","ChatLLMOutput","results","constructor","response","register","messages","flatMap","BaseMessage","of","role","message","text","content","getTextContent","finalResult","length","LLMOutputError","customMerge","value","oldValue","images","tool_calls","total_duration","load_duration","model","done","done_reason","created_at","eval_duration","prompt_eval_duration","prompt_eval_count","safeSum","eval_count","merge","other","Cache","getInstance","clear","push","toString","createSnapshot","shallowCopy","loadSnapshot","snapshot","Object","assign","OllamaChatLLM","ChatLLM","emitter","Emitter","root","child","namespace","creator","client","parameters","modelId","executionOptions","cache","Client","fetch","host","getEnv","temperature","repeat_penalty","num_predict","registerClient","meta","show","extractModelMeta","embed","input","options","msg","truncate","embeddings","tokenize","contentLength","reduce","acc","tokensCount","Math","ceil","version","config","getPropStrict","retrieveVersion","_generate","run","signalRace","chat","prepareParameters","stream","signal","abort","_stream","chunk","aborted","throwIfAborted","overrides","map","format","retrieveFormat","guided"],"mappings":";;;;;;;;;;;;;;;;;AAcC,SAAA,YAAA,CAAA,UAAA,EAAA,MAAA,EAAA,GAAA,EAAA,IAAA,EAAA;;;;;;AAAA,MAAA,CAAA,YAAA,EAAA,cAAA,CAAA;;;;;AA+BM,MAAMA,4BAA4BC,sBAAAA,CAAAA;EA7CzC;;;AA8CkBC,EAAAA,OAAAA;AAEhBC,EAAAA,WAAAA,CAAYC,QAAwB,EAAA;AAClC,IAAK,KAAA,EAAA;AACL,IAAA,IAAA,CAAKF,OAAU,GAAA;AAACE,MAAAA;;AAClB;EAEA;AACE,IAAA,IAAA,CAAKC,QAAQ,EAAA;AACf;AAEA,EAAA,IAAIC,QAAW,GAAA;AACb,IAAA,OAAO,KAAKJ,OAAQK,CAAAA,OAAAA,CAAQ,CAACH,QAAAA,KAC3BI,wBAAYC,EAAG,CAAA;AACbC,MAAAA,IAAAA,EAAMN,SAASO,OAAQD,CAAAA,IAAAA;AACvBE,MAAAA,IAAAA,EAAMR,SAASO,OAAQE,CAAAA;AACzB,KAAA,CAAA,CAAA;AAEJ;EAEAC,cAAyB,GAAA;AACvB,IAAO,OAAA,IAAA,CAAKC,YAAYJ,OAAQE,CAAAA,OAAAA;AAClC;AAEA,EAAA,IACIE,WAAsC,GAAA;AACxC,IAAI,IAAA,IAAA,CAAKb,OAAQc,CAAAA,MAAAA,KAAW,CAAG,EAAA;AAC7B,MAAM,MAAA,IAAIC,wBAAe,qCAAA,CAAA;AAC3B;AAEA,IAAOC,OAAAA,sBAAAA,CAAY,KAAKhB,OAAS,EAAA;MAC/BS,OAAS,kBAAA,MAAA,CAAA,CAACQ,OAAOC,QAAc,MAAA;QAC7BV,IAAMS,EAAAA,KAAAA,CAAMT,QAAQU,QAASV,CAAAA,IAAAA;AAC7BG,QAAAA,OAAAA,EAAS,GAAGO,QAAUP,EAAAA,OAAAA,IAAW,EAAA,CAAKM,EAAAA,KAAAA,EAAON,WAAW,EAAA,CAAA,CAAA;QACxDQ,MAAQ,EAAA;AAAKD,UAAAA,GAAAA,QAAAA,EAAUC,UAAU,EAAA;AAASF,UAAAA,GAAAA,KAAAA,EAAOE,UAAU;;QAC3DC,UAAY,EAAA;AAAKF,UAAAA,GAAAA,QAAAA,EAAUE,cAAc,EAAA;AAASH,UAAAA,GAAAA,KAAAA,EAAOG,cAAc;;OAJhE,CAAA,EAAA,SAAA,CAAA;AAMTC,MAAAA,cAAAA,kBAAiBJ,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,gBAAA,CAAA;AAChBI,MAAAA,aAAAA,kBAAgBL,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,eAAA,CAAA;AACfK,MAAAA,KAAAA,kBAAQN,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,OAAA,CAAA;AACPM,MAAAA,IAAAA,kBAAOP,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,MAAA,CAAA;AACNO,MAAAA,WAAAA,kBAAcR,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,aAAA,CAAA;AACbQ,MAAAA,UAAAA,kBAAaT,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,YAAA,CAAA;AACZS,MAAAA,aAAAA,kBAAgBV,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,eAAA,CAAA;AACfU,MAAAA,oBAAAA,kBAAuBX,MAAAA,CAAAA,CAAAA,KAAAA,EAAOC,QAAaD,KAAAA,KAAAA,IAASC,QAA9B,EAAA,sBAAA,CAAA;MACtBW,iBAAmBC,EAAAA,kBAAAA;MACnBC,UAAYD,EAAAA;KACd,CAAA;AACF;AAEAE,EAAAA,KAAAA,CAAMC,KAAkC,EAAA;AACtCC,IAAAA,wBAAAA,CAAMC,WAAY,CAAA,IAAA,EAAM,aAAA,CAAA,CAAeC,KAAK,EAAA;AAC5C,IAAA,IAAA,CAAKpC,OAAQqC,CAAAA,IAAAA,CAAI,GAAIJ,KAAAA,CAAMjC,OAAO,CAAA;AACpC;EAEAsC,QAAmB,GAAA;AACjB,IAAA,OAAO,KAAK1B,cAAc,EAAA;AAC5B;EAEA2B,cAAiB,GAAA;AACf,IAAO,OAAA;MACLvC,OAASwC,EAAAA,qBAAAA,CAAY,KAAKxC,OAAO;AACnC,KAAA;AACF;AAEAyC,EAAAA,YAAAA,CAAaC,QAAwD,EAAA;AACnEC,IAAOC,MAAAA,CAAAA,MAAAA,CAAO,MAAMF,QAAAA,CAAAA;AACtB;AACF;;;;;;AAYO,MAAMG,sBAAsBC,gBAAAA,CAAAA;EA9HnC;;;EA+HkBC,OAAUC,GAAAA,mBAAAA,CAAQC,KAAKC,KAA2B,CAAA;IAChEC,SAAW,EAAA;AAAC,MAAA,QAAA;AAAU,MAAA;;IACtBC,OAAS,EAAA;GACX,CAAA;AAEgBC,EAAAA,MAAAA;AACAC,EAAAA,UAAAA;EAEhBrD,WACE,CAAA,EAAEoD,QAAQE,OAASD,EAAAA,UAAAA,EAAYE,mBAAmB,EAAC,EAAGC,OAAiB,GAAA;IACrEF,OAAS,EAAA;GAEX,EAAA;AACA,IAAMA,KAAAA,CAAAA,OAAAA,EAASC,kBAAkBC,KAAAA,CAAAA;AACjC,IAAKJ,IAAAA,CAAAA,MAAAA,GAASA,MAAU,IAAA,IAAIK,aAAO,CAAA;AAAEC,MAAAA,KAAAA;AAAOC,MAAAA,IAAAA,EAAMC,eAAO,aAAA;KAAe,CAAA;AACxE,IAAA,IAAA,CAAKP,aAAaA,UAAc,IAAA;MAC9BQ,WAAa,EAAA,CAAA;MACbC,cAAgB,EAAA,CAAA;MAChBC,WAAa,EAAA;AACf,KAAA;AACF;EAEA;AACE,IAAA,IAAA,CAAK7D,QAAQ,EAAA;AACb8D,IAAAA,yBAAAA,EAAAA;AACF;AAEA,EAAA,MAAMC,IAAO,GAAA;AACX,IAAA,MAAM3C,KAAQ,GAAA,MAAM,IAAK8B,CAAAA,MAAAA,CAAOc,IAAK,CAAA;AACnC5C,MAAAA,KAAAA,EAAO,IAAKgC,CAAAA;KACd,CAAA;AAEA,IAAA,OAAOa,4BAAiB7C,KAAAA,CAAAA;AAC1B;AAEA,EAAA,MAAM8C,KACJC,CAAAA,KAAAA,EACAC,OAAkC,GAAA,EACR,EAAA;AAC1B,IAAA,MAAMrE,QAAW,GAAA,MAAM,IAAKmD,CAAAA,MAAAA,CAAOgB,KAAM,CAAA;AACvC9C,MAAAA,KAAAA,EAAO,IAAKgC,CAAAA,OAAAA;MACZe,KAAOA,EAAAA,KAAAA,CAAMjE,OAAQ,CAAA,CAACD,QAAaA,KAAAA,QAAAA,EAAUC,OAAQ,CAAA,CAACmE,GAAQA,KAAAA,GAAAA,CAAI9D,IAAI,CAAA;AACtE6D,MAAAA,OAAAA,EAASA,OAASA,EAAAA,OAAAA;AAClBE,MAAAA,QAAAA,EAAUF,OAASE,EAAAA;KACrB,CAAA;AACA,IAAO,OAAA;AAAEC,MAAAA,UAAAA,EAAYxE,QAASwE,CAAAA;AAAW,KAAA;AAC3C;AAEA,EAAA,MAAMC,SAASL,KAAsD,EAAA;AACnE,IAAMM,MAAAA,aAAAA,GAAgBN,KAAMO,CAAAA,MAAAA,CAAO,CAACC,GAAAA,EAAKN,QAAQM,GAAMN,GAAAA,GAAAA,CAAI9D,IAAKI,CAAAA,MAAAA,EAAQ,CAAA,CAAA;AAExE,IAAO,OAAA;MACLiE,WAAaC,EAAAA,IAAAA,CAAKC,IAAKL,CAAAA,aAAAA,GAAgB,CAAA;AACzC,KAAA;AACF;AAEA,EAAA,MACMM,OAAU,GAAA;AACd,IAAA,MAAMC,MAASC,GAAAA,wBAAAA,CAAc,IAAK/B,CAAAA,MAAAA,EAAQ,QAAA,CAAA;AAC1C,IAAA,OAAOgC,0BAAgBF,CAAAA,MAAAA,CAAOvB,IAAMuB,EAAAA,MAAAA,CAAOxB,KAAK,CAAA;AAClD;EAEA,MAAgB2B,SAAAA,CACdhB,KACAC,EAAAA,OAAAA,EACAgB,GAC8B,EAAA;AAC9B,IAAA,MAAMrF,WAAW,MAAMsF,sBAAAA,CACrB,YACE,IAAA,CAAKnC,OAAOoC,IAAK,CAAA;AACf,MAAA,GAAI,MAAM,IAAA,CAAKC,iBAAkBpB,CAAAA,KAAAA,EAAOC,OAAAA,CAAAA;MACxCoB,MAAQ,EAAA;AACV,KAAA,GACFJ,GAAIK,CAAAA,MAAAA,EACJ,MAAM,IAAKvC,CAAAA,MAAAA,CAAOwC,OAAK,CAAA;AAGzB,IAAO,OAAA,IAAI/F,oBAAoBI,QAAAA,CAAAA;AACjC;EAEA,OAAiB4F,OAAAA,CACfxB,KACAC,EAAAA,OAAAA,EACAgB,GACkC,EAAA;AAClC,IAAA,WAAA,MAAiBQ,KAAS,IAAA,MAAM,IAAK1C,CAAAA,MAAAA,CAAOoC,IAAK,CAAA;AAC/C,MAAA,GAAI,MAAM,IAAA,CAAKC,iBAAkBpB,CAAAA,KAAAA,EAAOC,OAAAA,CAAAA;MACxCoB,MAAQ,EAAA;AACV,KAAA,CAAI,EAAA;AACF,MAAIJ,IAAAA,GAAAA,CAAIK,OAAOI,OAAS,EAAA;AACtB,QAAA;AACF;AACA,MAAM,MAAA,IAAIlG,oBAAoBiG,KAAAA,CAAAA;AAChC;AACAR,IAAAA,GAAAA,CAAIK,OAAOK,cAAc,EAAA;AAC3B;EAEA,MAAgBP,iBAAAA,CACdpB,OACA4B,SACsB,EAAA;AACtB,IAAO,OAAA;AACL3E,MAAAA,KAAAA,EAAO,IAAKgC,CAAAA,OAAAA;MACZnD,QAAUkE,EAAAA,KAAAA,CAAM6B,GAAI,CAAA,CAAC3B,GAAS,MAAA;AAC5BhE,QAAAA,IAAAA,EAAMgE,GAAIhE,CAAAA,IAAAA;AACVG,QAAAA,OAAAA,EAAS6D,GAAI9D,CAAAA;OACf,CAAA,CAAA;AACA6D,MAAAA,OAAAA,EAAS,IAAKjB,CAAAA,UAAAA;AACd8C,MAAAA,MAAAA,EAAQC,0BAAe,MAAM,IAAA,CAAKnB,OAAO,EAAA,EAAIgB,WAAWI,MAAAA;AAC1D,KAAA;AACF;EAEA/D,cAAiB,GAAA;AACf,IAAO,OAAA;AACL,MAAA,GAAG,MAAMA,cAAAA,EAAAA;AACTgB,MAAAA,OAAAA,EAAS,IAAKA,CAAAA,OAAAA;MACdD,UAAYd,EAAAA,qBAAAA,CAAY,KAAKc,UAAU,CAAA;MACvCE,gBAAkBhB,EAAAA,qBAAAA,CAAY,KAAKgB,gBAAgB,CAAA;AACnDH,MAAAA,MAAAA,EAAQ,IAAKA,CAAAA;AACf,KAAA;AACF;AACF","file":"chat.cjs","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport {\n  AsyncStream,\n  BaseLLMTokenizeOutput,\n  EmbeddingOutput,\n  ExecutionOptions,\n  GenerateOptions,\n  LLMCache,\n  LLMOutputError,\n  StreamGenerateOptions,\n} from \"@/llms/base.js\";\nimport { shallowCopy } from \"@/serializer/utils.js\";\nimport { ChatLLM, ChatLLMGenerateEvents, ChatLLMOutput } from \"@/llms/chat.js\";\nimport { BaseMessage } from \"@/llms/primitives/message.js\";\nimport { Emitter } from \"@/emitter/emitter.js\";\nimport { ChatRequest, ChatResponse, Config, Ollama as Client, Options as Parameters } from \"ollama\";\nimport { signalRace } from \"@/internals/helpers/promise.js\";\nimport { GetRunContext } from \"@/context.js\";\nimport { Cache } from \"@/cache/decoratorCache.js\";\nimport { customMerge, getPropStrict } from \"@/internals/helpers/object.js\";\nimport { safeSum } from \"@/internals/helpers/number.js\";\nimport {\n  extractModelMeta,\n  registerClient,\n  retrieveFormat,\n  retrieveVersion,\n} from \"@/adapters/ollama/shared.js\";\nimport { getEnv } from \"@/internals/env.js\";\nimport { OllamaEmbeddingOptions } from \"@/adapters/ollama/llm.js\";\n\nexport class OllamaChatLLMOutput extends ChatLLMOutput {\n  public readonly results: ChatResponse[];\n\n  constructor(response: ChatResponse) {\n    super();\n    this.results = [response];\n  }\n\n  static {\n    this.register();\n  }\n\n  get messages() {\n    return this.results.flatMap((response) =>\n      BaseMessage.of({\n        role: response.message.role,\n        text: response.message.content,\n      }),\n    );\n  }\n\n  getTextContent(): string {\n    return this.finalResult.message.content;\n  }\n\n  @Cache()\n  get finalResult(): Readonly<ChatResponse> {\n    if (this.results.length === 0) {\n      throw new LLMOutputError(\"No chunks to get final result from!\");\n    }\n\n    return customMerge(this.results, {\n      message: (value, oldValue) => ({\n        role: value.role ?? oldValue.role,\n        content: `${oldValue?.content ?? \"\"}${value?.content ?? \"\"}`,\n        images: [...(oldValue?.images ?? []), ...(value?.images ?? [])] as string[],\n        tool_calls: [...(oldValue?.tool_calls ?? []), ...(value?.tool_calls ?? [])],\n      }),\n      total_duration: (value, oldValue) => value ?? oldValue,\n      load_duration: (value, oldValue) => value ?? oldValue,\n      model: (value, oldValue) => value ?? oldValue,\n      done: (value, oldValue) => value ?? oldValue,\n      done_reason: (value, oldValue) => value ?? oldValue,\n      created_at: (value, oldValue) => value ?? oldValue,\n      eval_duration: (value, oldValue) => value ?? oldValue,\n      prompt_eval_duration: (value, oldValue) => value ?? oldValue,\n      prompt_eval_count: safeSum,\n      eval_count: safeSum,\n    });\n  }\n\n  merge(other: OllamaChatLLMOutput): void {\n    Cache.getInstance(this, \"finalResult\").clear();\n    this.results.push(...other.results);\n  }\n\n  toString(): string {\n    return this.getTextContent();\n  }\n\n  createSnapshot() {\n    return {\n      results: shallowCopy(this.results),\n    };\n  }\n\n  loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void {\n    Object.assign(this, snapshot);\n  }\n}\n\ninterface Input {\n  modelId: string;\n  client?: Client;\n  parameters?: Partial<Parameters>;\n  executionOptions?: ExecutionOptions;\n  cache?: LLMCache<OllamaChatLLMOutput>;\n}\n\nexport type OllamaChatLLMEvents = ChatLLMGenerateEvents<OllamaChatLLMOutput>;\n\nexport class OllamaChatLLM extends ChatLLM<OllamaChatLLMOutput> {\n  public readonly emitter = Emitter.root.child<OllamaChatLLMEvents>({\n    namespace: [\"ollama\", \"chat_llm\"],\n    creator: this,\n  });\n\n  public readonly client: Client;\n  public readonly parameters: Partial<Parameters>;\n\n  constructor(\n    { client, modelId, parameters, executionOptions = {}, cache }: Input = {\n      modelId: \"llama3.1\",\n    },\n  ) {\n    super(modelId, executionOptions, cache);\n    this.client = client ?? new Client({ fetch, host: getEnv(\"OLLAMA_HOST\") });\n    this.parameters = parameters ?? {\n      temperature: 0,\n      repeat_penalty: 1.0,\n      num_predict: 2048,\n    };\n  }\n\n  static {\n    this.register();\n    registerClient();\n  }\n\n  async meta() {\n    const model = await this.client.show({\n      model: this.modelId,\n    });\n\n    return extractModelMeta(model);\n  }\n\n  async embed(\n    input: BaseMessage[][],\n    options: OllamaEmbeddingOptions = {},\n  ): Promise<EmbeddingOutput> {\n    const response = await this.client.embed({\n      model: this.modelId,\n      input: input.flatMap((messages) => messages).flatMap((msg) => msg.text),\n      options: options?.options,\n      truncate: options?.truncate,\n    });\n    return { embeddings: response.embeddings };\n  }\n\n  async tokenize(input: BaseMessage[]): Promise<BaseLLMTokenizeOutput> {\n    const contentLength = input.reduce((acc, msg) => acc + msg.text.length, 0);\n\n    return {\n      tokensCount: Math.ceil(contentLength / 4),\n    };\n  }\n\n  @Cache()\n  async version() {\n    const config = getPropStrict(this.client, \"config\") as Config;\n    return retrieveVersion(config.host, config.fetch);\n  }\n\n  protected async _generate(\n    input: BaseMessage[],\n    options: GenerateOptions,\n    run: GetRunContext<typeof this>,\n  ): Promise<OllamaChatLLMOutput> {\n    const response = await signalRace(\n      async () =>\n        this.client.chat({\n          ...(await this.prepareParameters(input, options)),\n          stream: false,\n        }),\n      run.signal,\n      () => this.client.abort(),\n    );\n\n    return new OllamaChatLLMOutput(response);\n  }\n\n  protected async *_stream(\n    input: BaseMessage[],\n    options: Partial<StreamGenerateOptions>,\n    run: GetRunContext<typeof this>,\n  ): AsyncStream<OllamaChatLLMOutput> {\n    for await (const chunk of await this.client.chat({\n      ...(await this.prepareParameters(input, options)),\n      stream: true,\n    })) {\n      if (run.signal.aborted) {\n        break;\n      }\n      yield new OllamaChatLLMOutput(chunk);\n    }\n    run.signal.throwIfAborted();\n  }\n\n  protected async prepareParameters(\n    input: BaseMessage[],\n    overrides?: GenerateOptions,\n  ): Promise<ChatRequest> {\n    return {\n      model: this.modelId,\n      messages: input.map((msg) => ({\n        role: msg.role,\n        content: msg.text,\n      })),\n      options: this.parameters,\n      format: retrieveFormat(await this.version(), overrides?.guided),\n    };\n  }\n\n  createSnapshot() {\n    return {\n      ...super.createSnapshot(),\n      modelId: this.modelId,\n      parameters: shallowCopy(this.parameters),\n      executionOptions: shallowCopy(this.executionOptions),\n      client: this.client,\n    };\n  }\n}\n"]}