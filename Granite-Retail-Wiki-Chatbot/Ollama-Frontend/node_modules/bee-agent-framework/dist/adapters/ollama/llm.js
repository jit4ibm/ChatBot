import { LLM } from '../../llms/llm.js';
import { Emitter } from '../../emitter/emitter.js';
import { BaseLLMOutput, LLMOutputError } from '../../llms/base.js';
import { Ollama } from 'ollama';
import { Cache } from '../../cache/decoratorCache.js';
import { safeSum } from '../../internals/helpers/number.js';
import { shallowCopy } from '../../serializer/utils.js';
import { signalRace } from '../../internals/helpers/promise.js';
import { customMerge, getPropStrict } from '../../internals/helpers/object.js';
import { registerClient, retrieveVersion, extractModelMeta, retrieveFormat } from './shared.js';
import { getEnv } from '../../internals/env.js';

var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
__name(_ts_decorate, "_ts_decorate");
function _ts_metadata(k, v) {
  if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
}
__name(_ts_metadata, "_ts_metadata");
class OllamaLLMOutput extends BaseLLMOutput {
  static {
    __name(this, "OllamaLLMOutput");
  }
  results;
  constructor(result) {
    super();
    this.results = [
      result
    ];
  }
  static {
    this.register();
  }
  getTextContent() {
    return this.finalResult.response;
  }
  get finalResult() {
    if (this.results.length === 0) {
      throw new LLMOutputError("No chunks to get final result from!");
    }
    return customMerge(this.results, {
      response: /* @__PURE__ */ __name((value = "", oldValue = "") => oldValue + value, "response"),
      total_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "total_duration"),
      load_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "load_duration"),
      model: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "model"),
      done: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done"),
      done_reason: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "done_reason"),
      created_at: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "created_at"),
      eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "eval_duration"),
      prompt_eval_duration: /* @__PURE__ */ __name((value, oldValue) => value ?? oldValue, "prompt_eval_duration"),
      prompt_eval_count: safeSum,
      eval_count: safeSum,
      context: /* @__PURE__ */ __name((value, oldValue) => [
        ...value || [],
        ...oldValue || []
      ], "context")
    });
  }
  merge(other) {
    Cache.getInstance(this, "finalResult").clear();
    this.results.push(...other.results);
  }
  createSnapshot() {
    return {
      results: shallowCopy(this.results)
    };
  }
  loadSnapshot(snapshot) {
    Object.assign(this, snapshot);
  }
  toString() {
    return this.getTextContent();
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", typeof Readonly === "undefined" ? Object : Readonly),
  _ts_metadata("design:paramtypes", [])
], OllamaLLMOutput.prototype, "finalResult", null);
class OllamaLLM extends LLM {
  static {
    __name(this, "OllamaLLM");
  }
  emitter = Emitter.root.child({
    namespace: [
      "ollama",
      "llm"
    ],
    creator: this
  });
  client;
  parameters;
  static {
    this.register();
    registerClient();
  }
  constructor({ client, modelId, parameters, executionOptions = {}, cache }) {
    super(modelId, executionOptions, cache);
    this.client = client ?? new Ollama({
      host: getEnv("OLLAMA_HOST")
    });
    this.parameters = parameters ?? {};
  }
  async _generate(input, options, run) {
    const response = await signalRace(async () => this.client.generate({
      ...await this.prepareParameters(input, options),
      stream: false
    }), run.signal, () => this.client.abort());
    return new OllamaLLMOutput(response);
  }
  async *_stream(input, options, run) {
    for await (const chunk of await this.client.generate({
      ...await this.prepareParameters(input, options),
      stream: true
    })) {
      if (run.signal.aborted) {
        break;
      }
      yield new OllamaLLMOutput(chunk);
    }
    run.signal.throwIfAborted();
  }
  async version() {
    const config = getPropStrict(this.client, "config");
    return retrieveVersion(config.host, config.fetch);
  }
  async meta() {
    const model = await this.client.show({
      model: this.modelId
    });
    return extractModelMeta(model);
  }
  async embed(input, options = {}) {
    const response = await this.client.embed({
      model: this.modelId,
      input,
      options: options?.options,
      truncate: options?.truncate
    });
    return {
      embeddings: response.embeddings
    };
  }
  async tokenize(input) {
    return {
      tokensCount: Math.ceil(input.length / 4)
    };
  }
  async prepareParameters(input, overrides) {
    return {
      model: this.modelId,
      prompt: input,
      raw: true,
      options: this.parameters,
      format: retrieveFormat(await this.version(), overrides?.guided)
    };
  }
  createSnapshot() {
    return {
      ...super.createSnapshot(),
      modelId: this.modelId,
      executionOptions: shallowCopy(this.executionOptions),
      parameters: shallowCopy(this.parameters),
      client: this.client
    };
  }
}
_ts_decorate([
  Cache(),
  _ts_metadata("design:type", Function),
  _ts_metadata("design:paramtypes", []),
  _ts_metadata("design:returntype", Promise)
], OllamaLLM.prototype, "version", null);

export { OllamaLLM, OllamaLLMOutput };
//# sourceMappingURL=llm.js.map
//# sourceMappingURL=llm.js.map