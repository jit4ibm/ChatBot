{"version":3,"sources":["../../../src/adapters/ibm-vllm/chat.ts"],"names":["GrpcChatLLMOutput","ChatLLMOutput","raw","constructor","rawOutput","messages","text","getTextContent","BaseMessage","of","role","Role","ASSISTANT","meta","merge","other","Cache","getInstance","clear","message","toString","createSnapshot","shallowCopy","loadSnapshot","snapshot","Object","assign","IBMVllmChatLLM","ChatLLM","emitter","Emitter","namespace","creator","llm","config","cache","modelId","executionOptions","register","embed","input","options","inputs","map","messagesToPrompt","tokenize","prompt","_generate","run","rawResponse","_stream","response","transformAsyncIterable","output","fromPreset","overrides","presetFactory","IBMVllmChatLLMPreset","LLMError","preset","parameters","base","isFunction","isObjectType","chat","IBMvLLM"],"mappings":";;;;;;;;;;;;;AAcC,SAAA,YAAA,CAAA,UAAA,EAAA,MAAA,EAAA,GAAA,EAAA,IAAA,EAAA;;;;;;AAAA,MAAA,CAAA,YAAA,EAAA,cAAA,CAAA;;;;;AA8BM,MAAMA,0BAA0BC,aAAAA,CAAAA;EA5CvC;;;AA6CkBC,EAAAA,GAAAA;AAEhBC,EAAAA,WAAAA,CAAYC,SAA0B,EAAA;AACpC,IAAK,KAAA,EAAA;AACL,IAAA,IAAA,CAAKF,GAAME,GAAAA,SAAAA;AACb;AAEA,EAAA,IACIC,QAA0B,GAAA;AAC5B,IAAMC,MAAAA,IAAAA,GAAO,IAAKJ,CAAAA,GAAAA,CAAIK,cAAc,EAAA;AACpC,IAAO,OAAA;AACLC,MAAAA,WAAAA,CAAYC,EAAG,CAAA;AACbC,QAAAA,IAAAA,EAAMC,IAAKC,CAAAA,SAAAA;AACXN,QAAAA,IAAAA;AACAO,QAAAA,IAAAA,EAAM,KAAKX,GAAIW,CAAAA;OACjB;;AAEJ;AAEAC,EAAAA,KAAAA,CAAMC,KAAgC,EAAA;AACpCC,IAAAA,KAAAA,CAAMC,WAAY,CAAA,IAAA,EAAM,UAAA,CAAA,CAAYC,KAAK,EAAA;AACzC,IAAKhB,IAAAA,CAAAA,GAAAA,CAAIY,KAAMC,CAAAA,KAAAA,CAAMb,GAAG,CAAA;AAC1B;EAEAK,cAAyB,GAAA;AACvB,IAAM,MAAA,CAACY,OAAAA,CAAAA,GAAW,IAAKd,CAAAA,QAAAA;AACvB,IAAA,OAAOc,OAAQb,CAAAA,IAAAA;AACjB;EAEAc,QAAmB,GAAA;AACjB,IAAA,OAAO,KAAKb,cAAc,EAAA;AAC5B;EAEAc,cAAiB,GAAA;AACf,IAAO,OAAA;MACLnB,GAAKoB,EAAAA,WAAAA,CAAY,KAAKpB,GAAG;AAC3B,KAAA;AACF;AAEAqB,EAAAA,YAAAA,CAAaC,QAAkD,EAAA;AAC7DC,IAAOC,MAAAA,CAAAA,MAAAA,CAAO,MAAMF,QAAAA,CAAAA;AACtB;AACF;;;;;;AAcO,MAAMG,uBAAuBC,OAAAA,CAAAA;EArGpC;;;AAsGkBC,EAAAA,OAAAA,GAAU,IAAIC,OAA2B,CAAA;IACvDC,SAAW,EAAA;AAAC,MAAA,UAAA;AAAY,MAAA;;IACxBC,OAAS,EAAA;GACX,CAAA;AAEgBC,EAAAA,GAAAA;AACGC,EAAAA,MAAAA;AAEnB/B,EAAAA,WAAAA,CAAY,EAAE8B,GAAAA,EAAKC,MAAQC,EAAAA,KAAAA,EAA2B,EAAA;AACpD,IAAA,KAAA,CAAMF,GAAIG,CAAAA,OAAAA,EAASH,GAAII,CAAAA,gBAAAA,EAAkBF,KAAAA,CAAAA;AACzC,IAAA,IAAA,CAAKF,GAAMA,GAAAA,GAAAA;AACX,IAAA,IAAA,CAAKC,MAASA,GAAAA,MAAAA;AAChB;EAEA;AACE,IAAA,IAAA,CAAKI,QAAQ,EAAA;AACf;AAEA,EAAA,MAAMzB,IAAyB,GAAA;AAC7B,IAAO,OAAA,IAAA,CAAKoB,IAAIpB,IAAI,EAAA;AACtB;EAEA,MAAM0B,KAAAA,CAAMC,OAAwBC,OAA6D,EAAA;AAC/F,IAAMC,MAAAA,MAAAA,GAASF,MAAMG,GAAI,CAAA,CAACtC,aAAa,IAAKuC,CAAAA,gBAAAA,CAAiBvC,QAAAA,CAAAA,CAAAA;AAC7D,IAAA,OAAO,IAAK4B,CAAAA,GAAAA,CAAIM,KAAMG,CAAAA,MAAAA,EAAQD,OAAAA,CAAAA;AAChC;EAEApB,cAAiB,GAAA;AACf,IAAO,OAAA;AACL,MAAA,GAAG,MAAMA,cAAAA,EAAAA;AACTe,MAAAA,OAAAA,EAAS,IAAKA,CAAAA,OAAAA;AACdC,MAAAA,gBAAAA,EAAkB,IAAKA,CAAAA,gBAAAA;AACvBJ,MAAAA,GAAAA,EAAK,IAAKA,CAAAA,GAAAA;MACVC,MAAQZ,EAAAA,WAAAA,CAAY,KAAKY,MAAM;AACjC,KAAA;AACF;AAEA,EAAA,MAAMW,SAASxC,QAAyD,EAAA;AACtE,IAAMyC,MAAAA,MAAAA,GAAS,IAAKF,CAAAA,gBAAAA,CAAiBvC,QAAAA,CAAAA;AACrC,IAAO,OAAA,IAAA,CAAK4B,GAAIY,CAAAA,QAAAA,CAASC,MAAAA,CAAAA;AAC3B;EAEA,MAAgBC,SAAAA,CACd1C,QACAoC,EAAAA,OAAAA,EACAO,GAC4B,EAAA;AAC5B,IAAMF,MAAAA,MAAAA,GAAS,IAAKF,CAAAA,gBAAAA,CAAiBvC,QAAAA,CAAAA;AAErC,IAAA,MAAM4C,cAAc,MAAM,IAAA,CAAKhB,IAAIc,SAAUD,CAAAA,MAAAA,EAAQL,SAASO,GAAAA,CAAAA;AAC9D,IAAO,OAAA,IAAIhD,kBAAkBiD,WAAAA,CAAAA;AAC/B;EAEA,OAAiBC,OAAAA,CACf7C,QACAoC,EAAAA,OAAAA,EACAO,GACsC,EAAA;AACtC,IAAMF,MAAAA,MAAAA,GAAS,IAAKF,CAAAA,gBAAAA,CAAiBvC,QAAAA,CAAAA;AAErC,IAAA,MAAM8C,WAAW,IAAKlB,CAAAA,GAAAA,CAAIiB,OAAQJ,CAAAA,MAAAA,EAAQL,SAASO,GAAAA,CAAAA;AACnD,IAAO,OAAA,OAAOI,uBAAuBD,QAAU,EAAA,CAACE,WAAW,IAAIrD,iBAAAA,CAAkBqD,MAAAA,CAAAA,CAAAA;AACnF;AAEAT,EAAAA,gBAAAA,CAAiBvC,QAAyB,EAAA;AACxC,IAAO,OAAA,IAAA,CAAK6B,MAAOU,CAAAA,gBAAAA,CAAiBvC,QAAAA,CAAAA;AACtC;EAEA,OAAOiD,UAAAA,CACLlB,SACAmB,SAIA,EAAA;AACA,IAAMC,MAAAA,aAAAA,GAAgBC,qBAAqBrB,OAAAA,CAAAA;AAC3C,IAAA,IAAI,CAACoB,aAAe,EAAA;AAClB,MAAA,MAAM,IAAIE,QAAAA,CAAS,CAAUtB,OAAAA,EAAAA,OAAAA,CAAoC,2BAAA,CAAA,CAAA;AACnE;AAEA,IAAA,MAAMuB,SAASH,aAAAA,EAAAA;AACf,IAAA,IAAII,UAAaD,GAAAA,MAAAA,CAAOE,IAAKD,CAAAA,UAAAA,IAAc,EAAC;AAC5C,IAAA,IAAIL,SAAW,EAAA;AACb,MAAIO,IAAAA,UAAAA,CAAWP,SAAUK,CAAAA,UAAU,CAAG,EAAA;AACpCA,QAAaL,UAAAA,GAAAA,SAAAA,CAAUK,WAAWA,UAAAA,CAAAA;OACzBG,MAAAA,IAAAA,YAAAA,CAAaR,SAAUK,CAAAA,UAAU,CAAG,EAAA;AAC7CA,QAAAA,UAAAA,GAAaL,SAAUK,CAAAA,UAAAA;AACzB;AACF;AAEA,IAAA,OAAO,IAAIjC,cAAe,CAAA;AACxBO,MAAAA,MAAAA,EAAQyB,MAAOK,CAAAA,IAAAA;AACf/B,MAAAA,GAAAA,EAAK,IAAIgC,OAAQ,CAAA;AACf,QAAA,GAAGN,MAAOE,CAAAA,IAAAA;QACV,GAAGN,SAAAA;AACHK,QAAAA,UAAAA;AACAxB,QAAAA;OACF;KACF,CAAA;AACF;AACF","file":"chat.js","sourcesContent":["/**\n * Copyright 2024 IBM Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { isFunction, isObjectType } from \"remeda\";\n\nimport {\n  IBMvLLM,\n  IBMvLLMEmbeddingOptions,\n  IBMvLLMGenerateOptions,\n  IBMvLLMOutput,\n  IBMvLLMParameters,\n} from \"./llm.js\";\n\nimport { Cache } from \"@/cache/decoratorCache.js\";\nimport { BaseMessage, Role } from \"@/llms/primitives/message.js\";\nimport { Emitter } from \"@/emitter/emitter.js\";\nimport { ChatLLM, ChatLLMGenerateEvents, ChatLLMOutput } from \"@/llms/chat.js\";\nimport {\n  AsyncStream,\n  BaseLLMTokenizeOutput,\n  EmbeddingOutput,\n  LLMCache,\n  LLMError,\n  LLMMeta,\n} from \"@/llms/base.js\";\nimport { transformAsyncIterable } from \"@/internals/helpers/stream.js\";\nimport { shallowCopy } from \"@/serializer/utils.js\";\nimport { IBMVllmChatLLMPreset, IBMVllmChatLLMPresetModel } from \"@/adapters/ibm-vllm/chatPreset.js\";\nimport { Client } from \"./client.js\";\nimport { GetRunContext } from \"@/context.js\";\n\nexport class GrpcChatLLMOutput extends ChatLLMOutput {\n  public readonly raw: IBMvLLMOutput;\n\n  constructor(rawOutput: IBMvLLMOutput) {\n    super();\n    this.raw = rawOutput;\n  }\n\n  @Cache()\n  get messages(): BaseMessage[] {\n    const text = this.raw.getTextContent();\n    return [\n      BaseMessage.of({\n        role: Role.ASSISTANT,\n        text,\n        meta: this.raw.meta,\n      }),\n    ];\n  }\n\n  merge(other: GrpcChatLLMOutput): void {\n    Cache.getInstance(this, \"messages\").clear();\n    this.raw.merge(other.raw);\n  }\n\n  getTextContent(): string {\n    const [message] = this.messages;\n    return message.text;\n  }\n\n  toString(): string {\n    return this.getTextContent();\n  }\n\n  createSnapshot() {\n    return {\n      raw: shallowCopy(this.raw),\n    };\n  }\n\n  loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>) {\n    Object.assign(this, snapshot);\n  }\n}\n\nexport interface IBMVllmInputConfig {\n  messagesToPrompt: (messages: BaseMessage[]) => string;\n}\n\nexport interface GrpcChatLLMInput {\n  llm: IBMvLLM;\n  config: IBMVllmInputConfig;\n  cache?: LLMCache<GrpcChatLLMOutput>;\n}\n\nexport type IBMVllmChatEvents = ChatLLMGenerateEvents<GrpcChatLLMOutput>;\n\nexport class IBMVllmChatLLM extends ChatLLM<GrpcChatLLMOutput> {\n  public readonly emitter = new Emitter<IBMVllmChatEvents>({\n    namespace: [\"ibm_vllm\", \"chat_llm\"],\n    creator: this,\n  });\n\n  public readonly llm: IBMvLLM;\n  protected readonly config: IBMVllmInputConfig;\n\n  constructor({ llm, config, cache }: GrpcChatLLMInput) {\n    super(llm.modelId, llm.executionOptions, cache);\n    this.llm = llm;\n    this.config = config;\n  }\n\n  static {\n    this.register();\n  }\n\n  async meta(): Promise<LLMMeta> {\n    return this.llm.meta();\n  }\n\n  async embed(input: BaseMessage[][], options?: IBMvLLMEmbeddingOptions): Promise<EmbeddingOutput> {\n    const inputs = input.map((messages) => this.messagesToPrompt(messages));\n    return this.llm.embed(inputs, options);\n  }\n\n  createSnapshot() {\n    return {\n      ...super.createSnapshot(),\n      modelId: this.modelId,\n      executionOptions: this.executionOptions,\n      llm: this.llm,\n      config: shallowCopy(this.config),\n    };\n  }\n\n  async tokenize(messages: BaseMessage[]): Promise<BaseLLMTokenizeOutput> {\n    const prompt = this.messagesToPrompt(messages);\n    return this.llm.tokenize(prompt);\n  }\n\n  protected async _generate(\n    messages: BaseMessage[],\n    options: IBMvLLMGenerateOptions | undefined,\n    run: GetRunContext<typeof this>,\n  ): Promise<GrpcChatLLMOutput> {\n    const prompt = this.messagesToPrompt(messages);\n    // @ts-expect-error protected property\n    const rawResponse = await this.llm._generate(prompt, options, run);\n    return new GrpcChatLLMOutput(rawResponse);\n  }\n\n  protected async *_stream(\n    messages: BaseMessage[],\n    options: IBMvLLMGenerateOptions | undefined,\n    run: GetRunContext<typeof this>,\n  ): AsyncStream<GrpcChatLLMOutput, void> {\n    const prompt = this.messagesToPrompt(messages);\n    // @ts-expect-error protected property\n    const response = this.llm._stream(prompt, options, run);\n    return yield* transformAsyncIterable(response, (output) => new GrpcChatLLMOutput(output));\n  }\n\n  messagesToPrompt(messages: BaseMessage[]) {\n    return this.config.messagesToPrompt(messages);\n  }\n\n  static fromPreset(\n    modelId: IBMVllmChatLLMPresetModel,\n    overrides?: {\n      client?: Client;\n      parameters?: IBMvLLMParameters | ((value: IBMvLLMParameters) => IBMvLLMParameters);\n    },\n  ) {\n    const presetFactory = IBMVllmChatLLMPreset[modelId];\n    if (!presetFactory) {\n      throw new LLMError(`Model \"${modelId}\" does not exist in preset.`);\n    }\n\n    const preset = presetFactory();\n    let parameters = preset.base.parameters ?? {};\n    if (overrides) {\n      if (isFunction(overrides.parameters)) {\n        parameters = overrides.parameters(parameters);\n      } else if (isObjectType(overrides.parameters)) {\n        parameters = overrides.parameters;\n      }\n    }\n\n    return new IBMVllmChatLLM({\n      config: preset.chat,\n      llm: new IBMvLLM({\n        ...preset.base,\n        ...overrides,\n        parameters,\n        modelId,\n      }),\n    });\n  }\n}\n"]}