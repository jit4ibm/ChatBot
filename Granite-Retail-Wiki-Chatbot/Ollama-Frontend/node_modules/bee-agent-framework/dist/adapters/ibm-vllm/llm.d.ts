import { BaseLLMOutput, ExecutionOptions, LLMCache, GenerateOptions, EmbeddingOptions, LLMMeta, EmbeddingOutput, BaseLLMTokenizeOutput, AsyncStream, BaseLLMEvents } from '../../llms/base.js';
import { BatchedGenerationRequest, SingleGenerationRequest, EmbeddingTasksRequest, Parameters } from './types.js';
import { LLMEvents, LLM, LLMInput } from '../../llms/llm.js';
import { E as Emitter } from '../../emitter-DRfJC1TP.js';
import { Client } from './client.js';
import { GetRunContext } from '../../context.js';
import { OmitPrivateKeys } from '../../internals/types.js';
import '../../errors.js';
import '../../internals/helpers/guards.js';
import '../../internals/serializable.js';
import '../../cache/base.js';
import 'promise-based-task';
import '../../internals/helpers/promise.js';
import '@grpc/grpc-js';
import '@grpc/proto-loader';
import 'p-queue-compat';
import 'node_modules/p-queue-compat/dist/types/priority-queue.js';
import '@grpc/grpc-js/build/src/client.js';

declare class IBMvLLMOutput extends BaseLLMOutput {
    text: string;
    readonly meta: Record<string, any>;
    constructor(text: string, meta: Record<string, any>);
    merge(other: IBMvLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        text: string;
        meta: Record<string, any>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface IBMvLLMInput {
    client?: Client;
    modelId: string;
    parameters?: IBMvLLMParameters;
    executionOptions?: ExecutionOptions;
    cache?: LLMCache<IBMvLLMOutput>;
}
type IBMvLLMParameters = NonNullable<BatchedGenerationRequest["params"] & SingleGenerationRequest["params"]>;
interface IBMvLLMGenerateOptions extends GenerateOptions {
}
interface IBMvLLMEmbeddingOptions extends EmbeddingOptions, Omit<OmitPrivateKeys<EmbeddingTasksRequest>, "texts"> {
    chunkSize?: number;
}
type IBMvLLMEvents = LLMEvents<IBMvLLMOutput>;
declare class IBMvLLM extends LLM<IBMvLLMOutput, IBMvLLMGenerateOptions> {
    readonly emitter: Emitter<IBMvLLMEvents>;
    readonly client: Client;
    readonly parameters: Partial<IBMvLLMParameters>;
    constructor({ client, modelId, parameters, executionOptions, cache }: IBMvLLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: LLMInput[], { chunkSize, signal, ...options }?: IBMvLLMEmbeddingOptions): Promise<EmbeddingOutput>;
    tokenize(input: LLMInput): Promise<BaseLLMTokenizeOutput>;
    protected _generate(input: LLMInput, options: IBMvLLMGenerateOptions | undefined, run: GetRunContext<this>): Promise<IBMvLLMOutput>;
    protected _stream(input: string, options: IBMvLLMGenerateOptions | undefined, run: GetRunContext<typeof this>): AsyncStream<IBMvLLMOutput>;
    createSnapshot(): {
        client: Client;
        modelId: string;
        parameters: Partial<Parameters>;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, IBMvLLMOutput>>;
        cache: LLMCache<IBMvLLMOutput>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
    protected _transformError(error: Error): Error;
    protected _prepareParameters(overrides?: GenerateOptions): typeof this.parameters;
}

export { IBMvLLM, type IBMvLLMEmbeddingOptions, type IBMvLLMEvents, type IBMvLLMGenerateOptions, type IBMvLLMInput, IBMvLLMOutput, type IBMvLLMParameters };
