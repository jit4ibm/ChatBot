import { LLMCache, LLMMeta, EmbeddingOutput, ExecutionOptions, BaseLLMEvents, BaseLLMTokenizeOutput, AsyncStream } from './llms/base.js';
import { IBMvLLMInput, IBMvLLMOutput, IBMvLLM, IBMvLLMEmbeddingOptions, IBMvLLMGenerateOptions, IBMvLLMParameters } from './adapters/ibm-vllm/llm.js';
import { BaseMessage } from './llms/primitives/message.js';
import { E as Emitter } from './emitter-DRfJC1TP.js';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from './llms/chat.js';
import { Client } from './adapters/ibm-vllm/client.js';
import { GetRunContext } from './context.js';

/**
 * Copyright 2024 IBM Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

declare const IBMVllmModel: {
    readonly LLAMA_3_3_70B_INSTRUCT: "meta-llama/llama-3-3-70b-instruct";
    readonly LLAMA_3_1_405B_INSTRUCT_FP8: "meta-llama/llama-3-1-405b-instruct-fp8";
    readonly LLAMA_3_1_70B_INSTRUCT: "meta-llama/llama-3-1-70b-instruct";
    readonly LLAMA_3_1_8B_INSTRUCT: "meta-llama/llama-3-1-8b-instruct";
    readonly GRANITE_3_1_8B_INSTRUCT: "ibm-granite/granite-3-1-8b-instruct";
};
type IBMVllmModel = (typeof IBMVllmModel)[keyof typeof IBMVllmModel];
interface IBMVllmChatLLMPreset {
    chat: IBMVllmInputConfig;
    base: IBMvLLMInput;
}
declare const IBMVllmChatLLMPreset: {
    readonly "meta-llama/llama-3-3-70b-instruct": () => IBMVllmChatLLMPreset;
    readonly "meta-llama/llama-3-1-405b-instruct-fp8": () => IBMVllmChatLLMPreset;
    readonly "meta-llama/llama-3-1-70b-instruct": () => IBMVllmChatLLMPreset;
    readonly "meta-llama/llama-3-1-8b-instruct": () => IBMVllmChatLLMPreset;
    readonly "ibm-granite/granite-3-1-8b-instruct": () => IBMVllmChatLLMPreset;
};
type IBMVllmChatLLMPresetModel = keyof typeof IBMVllmChatLLMPreset;

declare class GrpcChatLLMOutput extends ChatLLMOutput {
    readonly raw: IBMvLLMOutput;
    constructor(rawOutput: IBMvLLMOutput);
    get messages(): BaseMessage[];
    merge(other: GrpcChatLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        raw: IBMvLLMOutput;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface IBMVllmInputConfig {
    messagesToPrompt: (messages: BaseMessage[]) => string;
}
interface GrpcChatLLMInput {
    llm: IBMvLLM;
    config: IBMVllmInputConfig;
    cache?: LLMCache<GrpcChatLLMOutput>;
}
type IBMVllmChatEvents = ChatLLMGenerateEvents<GrpcChatLLMOutput>;
declare class IBMVllmChatLLM extends ChatLLM<GrpcChatLLMOutput> {
    readonly emitter: Emitter<IBMVllmChatEvents>;
    readonly llm: IBMvLLM;
    protected readonly config: IBMVllmInputConfig;
    constructor({ llm, config, cache }: GrpcChatLLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: IBMvLLMEmbeddingOptions): Promise<EmbeddingOutput>;
    createSnapshot(): {
        modelId: string;
        executionOptions: ExecutionOptions;
        llm: IBMvLLM;
        config: IBMVllmInputConfig;
        emitter: Emitter<BaseLLMEvents<unknown, GrpcChatLLMOutput>>;
        cache: LLMCache<GrpcChatLLMOutput>;
    };
    tokenize(messages: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected _generate(messages: BaseMessage[], options: IBMvLLMGenerateOptions | undefined, run: GetRunContext<typeof this>): Promise<GrpcChatLLMOutput>;
    protected _stream(messages: BaseMessage[], options: IBMvLLMGenerateOptions | undefined, run: GetRunContext<typeof this>): AsyncStream<GrpcChatLLMOutput, void>;
    messagesToPrompt(messages: BaseMessage[]): string;
    static fromPreset(modelId: IBMVllmChatLLMPresetModel, overrides?: {
        client?: Client;
        parameters?: IBMvLLMParameters | ((value: IBMvLLMParameters) => IBMvLLMParameters);
    }): IBMVllmChatLLM;
}

export { GrpcChatLLMOutput as G, IBMVllmModel as I, IBMVllmChatLLMPreset as a, type IBMVllmChatLLMPresetModel as b, type IBMVllmInputConfig as c, type GrpcChatLLMInput as d, type IBMVllmChatEvents as e, IBMVllmChatLLM as f };
