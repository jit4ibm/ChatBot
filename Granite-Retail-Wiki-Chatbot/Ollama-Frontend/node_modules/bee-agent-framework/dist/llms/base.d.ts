import { Run, GetRunContext } from '../context.js';
import { FrameworkError } from '../errors.js';
import { Serializable } from '../internals/serializable.js';
import { OneOf } from '../internals/types.js';
import { C as Callback, E as Emitter } from '../emitter-DRfJC1TP.js';
import { BaseCache } from '../cache/base.js';
import { Task } from 'promise-based-task';
import '../internals/helpers/promise.js';
import '../internals/helpers/guards.js';

interface BaseLLMEvents<TInput = any, TOutput extends BaseLLMOutput = BaseLLMOutput> {
    newToken?: Callback<{
        value: TOutput;
        callbacks: {
            abort: () => void;
        };
    }>;
    success?: Callback<{
        value: TOutput;
    }>;
    start?: Callback<{
        input: TInput;
        options: unknown;
    }>;
    error?: Callback<{
        input: TInput;
        error: FrameworkError;
        options: unknown;
    }>;
    finish?: Callback<null>;
}
/**
 * @deprecated Use BaseLLMEvents instead
 */
type GenerateCallbacks = BaseLLMEvents;
type GuidedOptions = OneOf<[
    {
        json?: string | Record<string, any>;
    },
    {
        regex?: string;
    },
    {
        choice?: string[];
    },
    {
        grammar?: string;
    },
    {
        decoding_backend?: string;
    },
    {
        whitespace_pattern?: string;
    }
]>;
interface GenerateOptions {
    stream?: boolean;
    signal?: AbortSignal;
    guided?: GuidedOptions;
}
interface InternalGenerateOptions {
    signal?: AbortSignal;
}
interface StreamGenerateOptions {
    signal?: AbortSignal;
    guided?: GuidedOptions;
}
type AsyncStream<T, T2 = any> = AsyncGenerator<T, T2, any>;
declare class LLMError extends FrameworkError {
}
declare class LLMFatalError extends LLMError {
    constructor(message: string, errors?: Error[]);
}
declare class LLMOutputError extends LLMFatalError {
}
interface BaseLLMTokenizeOutput {
    tokensCount: number;
    tokens?: string[];
}
declare abstract class BaseLLMOutput extends Serializable {
    mergeImmutable<T extends BaseLLMOutput>(this: T, other: T): T;
    abstract merge(other: BaseLLMOutput): void;
    abstract getTextContent(): string;
    abstract toString(): string;
}
interface ExecutionOptions {
    maxRetries?: number;
}
interface EmbeddingOptions {
    signal?: AbortSignal;
}
interface EmbeddingOutput {
    embeddings: number[][];
}
interface LLMMeta {
    tokenLimit: number;
}
type LLMCache<T extends BaseLLMOutput> = BaseCache<Task<T[]>>;
declare abstract class BaseLLM<TInput, TOutput extends BaseLLMOutput, TGenerateOptions extends GenerateOptions = GenerateOptions> extends Serializable<any> {
    readonly modelId: string;
    readonly executionOptions: ExecutionOptions;
    readonly cache: LLMCache<TOutput>;
    abstract readonly emitter: Emitter<BaseLLMEvents<unknown, TOutput>>;
    constructor(modelId: string, executionOptions?: ExecutionOptions, cache?: LLMCache<TOutput>);
    abstract meta(): Promise<LLMMeta>;
    abstract embed(input: TInput[], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    abstract tokenize(input: TInput): Promise<BaseLLMTokenizeOutput>;
    generate(input: TInput, options?: Partial<TGenerateOptions>): Run<TOutput, this, readonly [TInput, Partial<TGenerateOptions>]>;
    stream(input: TInput, options?: Partial<StreamGenerateOptions>): AsyncStream<TOutput>;
    protected abstract _generate(input: TInput, options: Partial<TGenerateOptions>, run: GetRunContext<typeof this>): Promise<TOutput>;
    protected abstract _stream(input: TInput, options: Partial<StreamGenerateOptions>, run: GetRunContext<typeof this>): AsyncStream<TOutput, void>;
    protected _mergeChunks(chunks: TOutput[]): TOutput;
    static cast<T extends BaseLLM<unknown, BaseLLMOutput>>(this: new (...args: any[]) => T, value: unknown): asserts value is T;
    static castInput<A>(this: new (...args: any[]) => BaseLLM<A, BaseLLMOutput>, value: unknown): asserts value is A;
    static castOutput<T extends BaseLLM<unknown, BaseLLMOutput>>(this: new (...args: any[]) => T, value: BaseLLMOutput): asserts value is InferLLMOutput<T>;
    createSnapshot(): {
        modelId: string;
        executionOptions: ExecutionOptions;
        emitter: Emitter<BaseLLMEvents<unknown, TOutput>>;
        cache: LLMCache<TOutput>;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
    protected createCacheAccessor(input: TInput, options: Partial<GenerateOptions> | Partial<StreamGenerateOptions>, ...extra: any[]): Promise<{
        key: string;
        value: TOutput[] | undefined;
        resolve: <T2 extends TOutput>(value: T2 | T2[]) => void;
        reject: (error: Error) => Promise<void>;
    }>;
}
type AnyLLM<I = string, T = BaseLLMOutput> = BaseLLM<I, T extends BaseLLMOutput ? T : never>;
type InferLLMOutput<T> = T extends BaseLLM<any, infer A, any> ? A : never;

export { type AnyLLM, type AsyncStream, BaseLLM, type BaseLLMEvents, BaseLLMOutput, type BaseLLMTokenizeOutput, type EmbeddingOptions, type EmbeddingOutput, type ExecutionOptions, type GenerateCallbacks, type GenerateOptions, type GuidedOptions, type InferLLMOutput, type InternalGenerateOptions, type LLMCache, LLMError, LLMFatalError, type LLMMeta, LLMOutputError, type StreamGenerateOptions };
