import { LLMCache, LLMMeta, EmbeddingOptions, EmbeddingOutput, ExecutionOptions, BaseLLMEvents, BaseLLMTokenizeOutput, StreamGenerateOptions, AsyncStream } from './llms/base.js';
import { ExcludeNonStringIndex } from './internals/types.js';
import { BAMLLMInput, BAMLLMOutput, BAMLLM, BAMLLMGenerateOptions, BAMLLMParameters } from './adapters/bam/llm.js';
import { ChatLLMOutput, ChatLLMGenerateEvents, ChatLLM } from './llms/chat.js';
import { BaseMessage } from './llms/primitives/message.js';
import { Client } from '@ibm-generative-ai/node-sdk';
import { E as Emitter } from './emitter-DRfJC1TP.js';
import { GetRunContext } from './context.js';

/**
 * Copyright 2024 IBM Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

interface BAMChatLLMPreset {
    chat: BAMChatLLMInputConfig;
    base: Omit<BAMLLMInput, "client" | "modelId">;
}
declare const BAMChatLLMPreset: {
    readonly "meta-llama/llama-3-3-70b-instruct": () => BAMChatLLMPreset;
    readonly "meta-llama/llama-3-1-70b-instruct": () => BAMChatLLMPreset;
    readonly "meta-llama/llama-3-8b-instruct": () => BAMChatLLMPreset;
    readonly "meta-llama/llama-3-70b-instruct": () => BAMChatLLMPreset;
};
type BAMChatLLMPresetModel = keyof typeof BAMChatLLMPreset;

declare class BAMChatLLMOutput extends ChatLLMOutput {
    readonly raw: BAMLLMOutput;
    constructor(rawOutput: BAMLLMOutput);
    get finalResult(): Readonly<ExcludeNonStringIndex<{
        [key: string]: unknown;
        input_text?: string | null;
        generated_text: string;
        generated_token_count: number;
        input_token_count?: number | null;
        stop_reason: "not_finished" | "max_tokens" | "eos_token" | "cancelled" | "time_limit" | "stop_sequence" | "token_limit" | "error";
        stop_sequence?: string | null;
        generated_tokens?: (({
            text?: string | null;
            logprob?: (number | null) | (string | null);
            rank?: number | null;
            top_tokens?: (({
                text?: string | null;
                logprob?: (number | null) | (string | null);
            })[]) | null;
        })[]) | null;
        input_tokens?: (({
            text?: string | null;
            logprob?: (number | null) | (string | null);
            rank?: number | null;
            top_tokens?: (({
                text?: string | null;
                logprob?: (number | null) | (string | null);
            })[]) | null;
        })[]) | null;
        seed?: number | null;
        moderations?: {
            hap?: {
                success: boolean;
                score: number;
                flagged: boolean;
                position: {
                    start: number;
                    end: number;
                };
                tokens?: {
                    token?: string;
                    index?: number;
                    score?: number;
                }[];
            }[];
            social_bias?: {
                success: boolean;
                score: number;
                flagged: boolean;
                position: {
                    start: number;
                    end: number;
                };
                tokens?: {
                    token?: string;
                    index?: number;
                    score?: number;
                }[];
            }[];
        };
    }>>;
    get messages(): BaseMessage[];
    merge(other: BAMChatLLMOutput): void;
    getTextContent(): string;
    toString(): string;
    createSnapshot(): {
        raw: BAMLLMOutput;
    };
    loadSnapshot(snapshot: ReturnType<typeof this.createSnapshot>): void;
}
interface BAMChatLLMInputConfig {
    messagesToPrompt: (messages: BaseMessage[]) => string;
}
interface BAMChatLLMInput {
    llm: BAMLLM;
    config: BAMChatLLMInputConfig;
    cache?: LLMCache<BAMChatLLMOutput>;
}
type BAMChatLLMEvents = ChatLLMGenerateEvents<BAMChatLLMOutput>;
declare class BAMChatLLM extends ChatLLM<BAMChatLLMOutput> {
    readonly emitter: Emitter<BAMChatLLMEvents>;
    readonly llm: BAMLLM;
    protected readonly config: BAMChatLLMInputConfig;
    constructor({ llm, config, cache }: BAMChatLLMInput);
    meta(): Promise<LLMMeta>;
    embed(input: BaseMessage[][], options?: EmbeddingOptions): Promise<EmbeddingOutput>;
    createSnapshot(): {
        modelId: string;
        executionOptions: ExecutionOptions;
        llm: BAMLLM;
        config: BAMChatLLMInputConfig;
        emitter: Emitter<BaseLLMEvents<unknown, BAMChatLLMOutput>>;
        cache: LLMCache<BAMChatLLMOutput>;
    };
    tokenize(messages: BaseMessage[]): Promise<BaseLLMTokenizeOutput>;
    protected _generate(messages: BaseMessage[], options: BAMLLMGenerateOptions | undefined, run: GetRunContext<typeof this>): Promise<BAMChatLLMOutput>;
    protected _stream(messages: BaseMessage[], options: StreamGenerateOptions | undefined, run: GetRunContext<typeof this>): AsyncStream<BAMChatLLMOutput, void>;
    messagesToPrompt(messages: BaseMessage[]): string;
    static fromPreset(modelId: BAMChatLLMPresetModel, overrides?: {
        client?: Client;
        parameters?: BAMLLMParameters | ((value: BAMLLMParameters) => BAMLLMParameters);
    }): BAMChatLLM;
}

export { type BAMChatLLMInputConfig as B, BAMChatLLMPreset as a, type BAMChatLLMPresetModel as b, BAMChatLLMOutput as c, type BAMChatLLMInput as d, type BAMChatLLMEvents as e, BAMChatLLM as f };
