import { B as BAMChatLLMInputConfig } from '../../chat-C9UQpVic.js';
import { BaseLLM } from '../../llms/base.js';
import '../../internals/types.js';
import '../../internals/helpers/guards.js';
import '../../adapters/bam/llm.js';
import '../../llms/llm.js';
import '../../emitter-DRfJC1TP.js';
import '../../internals/serializable.js';
import '../../context.js';
import '../../internals/helpers/promise.js';
import '../../errors.js';
import '../../cache/base.js';
import 'promise-based-task';
import '@ibm-generative-ai/node-sdk';
import '../../llms/chat.js';
import '../../llms/primitives/message.js';

/**
 * Copyright 2024 IBM Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

declare function assertLLMWithMessagesToPromptFn(instance: object): instance is BaseLLM<any, any> & {
    messagesToPrompt: BAMChatLLMInputConfig["messagesToPrompt"];
};

export { assertLLMWithMessagesToPromptFn };
